---
title: "Dummy Variables"
institute: "**Jam TBD**"
filter:
    - webr
---

```{r}
#| include: false
set.seed(2112)
```

::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

:::

## 0/1 Predictors

### Dummy Coding

"Dummy" variables are just predictors that only take the values 0 and 1.

\pspace

- 0 pairs of glasses versus 1 pair of glasses
    - This is a count that can only be 0 or 1\lspace
- 0 means automatic, 1 means manual
    - Arbitrary choice of 0/1\lspace
- 0 means off, 1 means on
    - Natural choice of 0/1, but still arbitrary

### Slopes with a Dummy Variable

Usual interpretation: as $x$ increases by 1, $y$ increases by $\beta$. 

\pspace

This doesn't go away, but we get a new interpretation!

- $x$ can *only* increase by one (from 0 to 1).
    - $\beta$ is the *difference in groups*.

\pspace

Note that we assume constant variance; this means the variance is the same in both groups

- *Exact* same assumptions as a t-test.
    - (Different from a "Welch" t-test)

### Categorical Variables

Consider the `cyl` column in mtcars. We *could* code three dummy variables:

- $I(cyl == 4)$
- $I(cyl == 6)$
- $I(cyl == 8)$

This would lead to the model:
$$
y = \beta_0 + \beta_1I(cyl == 4) + \beta_2I(cyl == 6) + \beta_3I(cyl == 8)
$$

What's the interpretation of the intercept here? 

### Categorical Variable Dummy Coding

Better: set one as a **reference** variable and let the intercept "absorb" it:

- $I(cyl == 6)$
- $I(cyl == 8)$

This would lead to the model:
$$
y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8)
$$
where

- $\beta_0$ is the mean of mpg when cyl = 4.
- $\beta_1$ is the difference in mean of mpg between 4 and 6 cylinder cars.
- $\beta_2$ is the difference in means for 4 versus 8.
    - Difference btwn 6 and 8 can be found with some cleverness.

### Models with Categorical Variables

The model $y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8)$ is equivalent to:
$$
y_i = \begin{cases}\beta_0 + 0 & \text{if }\; cyl == 4\\ \beta_0 + \beta_1 & \text{if }\; cyl == 6\\\beta_0  + \beta_2 & \text{if }\; cyl == 8\end{cases}
$$

This is equivalent to fitting an intercept-only model $y = \beta_0$ for subsets of the data.

\pspace

By putting them in the same model, we can easily test for significance. 

::: {.content-visible when-profile="book"}
The following code demonstrates that this is true - make sure you understand where each value is coming from!

```{web-r}
# Three intercepts
coef(lm(mpg ~ factor(cyl), data = mtcars))

# For each value of cylinder
# You'll have to do some math to get the same numbers as above!
coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 4)))
# 26.66364
coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 6)))
# 19.74286 = 26.66364 - 6.920779
coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 8)))
# 15.1 = 26.66364 - 11.563636
```

:::

## Interactions

### Different Intercepts, Same Slope

If we have `cyl` and `disp` in the model, we get the following:

$$
y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8) + \beta_3 disp
$$
which is equivalent to:
$$
y_i = \begin{cases}(\beta_0  + 0) + \beta_3 disp& \text{if }\; cyl == 4\\ (\beta_0 + \beta_1)  + \beta_3 disp& \text{if }\; cyl == 6\\(\beta_0  + \beta_2)  + \beta_3 disp& \text{if }\; cyl == 8\end{cases}
$$
*Like* three different models, but with a different **intercept** depending on `cyl`.

::: {.content-visible when-profile="book"}

In this case, the model is *not* equivalent. The slope must be the same for all three models, which cannot be done if we're fitting three separate models. As you know, the slope and intercept are correlated, so different slopes lead to different intercepts.

```{webr-r}
coef(lm(mpg ~ factor(cyl) + disp, data = mtcars))

coef(lm(mpg ~ disp, data = subset(mtcars, cyl == 4)))
```
:::

### Interaction Terms: Different Intercepts, Different Slopes

We can expand the model above with an **interaction term**.
$$
y = \beta_0 + \beta_1I(6) + \beta_2I(8) + \beta_3 disp + \beta_4I(6)disp + \beta_5I(8)disp
$$
where $I(6)$ is just shorthand for $I(cyl == 6)$.

This is the same as:
$$
y_i = \begin{cases}(\beta_0 + 0)  + (\beta_3 + 0) disp& \text{if }\; cyl == 4\\ (\beta_0 + \beta_1)  + (\beta_3 + \beta_4) disp& \text{if }\; cyl == 6\\(\beta_0  + \beta_2)  + (\beta_3 + \beta_5) disp& \text{if }\; cyl == 8\end{cases}
$$
In this case, we might as well fit 3 completely different models!

(Except we can test for significance!)

::: {.content-visible when-profile="book"}

And we're back to equivalence! Double check the intercepts and slopes for `cyl == 6` and `cyl == 8`, using the equations above.

```{webr-r}
coef(lm(mpg ~ factor(cyl) * disp, data = mtcars))

coef(lm(mpg ~ disp, data = subset(mtcars, cyl == 4)))
```
:::

### ANCOVA

If `g` is a categorical variable, then:

- `lm(y ~ g)` is a t-test (with equal variance) if `g` is binary
- `lm(y ~ g)` is an ANOVA if `g` has more than 2 categories
- `lm(y ~ x * g)` is an **ANCOVA** model
    - ANalysis of **COVAriance**.

\pspace

Main idea: Is the covariance (or correlation) between $x$ and $y$ different for different categories of $g$?

- Only a small extension to ANOVA
    - t-test: exactly 2 means
    - ANOVA: 2+ means
    - ANCOVA: 2+ covariances

### Beyond $x$ and $g$

- In the simple cases, we're doing t-test, ANOVA, or ANCOVA.\lspace
- Beyond this, we're just doing regression, no special names.
    - "Controlling for" is a term we'll use later.\lspace
- Choosing interaction terms is *hard*.
    - `ggplot2` makes parts of it a lot easier.



::: {.content-visible unless-profile="book"}
## Participation 

### Q1

A dummy variable is:

\pspace

a. A stupid variable. Just a dumb, stupid variable that only idiots use.
b. A variable that naturaly takes the values 0 and 1.
c. A variable that has been coded as 0 and 1 to indicate different groups.

<!--- C --->

### Q2

A reference category is used because:

\pspace

a. We are always more interested in one particular category and how the other categories compare to it.
b. The intercept is the mean of y when all predictors are 0, so we need a case where all predictors are 0.

<!--- B --->

### Q3

In the `mpg ~ factor(cyl)` there will be a dummy variable labelled:

\pspace

a. `6`
b. `cyl6`
c. `factor(cyl)6`
d. `factor(cyl)4`

<!--- C --->

### Q4

Which statement is false?

\pspace

a. t-test is a special case of linear regression.
b. ANCOVA is a special case of linear regression.
c. Welch's t-test for samples with unequal variances is a special case of regression.
d. ANOVA is a special case where Sequential Sum-of-Squares makes sense.

<!--- C --->

### Q5

It is possible to have interaction terms between two categorical variables.

\pspace

a. True
b. False

<!--- A --->

### Q6

In the regression `mpg ~ factor(cyl) + disp + factor(cyl):disp`, if we find that `disp` is not significant then we can safely remove it.

\pspace

a. True
b. False

<!--- B --->

:::

## Significance of a Group

### Output of `summary.lm()`

The output compares each slope to 0.

\pspace

- For a categorical predictor, this tests if a category is different *from the reference*. 

\pspace

There is not an easy built-in way to check significance of a specific group

- For example, we may want to test for equality of intercepts and slopes for 6 and 8 cylinder cars, allowing 4 to have separate values.
    - Test for equality of slopes is something covered in the textbook
    - Alternative: Change Reference group and use ESS

### Changing the reference group

Suppose we want to compare 6 and 8 cylinder cars. We can set up our model as:
$$
y = \beta_0 + \beta_1I(8) + \beta_2I(4) + \beta_3disp + \beta_4dispI(8) + \beta_5dispI(4)
$$
where now `6` is the reference group.

We can test $\beta_1=\beta_4 = 0$ using ESS to test whether mpg versus disp is the same in these two categories.

\pspace

In R, we need to set up our own dummies to do this.:

```{r}
#| label: set_reference
#| echo: true
#| eval: true
#| code-fold: false
mtcars$cyl <- relevel(factor(mtcars$cyl), ref = "8")
levels(mtcars$cyl)
```


::: {.content-visible when-profile="book"}
## Exercises

1. Complete the runnable R code above. 
:::


