---
title: "Dummy Variables"
institute: "**Jam TBD**"
filter:
    - webr
---

```{r}
#| include: false
library(ggplot2)
set.seed(2112)
```

::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

- A3 due Next Wednesday\lspace
- Project will be self-selected groups of 4\lspace

:::

## 0/1 Predictors

### Dummy Coding

"Dummy" variables are just predictors that only take the values 0 and 1.

\pspace

- 0 pairs of glasses versus 1 pair of glasses
    - This is a count that can only be 0 or 1\lspace
- 0 means automatic, 1 means manual
    - Arbitrary choice of 0/1\lspace
- 0 means off, 1 means on
    - Natural choice of 0/1, but still arbitrary

### Slopes with a Dummy Variable

Usual interpretation: as $x$ increases by 1, $y$ increases by $\beta$. 

\pspace

This doesn't go away, but we get a new interpretation!

- $x$ can *only* increase by one (from 0 to 1).
    - $\beta$ is the *difference in groups*.

\pspace

Note that we assume constant variance; this means the variance is the same in both groups

- *Exact* same assumptions as a t-test.
    - (Different from a "Welch" t-test)

### Categorical Variables

Consider the `cyl` column in mtcars. We *could* code three dummy variables:

- $I(cyl == 4)$
- $I(cyl == 6)$
- $I(cyl == 8)$

This would lead to the model:
$$
y = \beta_0 + \beta_1I(cyl == 4) + \beta_2I(cyl == 6) + \beta_3I(cyl == 8)
$$

What's the interpretation of the intercept here? 

### Categorical Variable Dummy Coding

Better: set one as a **reference** variable and let the intercept "absorb" it:

- $I(cyl == 6)$
- $I(cyl == 8)$

This would lead to the model:
$$
y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8)
$$
where

- $\beta_0$ is the mean of mpg when cyl = 4.
- $\beta_1$ is the difference in mean mpg between 4 and 6 cylinder cars.
- $\beta_2$ is the difference in mean mpg for 4 versus 8.
    - Difference btwn 6 and 8 can be found with some cleverness.

### Model-related plot

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

- `mpg` varies by different levels of `cyl`
    - **Model assumes equal variance.**\lspace

This is the plot you should check to see if a dummy variable makes sense to include.

\pspace

Another plot better describes the results...

:::
::: {.column width="50%"}
```{r}
#| echo: false
#| label: dummy-plot
#| fig-height: 6

plot(mpg ~ factor(cyl), data = mtcars)
```
:::
::::



### Models with Categorical Variables

The model $y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8)$ is equivalent to:
$$
y_i = \begin{cases}\beta_0 + 0 & \text{if }\; cyl == 4\\ \beta_0 + \beta_1 & \text{if }\; cyl == 6\\\beta_0  + \beta_2 & \text{if }\; cyl == 8\end{cases}
$$

This is equivalent to fitting an intercept-only model $y = \beta_0$ for subsets of the data.

\pspace

By putting them in the same model, we can easily test for significance. 

::: {.content-visible when-profile="book"}
The following code demonstrates that this is true - make sure you understand where each value is coming from!

```{webr-r}
#| label: intercept-model
#| echo: true
# Three intercepts
coef(lm(mpg ~ factor(cyl), data = mtcars))

# For each value of cylinder
# You'll have to do some math to get the same numbers as above!
coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 4)))
# 26.66364
coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 6)))
# 19.74286 = 26.66364 - 6.920779
coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 8)))
# 15.1 = 26.66364 - 11.563636
```

:::

### Testing Significance

\scriptsize

```{r}
#| label: intercept-significance
#| echo: true
summary(lm(mpg ~ factor(cyl), data = mtcars))$coef
```

So... is `cyl` significant? \pause

```{r}
#| label: intercept-significance-anova
#| echo: true
anova(lm(mpg ~ factor(cyl), data = mtcars))
```

It's an ESS test!!!


### Plotting the Results (Correct, but Don't Use This)

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

We fit a model with **two predictors**: 
$$
I(cyl == 6)\text{ and }I(cyl == 8)
$$
As far as R knows, these are two completely separate predictors!

\pspace

For plots on the right, Red line is the mean of `mpg` when cyl is 4, 6, and 8, respectively.

:::
::: {.column width="50%"}

```{r}
#| label: dummy-plot-2-predictors
#| echo: false
#| fig-height: 6

mylm <- lm(mpg ~ factor(cyl), data = mtcars)

X <- model.matrix(mylm)

par(mfrow = c(1, 2))
plot(X[, "factor(cyl)6"], mtcars$mpg, main = "mpg versus I(cyl == 6)")
abline(mylm$coef[1], mylm$coef[2])
lines(c(-0.05, 0.05), rep(mean(mtcars$mpg[mtcars$cyl == 4]), 2), col = "red")
lines(c(0.95, 1.05),  rep(mean(mtcars$mpg[mtcars$cyl == 6]), 2), col = "red")
plot(X[, "factor(cyl)8"], mtcars$mpg, main = "y versus x1")
abline(mylm$coef[1], mylm$coef[3], main = "mpg versus I(cyl == 8)")
lines(c(-0.05, 0.05), rep(mean(mtcars$mpg[mtcars$cyl == 4]), 2), col = "red")
lines(c(0.95, 1.05),  rep(mean(mtcars$mpg[mtcars$cyl == 8]), 2), col = "red")
```
:::
::::


## Interactions

### Different Intercepts, Same Slope

If we have `cyl` and `disp` in the model, we get the following:

$$
y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8) + \beta_3 disp
$$
which is equivalent to:
$$
y_i = \begin{cases}(\beta_0  + 0) + \beta_3 disp& \text{if }\; cyl == 4\\ (\beta_0 + \beta_1)  + \beta_3 disp& \text{if }\; cyl == 6\\(\beta_0  + \beta_2)  + \beta_3 disp& \text{if }\; cyl == 8\end{cases}
$$

- *Like* three different models, but with a different **intercept** depending on `cyl`.
    - Unless $\overline{disp} = 0$, the intercept and slope are correlated.

::: {.content-visible when-profile="book"}

In this case, the model is *not* equivalent. The slope must be the same for all three models, which cannot be done if we're fitting three separate models. As you know, the slope and intercept are correlated, so different slopes lead to different intercepts.

```{webr-r}
#| label: intercept-slope-model
#| echo: true
coef(lm(mpg ~ factor(cyl) + disp, data = mtcars))

coef(lm(mpg ~ disp, data = subset(mtcars, cyl == 4)))
```
:::

### Visualizing Different Intercepts (Same Slopes)

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

- `cyl == 4` seems to have a different intercept than the others.\lspace
- `cyl == 6` and `cyl == 8` could probbaly have the same intercept. 

\pspace

We would *not* generally fit a model where only one dummy gets a different value unless there was good reason.

:::
::: {.column width="50%"}

```{r}
#| label: intercept-slope-plot
#| fig-height: 6

mylm <- lm(mpg ~ disp + factor(cyl), data = mtcars)
mtcars$preds <- predict(mylm)

ggplot(mtcars) +
    theme_bw() +
    aes(x = disp, colour = factor(cyl)) +
    geom_point(aes(y = mpg)) +
    geom_line(aes(y = preds))
```
:::
::::

### Testing Significance

Let's introduce some fun R magic!

\scriptsize

```{r}
#| label: intercept-slope-sig
#| echo: true

mylm <- lm(mpg ~ factor(cyl) + disp, data = mtcars)
anova(mylm, update(mylm, ~ . - factor(cyl)))
```

\normalsize

The `update()` function takes `mylm`, uses the full formula (`~ .`), then removes `factor(cyl)`. 

This is an ESS ANOVA. There *is* a sig. diff., but it won't say which group is different!

::: {.content-visible when-profile="book"}
Note that we can get the same results if we put the predictors in a specific order and use R's built-in Type 2 ESS algorithm.

```{webr-r}
#| echo: true
#| label: intercept-slope-sig-type2SS

mylm <- lm(mpg ~ disp + factor(cyl), data = mtcars)
anova(mylm)
```

Ignore the line for `disp`, since that's testing whether adding displacement to an empty model improves the results.

Using the code above, verify and explain the following:

1. The p-value in the table would be different if we wrote `factor(cyl) + disp`.
    - Note that it would be *incorrect*.
2. The p-value for `disp` is different from the one we get in the `summary(mylm)` table.
:::


### Interaction Terms: Different Intercepts, Different Slopes

We can expand the model above with an **interaction term**.
$$
y = \beta_0 + \beta_1I(6) + \beta_2I(8) + \beta_3 disp + \beta_4I(6)disp + \beta_5I(8)disp
$$
where $I(6)$ is just shorthand for $I(cyl == 6)$.\pause

This is the same as:
$$
y_i = \begin{cases}(\beta_0 + 0)  + (\beta_3 + 0) disp& \text{if }\; cyl == 4\\ (\beta_0 + \beta_1)  + (\beta_3 + \beta_4) disp& \text{if }\; cyl == 6\\(\beta_0  + \beta_2)  + (\beta_3 + \beta_5) disp& \text{if }\; cyl == 8\end{cases}
$$
In this case, we might as well fit 3 completely different models!

(Except we can test for significance!)

::: {.content-visible when-profile="book"}

And we're back to equivalence! Double check the intercepts and slopes for `cyl == 6` and `cyl == 8`, using the equations above.

```{webr-r}
#| label: interaction-model
#| echo: true
coef(lm(mpg ~ factor(cyl) * disp, data = mtcars))

coef(lm(mpg ~ disp, data = subset(mtcars, cyl == 4)))
```
:::

### It's essentially three different models.

```{r}
#| label: interaction-plot
ggplot(mtcars) +
    theme_bw() +
    aes(x = disp, y = mpg, colour = factor(cyl)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, formula = y ~ x)
```

### Which "Significance" are we testing?

```{r}
#| label: interaction-lm
#| echo: true
#| eval: false
interact_lm <- lm(mpg ~ factor(cyl) * disp, data = mtcars)
```

- Significance of `factor(cyl)`?
- Significance of the interaction?

\pause

It depends on the context!

### Significance of the Interaction

This is just an ESS for a model with versus without the interaction term:

```{r}
#| label: interaction-sig
#| echo: true
interact_lm <- lm(mpg ~ factor(cyl) * disp, data = mtcars)
base_lm <- lm(mpg ~ factor(cyl) + disp, data = mtcars)
anova(interact_lm, base_lm)
```

\pspace

The difference in $SS_{Reg}$ is significant - what does that mean?

### ANCOVA

If `g` is a categorical variable, then:

- `lm(y ~ g)` is a t-test (with equal variance) if `g` is binary
- `lm(y ~ g)` is an ANOVA if `g` has more than 2 categories
- `lm(y ~ x * g)` is an **ANCOVA** model
    - ANalysis of **COVAriance**.

\pspace

Main idea: Is the covariance (or correlation) between $x$ and $y$ different for different categories of $g$?

- Only a small extension to ANOVA
    - t-test: exactly 2 means
    - ANOVA: 2+ means
    - ANCOVA: 2+ covariances

### Beyond $x$ and $g$

- In the simple cases, we're doing t-test, ANOVA, or ANCOVA.\lspace
- Beyond this, we're just doing regression, no special names.
    - "Controlling for" is a term we might use later.\lspace
- Choosing interaction terms is *hard*.
    - `ggplot2` makes parts of it a lot easier.



::: {.content-visible unless-profile="book"}
## Participation 

### Q1

A dummy variable is:

\pspace

a. A stupid variable. Just a dumb, stupid variable that only idiots use.
b. A variable that naturaly takes the values 0 and 1.
c. A variable that has been coded as 0 and 1 to indicate different groups.

<!--- C --->

### Q2

A reference category is used because:

\pspace

a. We are always more interested in one particular category and how the other categories compare to it.
b. The intercept is the mean of y when all predictors are 0, so we need a case where all predictors are 0.

<!--- B --->

### Q3

In the `mpg ~ factor(cyl)` there will be a dummy variable labelled:

\pspace

a. `6`
b. `cyl6`
c. `factor(cyl)6`
d. `factor(cyl)4`

<!--- C --->

### Q4

Which statement is false?

\pspace

a. t-test is a special case of linear regression.
b. ANCOVA is a special case of linear regression.
c. Welch's t-test for samples with unequal variances is a special case of regression.
d. ANOVA is a special case where Sequential Sum-of-Squares makes sense.

<!--- C --->

### Q5

It is possible to have interaction terms between two categorical variables.

\pspace

a. True
b. False

<!--- A --->

### Q6

In the regression `mpg ~ factor(cyl) + disp + factor(cyl):disp`, if we find that `disp` is not significant then we can safely remove it.

\pspace

a. True
b. False

<!--- B --->

:::

## Significance of a Group

### Output of `summary.lm()`

The output compares each slope to 0.

\pspace

- For a categorical predictor, this tests if a category is different *from the reference*. 

\pspace

There is not an easy built-in way to check significance of a specific group

- For example, we may want to test for equality of intercepts and slopes for 6 and 8 cylinder cars, allowing 4 to have separate values.
    - Test for equality of slopes is something covered in the textbook
    - Alternative: Change Reference group and use ESS

### Changing the reference group

Suppose we want to compare 6 and 8 cylinder cars. We can set up our model as:
$$
y = \beta_0 + \beta_1I(8) + \beta_2I(4) + \beta_3disp + \beta_4dispI(8) + \beta_5dispI(4)
$$
where now `6` is the reference group.

We can test $\beta_1=\beta_4 = 0$ using ESS to test whether mpg versus disp is the same in these two categories.

\pspace

In R, we need to set up our own dummies to do this.:

```{r}
#| label: set_reference
#| echo: true
#| eval: true
#| code-fold: false
mtcars$cyl <- relevel(factor(mtcars$cyl), ref = "8")
levels(mtcars$cyl)
```


::: {.content-visible when-profile="book"}
## Exercises

1. Complete the runnable R code above. 
:::


