---
title: "Dummy Variables"
institute: "**Same Mistake - But Different Remix** by Jan Blomqvist"
filter:
    - webr
---

```{r}
#| include: false
library(ggplot2)
library(palmerpenguins)
penguins <- penguins[complete.cases(penguins),]
set.seed(2112)
```

::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

- A3 due Next Wednesday\lspace
- Project will be self-selected groups of 4\lspace

:::

## 0/1 Predictors

### Dummy Coding

"Dummy" variables are just predictors that only take the values 0 and 1.

\pspace

- 0 pairs of glasses versus 1 pair of glasses
    - This is a count that can only be 0 or 1\lspace
- 0 means automatic, 1 means manual
    - Arbitrary choice of 0/1\lspace
- 0 means off, 1 means on
    - Natural choice of 0/1, but still arbitrary

### Slopes with a Dummy Variable

Usual interpretation: as $x$ increases by 1, $y$ increases by $\beta$. 

\pspace

This doesn't go away, but we get a new interpretation!

- $x$ can *only* increase by one (from 0 to 1).
    - $\beta$ is the *difference in groups*.

\pspace

Note that we assume constant variance; this means the variance is the same in both groups

- *Exact* same assumptions as a t-test.
    - (Different from a "Welch" t-test)

### Categorical Variables

Consider the `species` column in penguins. We *could* code three dummy variables:

- $I(species == Adelie)$
- $I(species == Chinstrap)$
- $I(species == Gentoo)$

For brevity, $I(Adelie)$ is the same as $I(species == Adelie)$.

This would lead to the model:
$$
y = \beta_0 + \beta_1I(Adelie) + \beta_2I(Chinstrap) + \beta_3I(Gentoo)
$$

What's the interpretation of the intercept here? 

### Categorical Variable Dummy Coding

Better: set one as a **reference** variable and let the intercept "absorb" it:

- $I(species == Chinstrap)$
- $I(species == Gentoo)$

This would lead to the model:
$$
y = \beta_0 + \beta_1I(Chinstrap) + \beta_2I(Gentoo)
$$
where

- $\beta_0$ is the mean of body mass for Adelie penguins.
- $\beta_1$ is the difference in mean body mass between Adelie and Chinstrap.
- $\beta_2$ is the difference in mean body mass for Adelie versus Gentoo.
    - Difference between Chinstrap and Gentoo can be found with some cleverness.

### Model-related plot

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

- `body_mass_g` varies by different levels of `species`
    - **Model assumes equal variance.**\lspace

This is the plot you should check to see if a dummy variable makes sense to include.

\pspace

Another plot better describes the results...

:::
::: {.column width="50%"}
```{r}
#| echo: false
#| label: dummy-plot
#| fig-height: 6

plot(body_mass_g ~ species, data = penguins)
```
:::
::::



### Models with Categorical Variables

The model $y = \beta_0 + \beta_1I(species == Chinstrap) + \beta_2I(species == Gentoo)$ is equivalent to:
$$
y_i = \begin{cases}\beta_0 + 0 & \text{if }\; species == Adelie\\ \beta_0 + \beta_1 & \text{if }\; species == Chinstrap\\\beta_0  + \beta_2 & \text{if }\; species == Gentoo\end{cases}
$$

This is equivalent to fitting an intercept-only model $y = \beta_0$ for subsets of the data.

\pspace

By putting them in the same model, we can easily test for significance. 

::: {.content-visible when-profile="book"}
The following code demonstrates that this is true - make sure you understand where each value is coming from!

```{webr-r}
#| label: intercept-model
#| echo: true
# Three intercepts
coef(lm(body_mass_g ~ species, data = penguins))
# 3700.66225, 32.42598, 1375.35401

# For each value of species
# You'll have to do some math to get the same numbers as above!
coef(lm(body_mass_g ~ 1, data = subset(penguins, species == "Adelie")))
# 3700.662
coef(lm(body_mass_g ~ 1, data = subset(penguins, species == "Chinstrap")))
# 3733.088 = 3700.662 + 32.42598
coef(lm(body_mass_g ~ 1, data = subset(penguins, species == "Gentoo")))
# 5076.016 = 3700.662 + 1375.35401
```

:::

### Testing Significance

\scriptsize

```{r}
#| label: intercept-significance
#| echo: true
summary(lm(body_mass_g ~ species, data = penguins))$coef
```

So... is `species` significant? \pause

```{r}
#| label: intercept-significance-anova
#| echo: true
anova(lm(body_mass_g ~ species, data = penguins))
```

It's an ESS test!!!


### Plotting the Results (Correct, but Don't Use This)

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

We fit a model with **two predictors**: 
$$
I(species == Chinstrap)\text{ and }I(species == Gentoo)
$$
As far as R knows, these are two completely separate predictors!

\pspace

For plots on the right, Red line is the mean of `body_mass_g` when species is Adelie, Chinstrap, or Gentoo, respectively.

:::
::: {.column width="50%"}

```{r}
#| label: dummy-plot-2-predictors
#| echo: false
#| fig-height: 6

mylm <- lm(body_mass_g ~ species, data = penguins)

X <- model.matrix(mylm)

par(mfrow = c(1, 2))
plot(X[, "speciesChinstrap"], penguins$body_mass_g, main = "body_mass_g versus I(species == \"Chinstrap\")")
abline(mylm$coef[1], mylm$coef[2])
lines(c(-0.05, 0.05), rep(mean(penguins$body_mass_g[penguins$species == "Adelie"]), 2), col = "red")
lines(c(0.95, 1.05),  rep(mean(penguins$body_mass_g[penguins$species == "Chinstrap"]), 2), col = "red")
plot(X[, "speciesGentoo"], penguins$body_mass_g, main = "y versus x1")
abline(mylm$coef[1], mylm$coef[3], main = "body_mass_g versus I(species == \"Gentoo\")")
lines(c(-0.05, 0.05), rep(mean(penguins$body_mass_g[penguins$species == "Adelie"]), 2), col = "red")
lines(c(0.95, 1.05),  rep(mean(penguins$body_mass_g[penguins$species == "Gentoo"]), 2), col = "red")
```
:::
::::


## Interactions

### Same Slope but Different Intercepts

If we have `species` and `flipper_length_mm` in the model, we get the following:

$$
y = \beta_0 + \beta_1I(species == Chinstrap) + \beta_2I(species == Gentoo) + \beta_3 flipper_length_mm
$$
which is equivalent to:
$$
y_i = \begin{cases}(\beta_0  + 0) + \beta_3 flipper_length_mm& \text{if }\; species == Adelie\\ (\beta_0 + \beta_1)  + \beta_3 flipper_length_mm& \text{if }\; species == Chinstrap\\(\beta_0  + \beta_2)  + \beta_3 flipper_length_mm& \text{if }\; species == Gentoo\end{cases}
$$

- *Like* three different models, but with a different **intercept** depending on `species`.
    - Unless $\overline{flipper_length_mm} = 0$, the intercept and slope are correlated.

::: {.content-visible when-profile="book"}

In this case, the model is *not* equivalent. The slope must be the same for all three models, which cannot be done if we're fitting three separate models. As you know, the slope and intercept are correlated, so different slopes lead to different intercepts.

```{webr-r}
#| label: intercept-slope-model
#| echo: true
coef(lm(body_mass_g ~ species + flipper_length_mm, data = penguins))

coef(lm(body_mass_g ~ flipper_length_mm, data = subset(penguins, species == "Adelie")))
```
:::

### Visualizing Different Intercepts (Same Slopes)

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

- `species == "Adelie"` seems to have a different intercept than the others.\lspace
- `species == "Chinstrap"` and `species == "Gentoo"` could probably have the same intercept. 

\pspace

We would *not* generally fit a model where only one dummy gets a different value unless there was good reason.

:::
::: {.column width="50%"}

```{r}
#| label: intercept-slope-plot
#| fig-height: 6

mylm <- lm(body_mass_g ~ flipper_length_mm + species, data = penguins)
penguins$preds <- predict(mylm)

ggplot(penguins) +
    theme_bw() +
    aes(x = flipper_length_mm, colour = species) +
    geom_point(aes(y = body_mass_g)) +
    geom_line(aes(y = preds))
```
:::
::::

### Testing Significance

Let's introduce some fun R magic!

\scriptsize

```{r}
#| label: intercept-slope-sig
#| echo: true

mylm <- lm(body_mass_g ~ species + flipper_length_mm, data = penguins)
anova(mylm, update(mylm, ~ . - species))
```

\normalsize

The `update()` function takes `mylm`, uses the full formula (`~ .`), then removes `species`. 

This is an ESS ANOVA. There *is* a sig. diff., but it won't say which group is different!

::: {.content-visible when-profile="book"}
Note that we can get the same results if we put the predictors in a specific order and use R's built-in Type 2 ESS algorithm.

```{webr-r}
#| echo: true
#| label: intercept-slope-sig-type2SS

mylm <- lm(body_mass_g ~ flipper_length_mm + species, data = penguins)
anova(mylm)
```

Ignore the line for `flipper_length_mm`, since that's testing whether adding flipper_length_mmlacement to an empty model improves the results.

Using the code above, verify and explain the following:

1. The p-value in the table would be different if we wrote `species + flipper_length_mm`.
    - Note that it would be *incorrect*.
2. The p-value for `flipper_length_mm` is different from the one we get in the `summary(mylm)` table.
:::


### Interaction Terms: Different Intercepts, Different Slopes

We can expand the model above with an **interaction term**.
$$
y = \beta_0 + \beta_1I(Chinstrap) + \beta_2I(Gentoo) + \beta_3 flipper_length_mm + \beta_4I(Chinstrap)flipper_length_mm + \beta_5I(Gentoo)flipper_length_mm
$$
where $I(Chinstrap)$ is just shorthand for $I(species == Chinstrap)$.\pause

This is the same as:
$$
y_i = \begin{cases}(\beta_0 + 0)  + (\beta_3 + 0) flipper_length_mm& \text{if }\; species == Adelie\\ (\beta_0 + \beta_1)  + (\beta_3 + \beta_4) flipper_length_mm& \text{if }\; species == Chinstrap\\(\beta_0  + \beta_2)  + (\beta_3 + \beta_5) flipper_length_mm& \text{if }\; species == Gentoo\end{cases}
$$
In this case, we might as well fit 3 completely different models!

(Except we can test for significance!)

::: {.content-visible when-profile="book"}

And we're back to equivalence! Double check the intercepts and slopes for `species == "Chinstrap"` and `species == "Gentoo"`, using the equations above.

```{webr-r}
#| label: interaction-model
#| echo: true
coef(lm(body_mass_g ~ species * flipper_length_mm, data = penguins))

coef(lm(body_mass_g ~ flipper_length_mm, data = subset(penguins, species == "Adelie")))
```
:::

### It's essentially three different models.

```{r}
#| label: interaction-plot
ggplot(penguins) +
    theme_bw() +
    aes(x = flipper_length_mm, y = body_mass_g, colour = species) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, formula = y ~ x)
```

### Which "Significance" are we testing?

```{r}
#| label: interaction-lm
#| echo: true
#| eval: false
interact_lm <- lm(body_mass_g ~ species * flipper_length_mm, data = penguins)
```

- Significance of `species`?
- Significance of the interaction?

\pause

It depends on the context!

### Significance of the Interaction

This is just an ESS for a model with versus without the interaction term:

```{r}
#| label: interaction-sig
#| echo: true
interact_lm <- lm(body_mass_g ~ species * flipper_length_mm, data = penguins)
base_lm <- lm(body_mass_g ~ species + flipper_length_mm, data = penguins)
anova(interact_lm, base_lm)
```

\pspace

The difference in $SS_{Reg}$ is significant - what does that mean?

### ANCOVA

If `g` is a categorical variable, then:

- `lm(y ~ g)` is a t-test (with equal variance) if `g` is binary
- `lm(y ~ g)` is an ANOVA if `g` has more than 2 categories
- `lm(y ~ x * g)` is an **ANCOVA** model
    - ANalysis of **COVAriance**.

\pspace

Main idea: Is the covariance (or correlation) between $x$ and $y$ different for different categories of $g$?

- Only a small extension to ANOVA
    - t-test: exactly 2 means
    - ANOVA: 2+ means
    - ANCOVA: 2+ covariances

### Beyond $x$ and $g$

- In the simple cases, we're doing t-test, ANOVA, or ANCOVA.\lspace
- Beyond this, we're just doing regression, no special names.
    - "Controlling for" is a term we might use later.\lspace
- Choosing interaction terms is *hard*.
    - `ggplot2` makes parts of it a lot easier.



::: {.content-visible unless-profile="book"}
## Participation 

### Q1

A dummy variable is:

\pspace

a. A stupid variable. Just a dumb, stupid variable that only idiots use.
b. A variable that naturaly takes the values 0 and 1.
c. A variable that has been coded as 0 and 1 to indicate different levels of a factor.

<!--- C --->

### Q2

A reference category is used because:

\pspace

a. We are always more interested in one particular category and how the other categories compare to it.
b. The intercept is the mean of y when all predictors are 0, so we need a case where all predictors are 0.

<!--- B --->

### Q3

In the output of `lm(body_mass_g ~ species)` there will be a term labelled:

\pspace

a. `Adelie`
b. `speciesAdelie`
c. `speciesChinstrap`
d. `species`

<!--- C --->

### Q4

Which statement is false?

\pspace

a. t-test is a special case of linear regression.
b. ANCOVA is a special case of linear regression.
c. Welch's t-test for samples with unequal variances is a special case of regression.
d. ANOVA is a special case where Sequential Sum-of-Squares makes sense.

<!--- C --->

### Q5

It is possible to have interaction terms between two categorical variables.

\pspace

a. True
b. False

<!--- A --->

### Q6

In the regression `body_mass_g ~ species + bill_length_mm + species:bill_length_mm`, if we find that `bill_length_mm` is not significant then we can safely remove it.

\pspace

a. True
b. False

<!--- B --->

:::

## Significance of a Group

### Output of `summary.lm()`

The output compares each slope to 0.

\pspace

- For a categorical predictor, this tests if a category is different *from the reference*. 

\pspace

There is not an easy built-in way to check significance of a specific group

- For example, we may want to test for equality of intercepts and slopes for Chinstrap and Gentoo penguins, allowing Adelie to have separate values.
    - Test for equality of slopes is something covered in the textbook
    - Alternative: Change Reference group and use ESS

### Changing the reference group

Suppose we want to compare Gentoo and Chinstrap penguins. We can set up our model as:
$$
y = \beta_0 + \beta_1I(Gentoo) + \beta_2I(Adelie) + \beta_3flipper_length_mm + \beta_4flipper_length_mmI(Gentoo) + \beta_5flipper_length_mmI(Adelie)
$$
where now `Chinstrap` is the reference group.

We can test $\beta_1=\beta_4 = 0$ using ESS to test whether `body_mass_g` versus `flipper_length_mm` is the same in these two categories.

\pspace

In R, we need to set up our own dummies to do this.:

```{r}
#| label: set_reference
#| echo: true
#| eval: true
#| code-fold: false
penguins$species<- relevel(factor(penguins$species), ref = "Gentoo")
levels(penguins$species)
```


::: {.content-visible when-profile="book"}
## Exercises

1. Complete the runnable R code above. 
:::


