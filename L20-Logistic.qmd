---
title: "Classification"
institute: "Jam: **Sigma Oasis** by Phish"
format: 
    revealjs:
        slide-level: 3
        theme: serif
        smaller: true
        height: 800
filters:
    - webr
    - shinylive
---

```{r}
#| include: false
set.seed(2112)
```

::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

:::

## Logistic Regression

### Goal: Predict a 1

- Response: 0 or 1
    - Predictions: probability of a 1?\lspace

### Why Not Linear Regression?

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

The plot on the right shows Default versus Credit Card Balance (with jittering).

- "Default" is 0 for No, 1 for Yes
    - Dummy encoding for the *response*
- Linear model predicts negative values!
    - Also, values above 1.
- What could the slope possibly represent???

:::
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: false
#| fig-height: 6
#| fig-width: 6

library(ISLR2)
library(ggplot2)

ggplot(Default) +
    aes(x = balance, y = as.numeric(default) - 1) +
    geom_jitter(height = 0.1, width = 0) +
    theme_bw() +
    labs(x = "Credit Card Balance",
        y = "Defaulted (No = 0, Yes = 1)") +
    scale_y_continuous(breaks = c(0, 1), minor_breaks = c(0, 1)) +
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE)
```
:::
::::

### The mean of y at each value of x

```{r}
#| label: mean-of-y-for-each-x
#| echo: false
#| eval: true
#| code-fold: false
plot_averages <- function(x, y, n = 50, window_frac = 1 / 5, ...) {
    plot(x, y, ...)
    xseq <- seq(min(x), max(x), length.out = n)
    window <- diff(range(x)) * window_frac
    mean_y <- c()
    for(i in seq_along(xseq)) {
        mean_y[i] <- mean(y[x < xseq[i] + window & x > xseq[i] - window])
    }
    lines(xseq, mean_y)
}
plot_averages(x = Default$balance, y = as.numeric(Default$default) - 1,
    window_frac = 1 / 15,
    ylab = "Default (0 = No, 1 = Yes)",
    xlab = "Credit Card Balance")
```

### The Logistic Function - A Sigmoid

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

If $t\in\mathbb{R}$, then
$$
\sigma(t) = \dfrac{\exp(t)}{1 + \exp(t)}\in[0,1]
$$
where $\sigma(\cdot)$ is the **logistic** function.

:::
::: {.column width="50%"}

```{r}
#| fig-height: 4
#| fig-width: 5
#| echo: false
library(ggplot2)
t <- seq(-10, 10, 0.01)
sigma_t <- exp(t)/(1 + exp(t))
ggplot() + theme_minimal() +
    geom_line(mapping = aes(x = t, y = sigma_t)) +
    geom_hline(yintercept = c(0,1), linetype = "dashed") +
    labs(x = "t", y = expression(sigma(t)))
```

:::
::::

### Logistic Function - Now with Parameters

$$
\sigma(\beta_0 + \beta_1 t)
$$

```{shinylive-r}
#| standalone: true
#| viewerHeight: 600
#| viewerWidth: 800

xseq <- seq(-10, 10, 0.05)
sigma <- function(t) exp(t)/(1 + exp(t))

input <- list(b0 = 0, b1 = 1)

ui <- fluidPage(
    sidebarPanel(
        sliderInput("b0", "Intercept B_0", -5, 5, 0, 0.1, animate = list(interval = 100)),
        sliderInput("b1", "Slope B_1", -5, 5, 1, 0.1, animate = list(interval = 100))
    ),
    mainPanel(plotOutput("plot"), )
)

server <- function(input, output) {
    output$plot <- renderPlot({
        yseq <- sigma(input$b0 + input$b1 * xseq)
        plot(yseq ~ xseq,
            xlim = c(-10, 10),
            ylim = c(0, 1),
            ylab = bquote(sigma (beta[0] *" + " * beta[1] * t)),
            xlab = "t",
            type = "l")
    })
}

shinyApp(ui = ui, server = server)
```

### Logistic Function - Now with Parameters *Estimated from DATA*

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

\begin{align*}
\eta(x_i) &= X\underline{\beta}\\
p_i &= \sigma(\eta(x_i))\\
\implies \log\left(\frac{p_i}{1-p_i}\right) &= X\underline{\beta}
\end{align*}

:::
::: {.column width="50%"}

```{r}
#| fig-height: 4
#| fig-width: 5
#| echo: false
#| code-fold: false
library(ISLR2)
library(ggplot2)
library(dplyr)

plot_averages(x = Default$balance, y = as.numeric(Default$default) - 1,
    window_frac = 1/15,
    ylab = "Default (0 = No, 1 = Yes)",
    xlab = "Credit Card Balance")
xseq <- seq(min(Default$balance), max(Default$balance), by = 10)
expit <- function(eta) exp(eta) / (1 + exp(eta))
yseq <- expit(-10.65 + 0.0054 * xseq)
lines(xseq, yseq,col = "red")
```

:::
::::

\centering
$\eta(x_i) = -10.65 + 0.0054\cdot\text{balance}_i$

### Logistic Regression

- The response is 0 or 1 (no or yes, don't default or default, etc.)\lspace

::: {.content-visible unless-profile="book"}
&nbsp;
:::


- The probability of a 1 increases according to the sigmoid function.
    - The **linear predictor** is $\eta(x_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \cdots$
    - The probability of class 1 is $P(\text{class }1 | \text{predictors}) = \sigma(\eta(x_i))$\lspace

::: {.content-visible unless-profile="book"}
&nbsp;
:::

- Instead of normality assumptions, we use a binomial distribution.

It's just one step away from a linear model!

### Interpreting Slope Parameters

- General structure: "For each one unit increase in $x_i$, some function of $y_i$ changes by some function of $\beta$".

::: {.content-visible unless-profile="book"}
&nbsp;
:::

- For logistic regression:
    - One unit increase in $x_i$, $\log\left(\frac{p(x_i)}{1-p(x_i)}\right)$ increases by $\beta$.\pause\newline

::: {.content-visible unless-profile="book"}
&nbsp;
:::

- The **odds** are $\frac{p(x_i)}{1-p(x_i)}$.
    - "1 in 5 people with odds of 1/4 will default."\newline

::: {.content-visible unless-profile="book"}
&nbsp;
:::

- $\beta$ is the **change in log odds** for a one unit increase.
    - "**log odds _ratio_**".

### Odds Ratios

Consider the one-predictor example. 

$$
\eta(x_i) = \beta_0 + \beta_1x_{i1}\text{ and }\eta(x_i + 1) = \beta_0 + \beta_1x_i + \beta_1\text{, }\therefore \eta(x_i + 1) - \eta(x_i) = \beta
$$

::: {.content-visible unless-profile="book"}
&nbsp;
:::

The logg odds ratio is defined as
$$
\log(OR) = \log\left( \frac{p(x_i + 1)}{1 - p(x_i + 1)} \middle/ \frac{p(x_i)}{1 - p(x_i)}\right)
$$
Using $\eta(x_i) = \log\left(\frac{p(x_i)}{1-p(x_i)}\right)$, show that $\log(OR) = \beta_1$

::: {.content-visible unless-profile="book"}
&nbsp;
:::

### The Loss Function

For all observations:

- If $y_i = 0$, we want $p(x_i)$ to be as *low* as possible.
    - Maximize $1 - P(Y_{i} = 1|\beta_0,\beta_1,X)$\lspace
- If $y_i = 1$, we want $p(x_i)$ to be as *high* as possible.
    - Maximize $P(Y_{i} = 1|\beta_0,\beta_1,X)$

These can be combined as:
$$
\prod_{i:y_{i} = 0}\left(1 - P(Y_{i} = 1|\beta_0,\beta_1,X)\right)\prod_{i:y_i=1}P(Y_i = 1|\beta_0,\beta_1,X)
$$
which is equivalent to:
$$
\prod_{i=1}^n(1 - P(Y_{i} = 1|\beta_0,\beta_1,X))^{1 - Y_i}P(Y_i = 1|\beta_0,\beta_1,X)^{Y_i}
$$

Which is NOT just the sum of squared errors!

::: {.content-visible unless-profile="book"}
&nbsp;
:::

Unlike linear regression, there's no closed form for $\hat{\beta}_0$ and $\hat\beta_1$; we need numerical methods.

### Examples: Two different predictors in the `Default` data

:::: {.columns}
::: {.column width="45%"}
```{r}
#| fig-height: 1.75
#| fig-width: 4
#| echo: false

Default$default <- as.numeric(factor(Default$default)) - 1
Default$student <- as.numeric(factor(Default$student)) - 1

ggplot(Default) + theme_minimal() +
    aes(x = student, y = default) +
    geom_jitter(alpha = 0.25, height = 0.1, width = 0.1) +
    geom_smooth(method = "glm", se = TRUE,
        method.args = list(family = "binomial")) +
    labs(x = "Student (0 = No)", y = "Default (0 = No)")
```

$$
\eta(x_i) = -3.5 + 0.5\cdot\text{student}
$$

Odds of a student defaulting are $\exp(0.5)\approx1.65$ times as high as a non-student.\pause
:::
::: {.column width="45%"}
```{r}
#| fig-height: 1.75
#| fig-width: 4
#| echo: false

library(ggplot2)
library(ISLR2)

ggplot(Default) + theme_minimal() +
    aes(x = balance, y = default) +
    geom_jitter(alpha = 0.25, height = 0.1, width = 0) +
    geom_smooth(method = "glm", se = FALSE,
        method.args = list(family = "binomial")) +
    labs(x = "Credit card balance", y = "Default (0 = No)")
```

$$
\eta(x_i) = -10.65 + 0.005\cdot\text{balance}
$$

Each extra dollar of CC balance increases odds of defaulting by a factor of 1.005.
:::
::::

::: {.content-visible unless-profile="book"}
&nbsp;
:::

\pause
*The scale of the predictors matters.*

### Odds versus Probabilities

"The odds of a student defaulting are $\exp(0.5)\approx1.65$ times as high as a non-student."

$$
\frac{P(\text{default} | \text{student} = 1)}{1 - P(\text{default} | \text{student} = 1)} \biggm/ \frac{P(\text{default} | \text{student} = 0)}{1 - P(\text{default} | \text{student} = 0)} = 1.65
$$


::: {.content-visible unless-profile="book"}
&nbsp;
:::

This **cannot** be solved for $P(\text{default} | \text{student} = 1)$!

$$
P(\text{default} | \text{student} = 1) = \dfrac{\exp(\eta(x_i))}{1 + \exp(\eta(x_i))} \approx 0.047
$$


### Multiple ~~Linear~~ Logistic Regression

- Predictors can be **multicollinear**, **confounded**, and have **interactions**.
    - Logistic is just Linear on a transformed scale!\lspace

::: {.content-visible unless-profile="book"}
&nbsp;
:::

- We do *not* look for transformations of the response.
    - It's already a transformation of the response $p_i(x_i)$!\lspace

::: {.content-visible unless-profile="book"}
&nbsp;
:::

- We *do* look for transformations of the predictors!
    - Sigmoid + Polynomial is where the real fun is.\lspace

### Errors in Logistic Regression: Deviance

- All "errors" are either $p(x_i)$ or $1 - p(x_i)$.
    - i.e., distances are measured form either 0 or 1.

::: {.content-visible unless-profile="book"}
&nbsp;
:::

Instead, we use the *deviance*.

- If $p(x_i)$ were the true probability in a binomial distribution, what's the probability of the observed value (0 or 1)? 
    - In other words, $P(Y_i = 1| \beta_0, \beta_1, X)$ and $1 - P(Y_i = 1| \beta_0, \beta_1, X)$ are the residuals!
        - The residual we use depends on what the response is. If $y_i = 0$, the residual is $P(Y_i = 1 | \beta_0, \beta_1, X)$
    - This is used more broadly in **Generalized Linear Models** (GLMs). Logistic Regression is one of many GLMs.




### Logistic Decision Boundaries

$$
P(\text{defaulting} | \eta(x_i)) > p \implies a + bx_1 + cx_2 + dx_3 > e
$$

for some (linear) hyperplane $a + bx_1 + cx_2 + dx_3$ and some value $e$.

\quad

- Choosing $p=0.5$ is standard, but other thresholds can be chosen.
    - Cancer example: want to be more admissive of false positives
        - Would rather operate and be wrong than falsely tell the patient that they're healthy!



### 


```{r}
#| label: decision_boundary
#| cache: true
#| fig-height: 4
#| fig-width: 9

library(ggplot2)
library(ISLR2)

Default$default <- as.numeric(factor(Default$default)) - 1
Default$student <- as.numeric(factor(Default$student)) - 1

decision_grid <- expand.grid(
    student = c(0,1),
    balance = seq(0, 2655, length.out = 250),
    income= seq(770, 73555, length.out = 250)
)

my_glm <- glm(default ~ student + balance + income,
    data = Default, family = binomial)
decision_grid$pred <- predict(my_glm, newdata = decision_grid)

student_labels <- c("Not Student", "Student")
names(student_labels) <- c(0, 1)

ggplot() + theme_minimal() +
    geom_tile(data = decision_grid,
        mapping = aes(x = balance, y = income, fill = factor(pred > 0.5))) +
    scale_fill_manual(values = c("firebrick", "green", "firebrick", "green")) +
    geom_point(data = Default, 
        mapping = aes(x = balance, y = income, 
            fill = factor(default == 1)),
        shape = 21) +
    facet_wrap(~ student, 
        labeller = labeller(student = student_labels)) +
    labs(x = "Credit Card Balance",
        y = "Income",
        fill = "Default?")

```


### Predictions - Just Plug it In!

```{r}
#| eval: false
#| include: false
#| echo: false

library(ISLR2)

mylm <- glm(default ~ student + balance + income, 
    data = Default, family = binomial)
predict(mylm, newdata = data.frame(student = "Yes", 
    balance = 2000, income = 20000),
    type = "link")
predict(mylm, newdata = data.frame(student = "Yes", 
    balance = 2000, income = 20000),
    type = "response")
```

| | Intercept | Student | Balance | Income |
| --- | --- | --- | --- | --- |
| $\beta$ | -10.09 | -0.65 | 0.0057 | 0.000003 |

We can make a prediction for a student with $2,000 balance and $20,000 income:
\begin{align*}
\eta(x) &= \beta_0 + \beta_1\cdot 1 + \beta_2\cdot 2000 + \beta_3\cdot 20000 \approx 0.0178\\
&\\
P(\text{defaulting} | x) &= \dfrac{\exp(\eta(x))}{1 + \exp(\eta(x))} \approx \dfrac{\exp(0.0178)}{1 + \exp(0.0178)} \approx 0.504\\
&\\
&P(\text{defaulting} | x) > 0.5 \implies \text{Predict Default}
\end{align*}

### Standard Errors: It's complicated

::: {.content-visible unless-profile="book"}
&nbsp;
:::

We don't have an estimate like $\hat\beta = (X^TX)^{-1}X^TY$. We had to resort to numerical methods.

::: {.content-visible unless-profile="book"}
&nbsp;
:::

There *is* a closed form for $V(\hat\beta)$ and a way to test significance, but they're beyond the scope of this course.

- Relies on likelihoods, which we avoided in this course.
- You can assume significance tests in R output are correct.

::: {.content-visible unless-profile="book"}

## Participation

### Q1

::: {.content-visible unless-profile="book"}
&nbsp;
:::

Logistic regression models the mean of y for each value of x.

::: {.content-visible unless-profile="book"}
&nbsp;
:::

a. TRUE
b. FALSE

<!--- A --->

### Q2

::: {.content-visible unless-profile="book"}
&nbsp;
:::

Logistic regression results in predictions of either 0 or 1.

::: {.content-visible unless-profile="book"}
&nbsp;
:::

a. TRUE
b. FALSE

<!--- B --->

### Q3

::: {.content-visible unless-profile="book"}
&nbsp;
:::

A slope parameter of 2 means that each one unit increase in x is associated with...

::: {.content-visible unless-profile="book"}
&nbsp;
:::

a. An increase of 2 in the odds of y.
b. An increase of 2 in y.
c. An increase of 2 in the log of the odds ratio of y.
d. An increase of 2 in the probability of y.

<!--- C --->

### Q4

::: {.content-visible unless-profile="book"}
&nbsp;
:::

Which of the following linear regression concepts does *not* apply to logistic regression?

::: {.content-visible unless-profile="book"}
&nbsp;
:::

a. Predictions are found by plugging in x values.
b. Multicollinearity will affect the covariance of the parameter estimates.
c. The SSE divided by its degrees of freedom will follow a Chi-Square distribution.
d. We can use polynomial terms and interactions to get different patterns.

<!--- A --->

:::


## Classification Basics

### Goal: Predict a Category

- **Binary:** Yes/no, success/failure, etc.\newline
- **Categorical:** 2 or more categories.
    - A.k.a. qualitative, but that's a social science word.

\quad

In both: predict whether an observation is in category $j$ given its predictors.
$$
P(Y_i = j| x = x_i) \stackrel{def}{=} p_j(x_i)
$$


### Classification Confusion

**Confusion Matrix:** A tabular summary of classification errors.

:::: {.columns}
::: {.column width="60%"}

| | True Pay ($\cdot 0$) | True Def ($\cdot 1$)|
|---|---|---|
| Pred Pay ($0 \cdot$) | Good (00) | Bad (01) |
| Pred Def ($1 \cdot$) | Bad (10) | Good (11) |


:::
::: {.column width="40%"}
\vspace{0.25cm}

- Two ways to be wrong\newline
- Two ways to be right\newline
- Different applications have different needs
:::
::::
\quad\pause 

\quad\centering

**Accuracy:** $\dfrac{\text{Correct Predictions}}{\text{Number of Predictions}} =\frac{00 + 11}{00 + 01 + 10 + 11}$


### Is "Accuracy" Good?

Task: Predict whether a person has cancer 

(In this made up example, 0.02\% of people have cancer).

\quad

| | True Healthy | True Cancer |
|---|---|---|
| Pred. Healthy | Save a Life | Lose a Life |
| Pred. Cancer | Expensive/Invasive | All good |

\quad

:::: {.columns}
::: {.column width="50%"}
- **Easy:**  99.8\% accuracy.
    - Always guess "Not Cancer"
:::
::: {.column width="50%"}
- **Very Hard:** 99.82\% accuracy.
:::
::::



### The Confusion Matrix for Default Data {.scrollable}

```{r}
#| eval: false
#| include: false

library(ISLR2)
mylm <- glm(default ~ student + balance + income, 
    data = Default, family = binomial)
mypred <- predict(mylm, type = "response")

conf <- table(mypred > 0.5, Default$default)

```

| | True Payment | True Default |
| --- | --- | --- |
| Pred Payment | 9627 | 228 |
| Pred Default | 40 | 105 |

- This model: 97.32% accuracy.
    - Naive model: always predict "Pay" - 96.67% accuracy!

Other important measures (not on exam): 

- Sensitivity: $\dfrac{\text{True Positives}}{\text{All Positives in Data}} = \dfrac{9627}{9627 + 40} = 99.58%$ (Naive: 100%)\lspace
- Specificity: $\dfrac{\text{True Negatives}}{\text{All Negatives in Data}} = \dfrac{105}{105 + 228} = 31.53$ (Naive: 0%)

### Logistic Regression in R {.scrollable}

```{webr-r}
library(palmerpenguins)
peng <- penguins[complete.cases(penguins), ]

log_cont <- glm(sex ~ bill_length_mm + 
        bill_depth_mm + flipper_length_mm,
    data = peng, family = "binomial")

anova(log_cont, test = "Chisq") # Sequential

log_spec <- update(log_cont, ~ . + species * flipper_length_mm)

anova(log_cont, log_spec, test = "Chisq")

full_spec <- glm(sex ~ species*(bill_length_mm + 
        bill_depth_mm + flipper_length_mm),
    data = peng, family = "binomial")

anova(log_spec, full_spec, test = "Chisq")
anova(log_spec, update(log_spec, ~ . - species:flipper_length_mm), test = "Chisq")

summary(log_spec)
coef(log_spec)
```

The residual plots are the same as before:

```{webr-r}
par(mfrow = c(2, 2))
plot(log_spec)
```

The predictions can either be on the logit scale (`type = "link"`, the default) or on the response scale (probabilities).

```{webr-r}
predict(log_spec, type = "response") |> head()
```

Regularization is often used with logistic regression (in python's scikit-learn package, Ridge regularization is used by default without warning the user).

```{webr-r}
library(glmnet)

X <- model.matrix(sex ~ species*(bill_length_mm + 
        bill_depth_mm + flipper_length_mm),
    data = peng)
y <- as.factor(peng$sex)

mycv <- cv.glmnet(X, y, family = binomial)

mylasso <- glmnet(X, y,
    data = peng, family = "binomial", alpha = 1,
    lambda = mycv$lambda.1se)
coef(mylasso)
```
