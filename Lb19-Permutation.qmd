---
title: "Permutation Tests"
filter:
  - webr
---

## Introduction

A *permutation* is a re-ordering of a set of things. In this case, we're randomly re-ordering the data. Note that we are not randomly sampling rows, we are just changing the order. In particular, we're randomly re-ordering the measurements independently of each other!

Why would we do this? Because it destroys the correlation between our variables while preserving their statistical properties. This gives us a "null" distribution that makes no assumptions about the distribution of the data!

## Intuition: the facial impact test

The best statistical test is the "facial impact test". In this test, we reject the null hypothesis if the results are so obvious that they jump off the page and slap you in the face (facial impact).

For this example I'm going to plot 20 different plots. One of them is going to be the original data, and the rest will be random permutations. If you can tell which is the tru data, then there's probably something important going on! In other words, the relationship between the response and the predictor matters.

```{webr-r}
data(women)
x <- women$weight
# Choose a random index. This plot will be the true data
real_data_index <- sample(1:20, size = 1)
# Prepare the plotting space to be 5 plots by 4 plots and smaller margins
par(mfrow = c(5, 4), mar = c(2, 2, 1, 1))
for(i in 1 : 20) {
    if (i != real_data_index) {
        # randomly reorder the rows
        # i.e. sample n rows from n rows without replacement.
        y = women$height[sample(1:nrow(women), size = nrow(women), replace = FALSE)]
    } else {
        y = women$height
    }
    plot(y ~ x)
}
```

I've ran this several times and my cheek hurts from how much the results have slapped me in the face.

Here's a less obvious example, but I'm sure you can still pick out the real data:

```{webr-r}
data(cars)
x <- cars$speed
# Choose a random index. This plot will be the true data
real_data_index <- sample(1:20, size = 1)
# Prepare the plotting space to be 5 plots by 4 plots and smaller margins
par(mfrow = c(5, 4), mar = c(2, 2, 1, 1))
for(i in 1 : 20) {
    if (i != real_data_index) {
        # randomly reorder the rows
        # i.e. sample n rows from n rows without replacement.
        y = cars$dist[sample(1:nrow(cars), size = nrow(cars), replace = FALSE)]
    } else {
        y = cars$dist
    }
    plot(y ~ x)
}
```

## The Permutation Test for Significance of Regression

A test for significance usually finds the probability of getting results at least as significant as the results we got, assuming that the null is true.

For regression, the null is that there is no relationship. Permutations destroy the relationship, so permutations create a null distribution without making any normality assumptions! To get the p-value, we can do a bunch of permutations and look at our data compared to the distribution under the null hypothesis. 

The code below demonstrates this for the `cars` data. Notice how similar this is to the simulation code we've used, but we're not generating any new data. Try it again with the `women` data!

```{webr-r}
data(cars)
x <- cars$speed
y <- cars$dist
obs_beta <- coef(lm(y ~ x))[2]
betas <- c()
for(i in 1:1000) {
    y <- cars$dist[sample(1:nrow(cars), size = nrow(cars), replace = FALSE)]
    mylm <- lm(y ~ x)
    betas[i] <- coef(mylm)[2] # Extract beta_1
}

hist(betas, xlim = range(betas, obs_beta))
abline(v = obs_beta, col = "red")

# Probability of a value *at least as extreme*
mean_beta <- mean(betas)
as_extreme <- abs(obs_beta - mean_beta)
above <- mean_beta + as_extreme
below <- mean_beta - as_extreme
pvalue <- mean(betas < below | betas > above)
pvalue
```

Under the null, the sample slopes can range from -2 to 2. However, our actual slope was 4! Notice that the p-value calculation is the probability of a value at least as extreme; we first find the distance from the mean, then count the number of values this far or further from the mean. The expression `betas < below | betas > above` returns a bunch of true/false values, with "TRUE" meaning that the point is either below `below` or above `above`. The `mean()` function, when applied to a bunch of true/false values, is a proportion. Thus, we are calculating the proportion of points that were at least as far away as the data we got, which is the definition of a p-value!

In the end, we got a p-value of exactly 0. The p-value from normality assumptions is below:

```{webr-r}
summary(lm(dist ~ speed, data = cars))$coef
```

That's a p-value of 0.0000000000015. To get a p-value this low in a permutation test, we'd need somewhere near 10000000000 permutations, and even then we might not get one outside of our critical region. It's not a surprise that our permutation test p-value was 0!

## Permute the response or the predictor(s)?

For the simple linear regression examples above, it doesn't matter. Once one is permuted, any existing relationship is destroyed.

For multiple linear regression, it suddenly matters a lot! 

- If we permute the response, it's severing the relationship between the response and *all* predictors. 
    - This is like an F-test for overall significance!!!
- If we permute *one* of the predictors, it severs the relationship between the response and that particular predictor, while *also* severing any multicollinearity!
    - This is often a good way to do things since it tests whether that predictor contributes to the regression, not just the response.
    - Watch how the other predictors' coefficients change!
- If we permute *all* of the predictors *together* then the relationship between y and x is removed, but all multicolinearity remains.
- If we permute the predictors individually, then there should be no relationships anywhere among the data. 

For the last two, we often just care about the response given our predictors; it doesn't often make sense to permute the predictors individually.

Note that there's a secret fifth option: permute *some* of the predictors together. This is like an Extra Sum-of-Squares test for a group of predictors!

Run the following code many times with `kind = "y"` and watch what changes. Change `kind` to `"x"` to see what changes, then also try `"xx"`. 

(Note: you can click on the last line and hit "Shit+Enter" to run the code cell without scrolling up and clicking the "Run Code" button.)

```{webr-r}
# Always get same data
set.seed(57362)
n <- 50

# Generate linearly related predictors.
x1 <- runif(n, -2, 2)
x2 <- 0.5 * x1 + rnorm(n, 0, 1)
x3 <- 1.5 * x1 + rnorm(n, 0, 1)

y <- 5 + x1 - 3 * x2 + 0.5 * x3 + rnorm(50, 0, 1)
mydf <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)

# Unset the seed to get different permutations
set.seed(as.numeric(Sys.time()))

# Assumes first column is y and there are three predictor columns
permute <- function(yxxx, kind = c("y", "x", "xx")[1]) {
    n <- nrow(yxxx)
    reshuffled_indices <- sample(1:n, n, replace = FALSE)
    if (kind == "y") {
        yxxx[, 1] <- yxxx[reshuffled_indices, 1]
    } else if (kind == "x") {
        yxxx[, 2] <- yxxx[reshuffled_indices, 2]
    } else if (kind == "xx") {
        yxxx[, 2:3] <- yxxx[reshuffled_indices, 2:3]
    }
    yxxx
}

pairs(permute(mydf, kind = "xx"))
```

- `kind = "y"` affects the response, without affecting multicollinearity.
- `kind = "x"` affects *all* relationships involving `x`
- `kind = "xx"` affects relationships involving `x1` *and* `x2`, but there's an important exception...

## Example: Permutation Test for Bill Depth

Is `bill_depth_mm` an important predictor in the Palmer Penguins data? 

The method for getting the answer depends on the model that we choose! In the following, we're modelling the body mass against the flipper length, bill length, and bill depth (all with an interaction term with species), and the the different islands are given different intercepts. This is by no means a "good" model - `flipper_length_mm * species` is a good enough model. However, this model has an interaction term that will persist if we take bill depth out of the equation, and so it's a good example of doing an ESS test for all parameters involving bill_depth. We also know that there's some multicollinearity in these data, which has implications for the permutation test.

```{webr-r}
# This will take a while the first time it's run in the browser
library(palmerpenguins)
peng <- na.omit(penguins)
mylm <- lm(
    formula = body_mass_g ~ species * (flipper_length_mm +
        bill_depth_mm + bill_length_mm) + island,
    data = peng)

obs_F <- summary(mylm)$fstatistic[1]

perm_F <- c()
for(i in 1:1000) {
    peng2 <- peng
    peng2$bill_depth_mm <- sample(
        peng2$bill_depth_mm, size = nrow(peng), replace = TRUE
    )
    # The update() function can also update the data set and re-fit
    newmod <- update(mylm, data = peng2)
    perm_F[i] <- summary(newmod)$fstatistic[1]
}

hist(perm_F, xlim = range(perm_F, obs_F))
abline(v = obs_F, col = "red")
```

That looks pretty definitively like `bill_depth_mm` affects the F statistic!

Compare this to the ESS test:

```{webr-r}
#| eval: FALSE
library(palmerpenguins)
peng <- na.omit(penguins)
mylm <- lm(
    formula = body_mass_g ~ species * (flipper_length_mm +
        bill_depth_mm + bill_length_mm) + island,
    data = peng)

mylm2 <- lm(
    formula = ???,
    data = peng)
anova(mylm2, mylm)
```

If you entered the formula correctly, you should get a p-value of 1.352e-11, which is also a pretty definitive difference.

If we wanted to do an ANOVA test for overall significance, we would have permuted `body_mass_g`. Try it out using the two code blocks above!
