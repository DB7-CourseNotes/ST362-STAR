[
  {
    "objectID": "Lb02-OLS_Estimates.html",
    "href": "Lb02-OLS_Estimates.html",
    "title": "Appendix A — OLS Estimates",
    "section": "",
    "text": "A.1 Analysis of Variance\nThe code below demonstrates how the ANOVA tables are calculated.\ny &lt;- mydata$y\nyhat &lt;- b0 + b1*mydata$x\nybar &lt;- mean(y)\n\nc(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n\n[1] 4809.33809   30.93852 4840.27661\n\nANOVA &lt;- data.frame(\n    Source = c(\"Regression\", \"Error\", \"Total (cor.)\"),\n    df = c(1, nrow(mydata) - 2, nrow(mydata)),\n    SS = c(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n)\nANOVA$MS &lt;- ANOVA$SS / ANOVA$df\nANOVA\n\n        Source df         SS          MS\n1   Regression  1 4809.33809 4809.338089\n2        Error 28   30.93852    1.104947\n3 Total (cor.) 30 4840.27661  161.342554\nThis is equivalent to what R’s built-in functions do!\nanova(lm(y ~ x, data = mydata))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 4809.3  4809.3  4352.5 &lt; 2.2e-16 ***\nResiduals 28   30.9     1.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>OLS Estimates</span>"
    ]
  },
  {
    "objectID": "Lb02-OLS_Estimates.html#dependence-and-centering",
    "href": "Lb02-OLS_Estimates.html#dependence-and-centering",
    "title": "Appendix A — OLS Estimates",
    "section": "A.2 Dependence and Centering",
    "text": "A.2 Dependence and Centering\nSomething not touched on in class is that \\(cov(\\hat\\beta_0, \\hat\\beta_1)\\) is not 0! This should be clear from the formula got \\(\\hat\\beta_0\\), which is \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\nThe code below repeats what we did before, but with higher variance to better demonstrate the problem.\nIt also records the estimates based on centering \\(\\underline x\\). Notice how the formula for \\(\\hat\\beta_0\\) is no longer dependent on \\(\\hat\\beta_1\\) if the mean of \\(\\underline x\\) is 0!\n\nb1s &lt;- double(R)\nb0s &lt;- double(R)\nb1cs &lt;- double(R)\nb0cs &lt;- double(R)\nn &lt;- 25\nx &lt;- runif(n, 0, 10)\nxc &lt;- x - mean(x) # centered\n\nfor (i in 1:R) {\n    y &lt;- 2 - 2 * x + rnorm(25, 0, 4)\n    b1 &lt;- Sdotdot(x, y) / Sdotdot(x, x)\n    b0 &lt;- mean(y) - b1 * mean(x)\n    b1s[i] &lt;- b1\n    b0s[i] &lt;- b0\n\n    # Centered\n    y &lt;- 2 - 2 * xc + rnorm(25, 0, 4)\n    b1c &lt;- Sdotdot(xc, y) / Sdotdot(xc, xc)\n    b1cs[i] &lt;- b1c\n    b0c &lt;- mean(y) - b1 * mean(xc)\n    b0cs[i] &lt;- b0c\n}\n\npar(mfrow = c(1,2))\nplot(b1s, b0s)\nplot(b1cs, b0cs)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>OLS Estimates</span>"
    ]
  },
  {
    "objectID": "Lb03-MSE.html",
    "href": "Lb03-MSE.html",
    "title": "Appendix B — Bias/variance of \\(\\sigma^2\\)",
    "section": "",
    "text": "B.1 Best estimator of \\(\\sigma^2\\)\nWe saw in the notes that \\(E(s^2) = \\sigma^2\\). Let’s explore why this might not be the best estimator.\nWe define the MSE of an estimator \\(\\theta\\) as \\(E((\\theta - \\hat\\theta^2)^2)\\). For the variance, this is \\(E((\\sigma^2 - s^2)^2)\\).\nLet’s simulate a bunch of linear models, calculate the standard deviations, and then calculate this quantity.\nWe’re going to focus on multiplicative bias, and you’ll see why at the end. This means that we’ll focus on estimators of the form \\(as^2\\).\nShow the code\nset.seed(2112)\n\nn &lt;- 30\nx &lt;- runif(n, 0, 10)\nbeta0 &lt;- -3\nbeta1 &lt;- -4\nsigma &lt;- 3\n\nreps &lt;- 1000\nesst &lt;- double(reps)\n\nfor (i in 1:reps) {\n    y &lt;- beta0 + beta1*x + rnorm(n, 0, sigma)\n    beta1 &lt;- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\n    beta0 &lt;- mean(y) - beta1 * mean(x)\n    yhat &lt;- beta0 + beta1 * x\n    e &lt;- y - yhat\n    esst[i] &lt;- sum(e^2)\n}\n\na &lt;- (n-4):(n+4)\nmse &lt;- double(length(a))\nbias2 &lt;- double(length(a))\nfor (i in seq_along(a)) {\n    mse[i] &lt;- mean((sigma^2 - esst / a[i])^2)\n    bias2[i] &lt;- (sigma^2 - mean(esst / a[i]))^2\n}\n\npar(mfrow = c(1,2))\nplot(a - n, mse, main = \"MSE = Bias^2 + Variance\")\nplot(a - n, bias2, main = \"Bias^2\")\nabline(h = 0)\nFrom these plots, we see that the lowest MSE, i.e. the lowest value of \\(E((\\sigma^2 - as^2)^2)\\), is at \\(a = 1/n\\). Note that this corresponds to the MLE of \\(\\sigma^2\\). However, this is a biased estimate, and the unbiased estimate occurs at \\(a=1/(n-2)\\).\nWhat’s happening here? Shouldn’t unbiased be best? Well, yes, if our criteria is minimizing bias! If we want to minimize \\(E((\\sigma^2 - \\hat\\sigma^2)^2)\\), we have to account for the variance of the estimator across all possible samples as well!\nHWK: Modify the code to show that the bias of the constant model (\\(\\beta_1 = 0\\)) is minimized at \\(n-1\\), with the MSE being minimized at \\(n+1\\). It’s bizarre, but that’s how it works!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Bias/variance of $\\sigma^2$</span>"
    ]
  },
  {
    "objectID": "Lb03-MSE.html#residuals",
    "href": "Lb03-MSE.html#residuals",
    "title": "Appendix B — Bias/variance of \\(\\sigma^2\\)",
    "section": "B.2 Residuals",
    "text": "B.2 Residuals\nThe plot.lm() function makes most of the plots you’ll need.\n\n\nShow the code\nmylm &lt;- lm(mpg ~ disp, data = mtcars)\nplot(mylm, which=1:6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe broom package will be very useful in the future. In particular, the augment() function results in a tidy data frame with columns that are very relevant to our analyses.\n\n\nShow the code\nlibrary(broom)\n\nhead(augment(mylm))\n\n\n# A tibble: 6 × 9\n  .rownames           mpg  disp .fitted .resid   .hat .sigma .cooksd .std.resid\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 Mazda RX4          21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n2 Mazda RX4 Wag      21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n3 Datsun 710         22.8   108    25.1  -2.35 0.0629   3.28 0.0187      -0.746\n4 Hornet 4 Drive     21.4   258    19.0   2.43 0.0328   3.27 0.00983      0.761\n5 Hornet Sportabout  18.7   360    14.8   3.94 0.0663   3.22 0.0558       1.25 \n6 Valiant            18.1   225    20.3  -2.23 0.0313   3.28 0.00782     -0.696\n\n\n\n\nShow the code\nglance(mylm)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.718         0.709  3.25      76.5 9.38e-10     1  -82.1  170.  175.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nShow the code\nanova(mylm)\n\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nShow the code\nx &lt;- rnorm(1000); qqnorm(x); qqline(x)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Bias/variance of $\\sigma^2$</span>"
    ]
  },
  {
    "objectID": "Lb04-R_Matrix_Form.html",
    "href": "Lb04-R_Matrix_Form.html",
    "title": "Appendix C — Verifying Matrix Identities",
    "section": "",
    "text": "C.1 Verifying Matrix Results\nWe’ll use the mtcars data for this. Here’s what it looks like:\nShow the code\nx &lt;- mtcars$disp\ny &lt;- mtcars$mpg\n\nplot(y ~ x)\nabline(lm(y ~ x))\nIt looks like the slope is negative, and the intercept will be somewhere between 25 and 35.\nLet’s use the formulae from the previous course: \\(\\hat\\beta_1 = S_{XY}/S_{XX}\\) and \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\nShow the code\nb1 &lt;- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\nb0 &lt;- mean(y) - mean(x) * b1\n\nmatrix(c(b0, b1))\n\n\n            [,1]\n[1,] 29.59985476\n[2,] -0.04121512\nTo make the matrix multiplication to work, we need \\(X\\) to be a column of 1s and a column representing our covariate.\nShow the code\nX &lt;- cbind(1, x)\nhead(X)\n\n\n         x\n[1,] 1 160\n[2,] 1 160\n[3,] 1 108\n[4,] 1 258\n[5,] 1 360\n[6,] 1 225\nThe estimates should be \\((X^TX)^{-1}X^T\\underline y\\). In R, we find the transpose with the t() function and we find inverse with the solve() function.\nShow the code\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta_hat\n\n\n         [,1]\n  29.59985476\nx -0.04121512\nIt works!\nNow let’s check the ANOVA table!\nShow the code\nn &lt;- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(2, n-2, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y)\n)\n\n\n      source df         SS\n1 Regression  2 13725.1513\n2      Error 30   317.1587\n3      Total 32 14042.3100\nShow the code\nanova(lm(y ~ x))\n\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx          1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nShow the code\ncolSums(anova(lm(y ~ x)))\n\n\n       Df    Sum Sq   Mean Sq   F value    Pr(&gt;F) \n  31.0000 1126.0472  819.4605        NA        NA\nThey’re slightly different? Why?\nBecause the equation in the textbook is for the uncorrected sum of squares, which basically means we’re looking at estimating both \\(\\beta_0\\) and \\(\\beta_1\\) at the same time (hence the df of \\(n-2\\)). The usual ANOVA table is the corrected sum of squares, which the textbook labels \\(SS(\\hat\\beta_1|\\hat\\beta_1)\\) to make it clear that it’s estimating \\(\\beta_1\\) only; \\(\\beta_0\\) has already been estimated.\nShow the code\nn &lt;- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(1, n-1, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y - n * mean(y)^2, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y - n * mean(y)^2)\n)\n\n\n      source df        SS\n1 Regression  1  808.8885\n2      Error 31  317.1587\n3      Total 32 1126.0472\nThe matrix form for \\(R^2\\) is a little different from what you might expect. It uses this idea of “corrected” sum-of-squares as well. For homework, verify that the corrected sum-of-squares works out to the same formula.\nHere’s how to extract the \\(R^2\\) value from R (note that the programming language R has nothing to do with the \\(R^2\\); R is named after S, which was the programming language that came before it (both chronologically and alphabetically); you’ll still find references to S and S-Plus).\nShow the code\nsummary(lm(y ~ x))$r.squared\n\n\n[1] 0.7183433\nIn the textbook, the formula is given as: \\[\nR^2 = \\frac{\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar y^2}{\\underline y^t\\underline y - n\\bar y^2}\n\\]\nShow the code\nnumerator &lt;- t(beta_hat) %*% t(X) %*% y - n * mean(y)^2\ndenominator &lt;- t(y) %*% y - n * mean(y)^2\nnumerator / denominator\n\n\n          [,1]\n[1,] 0.7183433",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Verifying Matrix Identities</span>"
    ]
  },
  {
    "objectID": "Lb04-R_Matrix_Form.html#section",
    "href": "Lb04-R_Matrix_Form.html#section",
    "title": "Appendix C — Verifying Matrix Identities",
    "section": "C.2 ",
    "text": "C.2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Verifying Matrix Identities</span>"
    ]
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html",
    "href": "Lb05-ANOVA-R2-F_test-CI.html",
    "title": "Appendix D — ANOVA",
    "section": "",
    "text": "D.1 Basic ANOVA\nShow the code\nset.seed(2221)\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\nShow the code\nanova(lm(mpg ~ qsec, data = mtcars))\n\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nqsec       1 197.39 197.392  6.3767 0.01708 *\nResiduals 30 928.66  30.955                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nShow the code\nsummary(lm(mpg ~ qsec, data = mtcars))$coef\n\n\n             Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept) -5.114038 10.0295433 -0.5098974 0.61385436\nqsec         1.412125  0.5592101  2.5252133 0.01708199\nNotice the p-values! Also notice that the \\(F\\)-value is the square of the \\(t\\)-value! It’s like magic! Math is cool.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#r2-always-increases-with-new-predictors",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#r2-always-increases-with-new-predictors",
    "title": "Appendix D — ANOVA",
    "section": "D.2 \\(R^2\\) always increases with new predictors",
    "text": "D.2 \\(R^2\\) always increases with new predictors\n\n\nShow the code\nnx &lt;- 10 # Number of uncorrelated predictors\nuncorr &lt;- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) &lt;- c(\"y\", paste0(\"x\", 1:nx))\nuncorr &lt;- as.data.frame(uncorr)\n\nrsquares &lt;- NA\nfor (i in 2:(nx + 1)) {\n    rsquares &lt;- c(rsquares,\n        summary(lm(y ~ ., data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n\n\n\n\n\n\n\n\n\nThe one exception is when one of the predictors is a linear combination of the previous predictors. In this case, \\(R^2\\) will not change!\n\n\nShow the code\nuncorr[, nx + 2] &lt;- uncorr[,2] + 3*uncorr[,3]\nrsquares &lt;- c(rsquares, summary(lm(y ~ ., data = uncorr))$r.squared)\nrsquares\n\n\n [1]          NA 0.003898013 0.056398466 0.056407225 0.069142075 0.073155890\n [7] 0.105824965 0.122449599 0.145307322 0.168746574 0.172758068 0.172758068\n\n\nShow the code\nplot(rsquares, type = \"b\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nx &lt;- runif(20, 0, 10)\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nb0s &lt;- b1s &lt;- double(1000)\nplot(NA, pch = 0, \n    xlim = c(-2, 12), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nabline(h = b0 + b1*mean(x))\nabline(v = mean(x))\nfor (i in 1:1000) {\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    abline(lm(y ~ x), col = rgb(0,0,0,0.1))\n}\n\n\n\n\n\n\n\n\n\nLet’s do that again, but record the values and only show the 89% quantiles!\n\n\nShow the code\nx &lt;- runif(20, 0, 10)\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_lines &lt;- replicate(1000, {\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine &lt;- apply(all_lines, 1, quantile, probs = c(0.045, 0.945))\n\n\n\n\nShow the code\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n\n\n\n\n\n\n\n\nNote that the theoretical calculation of these bounds is built into R:\n\n\nShow the code\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n## Add the TRUE relationship\nxseq &lt;- seq(0, 10, 0.1)\nlines(xseq, b0 + b1*xseq, col = 3)\n\n## New sample from the data generating process\nx &lt;- runif(20, 0, 10)\ny &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n\n## Extract the CI\nmylm &lt;- lm(y ~ x)\nxbeta &lt;- predict(mylm, interval = \"confidence\",\n    newdata = list(x = xseq))\n#lines(xseq, xbeta[,\"fit\"], col = 4)\nlines(xseq, xbeta[,\"upr\"], col = 4)\nlines(xseq, xbeta[,\"lwr\"], col = 4)\n\n\n\n\n\n\n\n\n\nNote that the intervals won’t exactly align - the samples are going to be different each time! In 95% of the samples we collect from this data generating process, the CI we construct from the sample will contain the true (green) line. This is a basic definition for confidence intervals, but it’s neat to see it around a line.\nNotice how the CI is curved. This is completely, 100% expected. Recall that \\((\\bar x, \\bar y)\\) is always a point on the line. If \\(x\\) is the same for all samples, then the variance in the height at \\(\\bar y\\) is just the variance in \\(y\\). However, we can rotate the line around this point and still fit most of the data “pretty well”, which is where the curved nature of the line comes from!\n\nAside\nWhy did I use the same \\(x\\) values for all of the simulations? Because that’s part of the assumptions (this isn’t an important point to make). Again, notice how the point \\((\\bar x, \\bar y)\\) is always on the line, and how the variance at the point \\(\\bar x\\) is minimized. If \\(\\bar x\\) is randomly moved, then there’s extra variance in the line.\n\n\nShow the code\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_lines2 &lt;- replicate(1000, {\n    x &lt;- runif(20, 0, 10)\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine2 &lt;- apply(all_lines2, 1, quantile, probs = c(0.045, 0.945))\n\n\n\n\nShow the code\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\nlines(seq(0, 10, 0.1), eightnine2[1, ], col = 2)\nlines(seq(0, 10, 0.1), eightnine2[2, ], col = 2)\nlegend(\"topleft\", legend = c(\"non-random x\", \"random x\"), col = 1:2, lty = 1)\n\n\n\n\n\n\n\n\n\nThe textbook for this course also covers models that incorporate randomness in \\(X\\), but this is not covered in this course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#covariance-of-the-parameters",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#covariance-of-the-parameters",
    "title": "Appendix D — ANOVA",
    "section": "D.3 Covariance of the parameters",
    "text": "D.3 Covariance of the parameters\n\n\nShow the code\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_params &lt;- replicate(1000, {\n    x &lt;- runif(20, 0, 10)\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    coef(lm(y ~ x))\n})\n\n\n\n\nShow the code\nplot(t(all_params))\n\n\n\n\n\n\n\n\n\nIntuition check: were you expecting a negative slope? Does this make sense? If you increase \\(\\beta_0\\), why would \\(\\beta_1\\) decrease?\nFor homework, try a negative intercept and see what happens! What about a negative slope?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#joint-normality-of-the-underlinehatbetas",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#joint-normality-of-the-underlinehatbetas",
    "title": "Appendix D — ANOVA",
    "section": "D.4 Joint Normality of the \\(\\underline{\\hat\\beta}\\)s",
    "text": "D.4 Joint Normality of the \\(\\underline{\\hat\\beta}\\)s\nJoint normality leads to marginal normality! This means we can create a confidence interval based on the marginal. However, if the joint distribution has a strong correlation, the marginal confidence intervals might contain unlikely points!\n\n\nShow the code\npar(mfrow = c(1, 3))\n\n## Marginal distribution of beta_0\nplot(density(all_params[1,]),\n    main = \"Distribution of b_0\",\n    xlab = \"b_0\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(v = 8, col = 3) # Hypothesized beta_0\n\n## Marginal distribution of beta_1\nplot(density(all_params[2,]),\n    main = \"Distribution of b_1\",\n    xlab = \"b_1\")\nabline(v = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\nabline(v = 6, col = 3) # Hypothesized beta_1\n\n## Joint distribution\nplot(t(all_params), main = \"Joint Distribution\",\n    xlab = \"b_0\", ylab = \"b_1\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(h = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\npoints(x = 8, y = 6, col = 3, pch = 16, cex = 1.5)\n\n\n\n\n\n\n\n\n\nNotice how the rectangular confidence region in the joint distribution contains regions where there are no points! For example, a hypothesis test for whether \\(\\beta_0=8\\) and \\(\\beta_1 = 6\\) (the green lines/points) would not be rejected if we checked the two confidence intervals separately, but likely should be rejected given the joint distribution! This is exactly what happens when the F-test is significant but none of the t-tests for individual predictors is significant.\nIn general, the CIs for each individual \\(\\hat\\beta\\) are missing something - especially if there’s correlation in the predictors!\nIn these examples, we repeatedly sampled from the true relationship to obtain simulation-based confidence intervals. The normality assumption allows us to make inferences about the distribution of the parameters - including the joint distribution - from a single sample! Inference is powerful!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html",
    "href": "Lb11-R_hat_resids_cook.html",
    "title": "Appendix E — Hat Matrix and Residuals in R",
    "section": "",
    "text": "The Hat Matrix\n\\[\nH = X(X^TX)^{-1}X^T\n\\]\nIn R, we can calculate the diagonal of the hat matrix as follows:\n\n\nShow the code\nmylm &lt;- lm(mpg ~ disp + wt, data = mtcars)\nhatvalues(mylm) |&gt; unname()\n\n\n [1] 0.04339369 0.04550894 0.06309504 0.03877647 0.14078260 0.04406584\n [7] 0.11516157 0.09635365 0.09875274 0.11012510 0.11012510 0.08141444\n[13] 0.04168379 0.04521644 0.17206264 0.19889125 0.19275897 0.08015728\n[19] 0.12405357 0.09579747 0.05703451 0.06246825 0.05648077 0.06838477\n[25] 0.14119998 0.08720679 0.07149742 0.16032953 0.18794989 0.05044456\n[31] 0.04474121 0.07408572\n\n\n\n\nExtracting the Diagonals from H\nThere isn’t a built-in function for the full hat matrix (the diagonals are usually all you’ll need). For demonstration, here are some demonstrations of the features of the hat matrix.\n\n\nShow the code\nX &lt;- model.matrix(mylm)\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\nall.equal(diag(H), hatvalues(mylm))\n\n\n[1] TRUE\n\n\n\n\nFeatures of H\nIn the code below, I use the unname() function because mtcars has rownames which make the output harder to see (this used to be the norm in R, but it’s fallen out of fashion).\n\n\nShow the code\ncolSums(H) |&gt; unname()\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nShow the code\nrowSums(H) |&gt; unname()\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nShow the code\nrange(H) # -1 &lt;= h_{ij} &lt;= 1\n\n\n[1] -0.1076609  0.1988913\n\n\nShow the code\nrange(diag(H)) # 0 &lt;= h_{ii} &lt;= 1\n\n\n[1] 0.03877647 0.19889125\n\n\nShow the code\nH %*% rep(1, ncol(H)) # H1 = 1\n\n\n                    [,1]\nMazda RX4              1\nMazda RX4 Wag          1\nDatsun 710             1\nHornet 4 Drive         1\nHornet Sportabout      1\nValiant                1\nDuster 360             1\nMerc 240D              1\nMerc 230               1\nMerc 280               1\nMerc 280C              1\nMerc 450SE             1\nMerc 450SL             1\nMerc 450SLC            1\nCadillac Fleetwood     1\nLincoln Continental    1\nChrysler Imperial      1\nFiat 128               1\nHonda Civic            1\nToyota Corolla         1\nToyota Corona          1\nDodge Challenger       1\nAMC Javelin            1\nCamaro Z28             1\nPontiac Firebird       1\nFiat X1-9              1\nPorsche 914-2          1\nLotus Europa           1\nFord Pantera L         1\nFerrari Dino           1\nMaserati Bora          1\nVolvo 142E             1\n\n\n\n\nExtracting the residuals\nSee ?influence.measures.\n\n\nShow the code\n?rstandard\ncooks.distance(mylm)\n\n\n\n\nThe broom package\nThe broom package has a wonderful function called augment(). This function sets up our data so that it’s super easy to see what we need.\n\n\nShow the code\nlibrary(broom)\naugment(mylm)\n\n\n# A tibble: 32 × 10\n   .rownames     mpg  disp    wt .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 Mazda RX4    21    160   2.62    23.3 -2.35  0.0434   2.93 0.0102      -0.822\n 2 Mazda RX4 …  21    160   2.88    22.5 -1.49  0.0455   2.95 0.00435     -0.523\n 3 Datsun 710   22.8  108   2.32    25.3 -2.47  0.0631   2.93 0.0172      -0.876\n 4 Hornet 4 D…  21.4  258   3.22    19.6  1.79  0.0388   2.95 0.00524      0.624\n 5 Hornet Spo…  18.7  360   3.44    17.1  1.65  0.141    2.95 0.0203       0.609\n 6 Valiant      18.1  225   3.46    19.4 -1.28  0.0441   2.96 0.00309     -0.448\n 7 Duster 360   14.3  360   3.57    16.6 -2.32  0.115    2.93 0.0309      -0.845\n 8 Merc 240D    24.4  147.  3.19    21.7  2.73  0.0964   2.92 0.0344       0.984\n 9 Merc 230     22.8  141.  3.15    21.9  0.890 0.0988   2.96 0.00378      0.322\n10 Merc 280     19.2  168.  3.44    20.5 -1.26  0.110    2.96 0.00869     -0.459\n# ℹ 22 more rows\n\n\nNotice that it includes:\n\n.fitted = \\(X\\underline{\\hat\\beta} = \\hat Y\\)\n.resid = \\(\\hat{\\underline\\epsilon}\\)\n.hat = \\(diag(H)\\)\n.sigma = \\(s_{(i)}\\)\n.cooksd = D\n.std.resid = are these standardized or studentized residuals? Find out yourself as homework!!\n\n\n\nPlotting the residuals\nIf you’ve ever accidentally typed plot(mylm), you’ve seen some plots of the residuals.\n\n\nShow the code\npar(mfrow = c(2, 2)) # plot.lm produces four plots.\nplot(mylm) # ?plot.lm\n\n\n\n\n\n\n\n\n\nYou can access individual plots with the which argument.\n\n\nShow the code\npar(mfrow = c(2, 3))\nplot(mylm, which = 1:6)\n\n\n\n\n\n\n\n\n\n99% of the time, the default plots are the ones you’ll want to look at. For teaching purposes, we’ll look at the extra.\n\n\nDemonstrations\nWe’ll use the Ozone data from the mlbench package.\n\nV4 is response (measurement of Ozone)\nV5 is atmospheric pressure\nV6 is wind speed\nV7 is humidity\nWe’ll ignore the rest.\n\n\n\nShow the code\nlibrary(mlbench)\ndata(Ozone)\nstr(Ozone) # V4 is response (measurement of Ozone)\n\n\n'data.frame':   366 obs. of  13 variables:\n $ V1 : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ V2 : Factor w/ 31 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ V3 : Factor w/ 7 levels \"1\",\"2\",\"3\",\"4\",..: 4 5 6 7 1 2 3 4 5 6 ...\n $ V4 : num  3 3 3 5 5 6 4 4 6 7 ...\n $ V5 : num  5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ...\n $ V6 : num  8 6 4 3 3 4 6 3 3 3 ...\n $ V7 : num  20 NA 28 37 51 69 19 25 73 59 ...\n $ V8 : num  NA 38 40 45 54 35 45 55 41 44 ...\n $ V9 : num  NA NA NA NA 45.3 ...\n $ V10: num  5000 NA 2693 590 1450 ...\n $ V11: num  -15 -14 -25 -24 25 15 -33 -28 23 -2 ...\n $ V12: num  30.6 NA 47.7 55 57 ...\n $ V13: num  200 300 250 100 60 60 100 250 120 120 ...\n\n\nShow the code\nOzone &lt;- Ozone[complete.cases(Ozone), ]\n\n\nA small amount of exploration first:\n\n\nShow the code\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\ng5 &lt;- ggplot(Ozone) +\n    aes(x = V5, y = V4) +\n    geom_point()\ng6 &lt;- ggplot(Ozone) +\n    aes(x = V6, y = V4) +\n    geom_jitter() # deal with overlapping points\ng7 &lt;- ggplot(Ozone) +\n    aes(x = V7, y = V4) +\n    geom_point()\ng5 + g6 + g7\n\n\n\n\n\n\n\n\n\nNow let’s check some residuals!\n\nChange .resid to .std.resid.\n\nTry rstudent(olm) as well.\n\nChange .hat to .cooksd.\n\n\n\nShow the code\nlibrary(dplyr)\nolm &lt;- lm(V4 ~ V5 + V6 + V7, data = Ozone)\naugment(olm) %&gt;%\n    ggplot() +\n        aes(x = .fitted, y = .std.resid, col = .cooksd) +\n        scale_colour_viridis_c(option = 2, end = 0.7) +\n        theme(legend.position = \"bottom\") +\n        geom_point(size = 2) +\n        geom_hline(yintercept = 0, colour = \"grey\")\n\n\n\n\n\n\n\n\n\nWhich ones have a large hat value?\nThe plots below are the same as the ones above, but coloured according to the hat values.\n\n\nShow the code\ng5 &lt;- ggplot(Ozone) +\n    aes(x = V5, y = V4, col = hatvalues(olm)) +\n    geom_point(size = 2)\ng6 &lt;- ggplot(Ozone) +\n    aes(x = V6, y = V4, col = hatvalues(olm)) +\n    geom_jitter(size = 2) # deal with overlapping points\ng7 &lt;- ggplot(Ozone) +\n    aes(x = V7, y = V4, col = hatvalues(olm)) +\n    geom_point(size = 2)\n(g5 + g6 + g7) +\n    plot_layout(guides = \"collect\") &\n    scale_colour_viridis_c(option = 2, end = 0.8) &\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nAdding an Outlier\nLet’s add an outlier to see what happens with these data.\n\n\nShow the code\nnewzone &lt;- Ozone[, c(4:7)]\nnewzone &lt;- rbind(newzone,\n    data.frame(V4 = 30, V5 = 5300, V6 = 5, V7 = 40))\nnewlm &lt;- augment(lm(V4 ~ ., data = newzone))\n\ng5 &lt;- ggplot(newlm) +\n    aes(x = V5, y = V4, col = .hat) +\n    geom_point(size = 2)\ng6 &lt;- ggplot(newlm) +\n    aes(x = V6, y = V4, col = .hat) +\n    geom_jitter(size = 2) # deal with overlapping points\ng7 &lt;- ggplot(newlm) +\n    aes(x = V7, y = V4, col = .hat) +\n    geom_point(size = 2)\n(g5 + g6 + g7) +\n    plot_layout(guides = \"collect\") &\n    scale_colour_viridis_c(option = 2, end = 0.8) &\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nUsing R’s Built-In Diagnostics\n\n\nShow the code\npar(mfrow = c(2, 2), mar = rep(2, 4))\nplot(lm(V4 ~ ., data = newzone))\n\n\n\n\n\n\n\n\n\nShow the code\nnewzone[row.names(newzone) %in% c(1, 58, 243), ]\n\n\n    V4   V5 V6 V7\n58  23 5740  3 47\n243 38 5950  5 62\n1   30 5300  5 40\n\n\n\nHuge residual!\n\nThis plot also just has a bad pattern\n\nDeviates from normality!\n\nOtherwise this looks pretty good.\n\nLarge standardized residual\n\nClear pattern without the outlier\n\nCook’s distance is massive compared to the others\n\nPotentially some large \\(D_i\\)’s\n\nIn the corner\n\nOtherwise this looks okay-ish\n\nLast plot also shows it as something different (harder to interpret)\n\n\n\nWhat to do with a large residual?\n\nMisrecorded: remove or fix, if possible\n\nFires with negative lengths (MDY versus DMY)\nCO2 measured as -99 (code for NA in a system with no NA option)\nHeights measured in the wrong units\n\nReal, but large residual: Consider whether it’s actually part of the population of interest\n\nStudying heights and got a basketball player in your sample? That’s a real data point and your model should allow for it!\nStudying fish and a shark was included? That’s real, but maybe you should narrow your scope!\n\nMany large outliers: you may need to try more predictors or a non-linear model.\n\nDO NOT remove a point simply because it’s an outlier!!!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hat Matrix and Residuals in R</span>"
    ]
  },
  {
    "objectID": "Lb13-Wrong_Model.html",
    "href": "Lb13-Wrong_Model.html",
    "title": "Appendix G — Lab Missing/Extra Predictors",
    "section": "",
    "text": "G.1 Missing Predictors\nWe’re going to do this a little differently than other days. Let’s look at the penguins data again:\nShow the code\nset.seed(2121)\nlibrary(palmerpenguins)\n## Remove NAs and take continuous variables\npenguins &lt;- subset(penguins, species == \"Chinstrap\")\npeng &lt;- penguins[complete.cases(penguins), c(3, 4, 5, 6)]\n## Standardize the x values\n#peng[, 1:3] &lt;- apply(peng[, 1:3], 2, scale)\nhead(peng)\n\n\n# A tibble: 6 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1           46.5          17.9               192        3500\n2           50            19.5               196        3900\n3           51.3          19.2               193        3650\n4           45.4          18.7               188        3525\n5           52.7          19.8               197        3725\n6           45.2          17.8               198        3950\nLet’s “““make”“” a true model:\nShow the code\npenglm &lt;- lm(body_mass_g ~ ., data = peng)\nbeta &lt;- coef(penglm)\nsigma &lt;- summary(penglm)$sigma\n\nbeta\n\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n      -3157.53005          16.03916          91.51275          22.57975 \n\n\nShow the code\nsigma\n\n\n[1] 276.9998\nWe’ll use these values as if they are population values and forget that they were calculated from a sample.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lab Missing/Extra Predictors</span>"
    ]
  },
  {
    "objectID": "Lb13-Wrong_Model.html#missing-predictors",
    "href": "Lb13-Wrong_Model.html#missing-predictors",
    "title": "Appendix G — Lab Missing/Extra Predictors",
    "section": "",
    "text": "True model: \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\)\nEstimated model: \\(Y = X\\underline\\beta + \\underline\\epsilon\\)\n\n\n\n\n\n\n\nThe x-values will stay the same, we’ll simulate new \\(y\\) values according to this model.\nThe advantage of this approach is that the predictors retain any correlation that they had!\n\nBy simulating from real data, we have correlations but we match the linear modelling assumptions perfectly!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lab Missing/Extra Predictors</span>"
    ]
  },
  {
    "objectID": "Lb13-Wrong_Model.html#simulating-from-the-right-model",
    "href": "Lb13-Wrong_Model.html#simulating-from-the-right-model",
    "title": "Appendix G — Lab Missing/Extra Predictors",
    "section": "G.2 Simulating from the “Right” model",
    "text": "G.2 Simulating from the “Right” model\nLet’s forget the actual values of body_mass_g, and pretend that this is the true relationship: \\[\nbodymass = 4207 + 18*billlength + 35*billdepth + 711.5*flipperlength + \\epsilon\n\\] where \\(\\epsilon_i \\sim N(0, 393)\\).\nWe can simulate from this as follows:\n\n\nShow the code\nX &lt;- cbind(1, as.matrix(peng[, 1:3]))\nn &lt;- nrow(X)\nbody_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n\nunname(beta)\n\n\n[1] -3157.53005    16.03916    91.51275    22.57975\n\n\nShow the code\nunname(coef(lm(body_mass_g ~ -1 + X)))\n\n\n[1] -3311.31867    16.27829   120.41292    20.77560\n\n\nShow the code\nunname(coef(lm(body_mass_g ~ X[, -1])))\n\n\n[1] -3311.31867    16.27829   120.41292    20.77560\n\n\nNow let’s do this 1000s of times!\n\n\nShow the code\nres &lt;- matrix(ncol = 4, nrow = 0)\nfor (i in 1:10000) {\n    body_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n    right_lm &lt;- lm(body_mass_g ~ -1 + X)\n    res &lt;- rbind(res, unname(coef(right_lm)))\n}\n\ndim(res)\n\n\n[1] 10000     4\n\n\n\n\nShow the code\npar(mfrow = c(2, 2))\nfor (i in 1:4) {\n    hist(beta[i] - res[, i],\n        main =paste0(\"Bias = \", round(beta[i], 2), \" - \",\n            round(mean(res[, i]), 2), \" = \",\n            round(beta[i] - mean(res[, i]), 2)))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nThis looks good - we simulated according to the values in beta, and we were able to recover them. We’ve also shown that the linear model is unbiased!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lab Missing/Extra Predictors</span>"
    ]
  },
  {
    "objectID": "Lb13-Wrong_Model.html#estimating-the-wrong-model---too-few-predictors",
    "href": "Lb13-Wrong_Model.html#estimating-the-wrong-model---too-few-predictors",
    "title": "Appendix G — Lab Missing/Extra Predictors",
    "section": "G.3 Estimating the Wrong Model - Too few predictors",
    "text": "G.3 Estimating the Wrong Model - Too few predictors\nIn the following code, I remove the “flipper_length_mm” (the third predictor) by only taking the first three columns of X, which includes the column of 1s.\nI then fit the model without flipper length, which we’ve seen before is an important predictor!\n\n\nShow the code\nres_reduced &lt;- matrix(ncol = 3, nrow = 0)\nX_reduced &lt;- X[, 1:3] # Still includes column of 1s\nbeta_reduced &lt;- beta[1:3]\nfor (i in 1:10000) {\n    # Simulate from the correct model\n    body_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n    # Only estimate beta 0-3 (not beta4)\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X_reduced)\n    res_reduced &lt;- rbind(res_reduced, unname(coef(wrong_lm)))\n}\n\ndim(res_reduced)\n\n\n[1] 10000     3\n\n\n\n\nShow the code\npar(mfrow = c(2, 2))\nfor (i in 1:3) {\n    bias &lt;- beta[i] - res_reduced[, i]\n    hist(bias, \n        main = paste0(\"Bias = \", round(beta[i], 2), \" - \",\n            round(mean(res_reduced[, i]), 2), \" = \",\n            round(mean(bias), 2)),\n        xlim = range(0, bias))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nEverything is biased! Since flipper_length_mm was an important predictor, the estimates from the other predictors are biased!\nHere’s how I like to think of this: the machine is trying to learn a pattern using the predictors we give it. These other predictors are trying to pick up on as much pattern as possible. Without the true pattern, they have to adjust.\nA big part of this comes from the fact that there’s correlation in the predictors. Since they’re correlated, if one is missing then the others can find the pattern through their correlation. Instead of flipper_length_mm causing a change in body mass, flipper_length_mm is correlated with bill_length_mm and bill_depth_mm, which then affect body_mass_g in place of flipper_length_m’s affect. In other words, they’re trying to make up for missing patterns through the correlation, like a game of telephone where information has been lost along the way.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lab Missing/Extra Predictors</span>"
    ]
  },
  {
    "objectID": "Lb13-Wrong_Model.html#too-many-predictors",
    "href": "Lb13-Wrong_Model.html#too-many-predictors",
    "title": "Appendix G — Lab Missing/Extra Predictors",
    "section": "G.4 Too Many Predictors",
    "text": "G.4 Too Many Predictors\nWhat happens if we include predictors that aren’t correlated with the response?\nBefore we run this code, what do you expect?\nRecall our results when \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\):\n\\[\n\\begin{align*}\nE(\\hat{\\underline\\beta}) &= E((X^TX)^{-1}X^TY)\\\\\n& = (X^TX)^{-1}X^T(X\\underline\\beta + X_2\\underline\\beta_2) \\\\\n& = (X^TX)^{-1}X^TX\\underline\\beta + (X^TX)^{-1}X^TX_2\\underline\\beta_2 \\\\\n&= \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\\\\\n&= \\underline\\beta + A\\underline\\beta_2\n\\end{align*}\n\\]\nThis isn’t directly applicable, but might help you think about what happens when \\(\\underline\\beta\\) is too big.\nSince we already have the objects created, let’s pretend that X_reduced is correct.\n\n\nShow the code\nres &lt;- matrix(ncol = 4, nrow = 0)\nX_reduced &lt;- X[, 1:3] # Still includes column of 1s\nbeta_reduced &lt;- beta[1:3]\nfor (i in 1:10000) {\n    # Simulate from the correct (smaller) model\n    body_mass_g &lt;- X_reduced %*% beta_reduced + rnorm(n, 0, sigma)\n    # Estimate the wrong model\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X)\n    res &lt;- rbind(res, unname(coef(wrong_lm)))\n\n}\n\npar(mfrow = c(2, 2))\nfor (i in 1:3) {\n    bias &lt;- beta_reduced[i] - res[, i]\n    hist(bias,\n        main =paste0(\"Bias = \", round(beta[i], 2), \" - \",\n            round(mean(res[, i]), 2), \" = \",\n            round(mean(bias), 2)),\n        xlim = range(0, bias))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nIt’s unbiased! In this case, the estimate of \\(\\beta\\) for flipper_length_mm is 0, and it’s successfully estimating this:\n\n\nShow the code\nhist(res[, 4])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lab Missing/Extra Predictors</span>"
    ]
  },
  {
    "objectID": "Lb13-Wrong_Model.html#too-many-and-too-few",
    "href": "Lb13-Wrong_Model.html#too-many-and-too-few",
    "title": "Appendix G — Lab Missing/Extra Predictors",
    "section": "G.5 Too many and too few",
    "text": "G.5 Too many and too few\nSo let’s get to the final case. As we know, bill length and bill depth are correlated:\n\n\nShow the code\ncor(X[, -1]) # correlation matrix, without column of 1s\n\n\n                  bill_length_mm bill_depth_mm flipper_length_mm\nbill_length_mm         1.0000000     0.6535362         0.4716073\nbill_depth_mm          0.6535362     1.0000000         0.5801429\nflipper_length_mm      0.4716073     0.5801429         1.0000000\n\n\nLet’s simulate with the coefficient for bill depth as 0, but include it in the model.\n\n\nShow the code\nprint(beta)\n\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n      -3157.53005          16.03916          91.51275          22.57975 \n\n\nShow the code\nprint(head(X))\n\n\n       bill_length_mm bill_depth_mm flipper_length_mm\n[1,] 1           46.5          17.9               192\n[2,] 1           50.0          19.5               196\n[3,] 1           51.3          19.2               193\n[4,] 1           45.4          18.7               188\n[5,] 1           52.7          19.8               197\n[6,] 1           45.2          17.8               198\n\n\nTo be clear:\n\nData Generating Process: body mass = \\(\\beta_0\\) + \\(\\beta_1\\) bill length + \\(\\beta_3\\) flipper length\nEstimating: body mass = \\(\\beta_0\\) + \\(\\beta_2\\) bill depth + \\(\\beta_3\\) flipper length\n\n\n\nShow the code\nres &lt;- matrix(ncol = 3, nrow = 0)\nbeta_fewmany &lt;- beta\nbeta_fewmany[3] &lt;- 0 # True coefficient for depth is 0, length != 0\nX_fewmany &lt;- X[, c(1, 3, 4)] # estimating depth, not length\n\nfor (i in 1:10000) {\n    # Simulate from the correct (smaller) model\n    body_mass_g &lt;- X %*% beta_fewmany + rnorm(n, 0, sigma)\n    # Estimate the wrong model\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X_fewmany)\n    res &lt;- rbind(res, unname(coef(wrong_lm)))\n\n}\n\npar(mfrow = c(2, 2))\nhist(res[, 1],\n    main = paste0(\"bias=\", round(beta[1], 2),\n        \"-\", round(mean(res[, 1]), 2),\n        \"=\", round(beta[1] - mean(res[, 1]), 2)))\nabline(v = beta[1], col = 2, lwd = 2)\n\nhist(res[, 2])\nabline(v = 0, col = 2, lwd = 2)\n\nhist(res[, 3],\n    main = paste0(\"bias=\", round(beta[4], 2),\n        \"-\", round(mean(res[, 3]), 2),\n        \"=\", round(beta[4] - mean(res[, 3]), 2)))\nabline(v = beta[4], col = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\n\nIt looks like flipper length is unbiased\n\nTechnically, it isn’t, but in this case it’s a small bias.\nIf we were primarily interested in flipper length, misspecifying bill length/depth isn’t so bad.\n\nThe estimate of bill_depth isn’t 0, but also doesn’t correspond to anything in the DGP!\n\nIt’s called a “proxy measure”, and the coefficient must be interpreted carefully.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lab Missing/Extra Predictors</span>"
    ]
  },
  {
    "objectID": "Lb13-Wrong_Model.html#summary",
    "href": "Lb13-Wrong_Model.html#summary",
    "title": "Appendix G — Lab Missing/Extra Predictors",
    "section": "G.6 Summary",
    "text": "G.6 Summary\nChoosing the right subset of predictors can be HARD!\n\nMissing predictors means your estimates are biased\nToo many predictors isn’t as bad of an issue\n\nOverfitting!\n\nThe wrong subset means no relation to DGP\n\nCan still give (nearly) unbiased estimates for predictors of interest.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lab Missing/Extra Predictors</span>"
    ]
  },
  {
    "objectID": "Lb15-Transformations.html",
    "href": "Lb15-Transformations.html",
    "title": "Appendix H — Transformations",
    "section": "",
    "text": "H.1 Finding a model for mpg ~ disp",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "Lb15-Transformations.html#finding-a-model-for-mpg-disp",
    "href": "Lb15-Transformations.html#finding-a-model-for-mpg-disp",
    "title": "Appendix H — Transformations",
    "section": "",
    "text": "Polynomial Models\nFirst, let’s consider the polynomial models from previous lecture:\n\n\nShow the code\nplot(mpg ~ disp, data = mtcars)\n\n\n\n\n\n\n\n\n\nA reasonable model for this looks to be \\(mpg_i = \\beta_0 + \\beta_1disp_i + \\beta_{11}disp_i^2\\):\n\n\nShow the code\nplot(mpg ~ disp, data = mtcars)\ndisp_seq &lt;- seq(min(mtcars$disp), max(mtcars$disp), 0.1)\npoly_seq &lt;- lm(mpg ~ poly(disp, 2), data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, poly_seq)\n\n\n\n\n\n\n\n\n\nIt fits! However, the point of the polynomial lecture was that polynomials are tempting, but must be justified by theory. I’m not sure it’s reasonable to assume that the fuel efficiency is proportional to the square of the displacement.\nMaybe a transformation will help?\n\n\nTransformations\nLet’s try the two main transformations we talked about in class.\n\n\nShow the code\npar(mfrow = c(2,2))\ndisp_seq &lt;- seq(min(mtcars$disp), max(mtcars$disp), 0.1)\nlm_seq &lt;- lm(mpg ~ disp, data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\n\nplot(mpg ~ disp, data = mtcars,\n    main = \"Polynomial Model\")\npoly_seq &lt;- lm(mpg ~ poly(disp, 2), data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, lm_seq, col = \"#4a4a4a\", lty = 2)\nlines(disp_seq, poly_seq, col = 2, lwd = 2)\n\nplot(sqrt(mpg) ~ disp, data = mtcars,\n    main = \"Square Root Transfomation\")\nsqrt_seq &lt;- lm(sqrt(mpg) ~ disp, data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, sqrt(lm_seq), col = \"#4a4a4a\", lty = 2)\nlines(disp_seq, sqrt_seq, col = 2, lwd = 2)\n\nplot(log(mpg) ~ disp, data = mtcars,\n    main = \"Log Transfomation\")\nlog_seq &lt;- lm(log(mpg) ~ disp, data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, log(lm_seq), col = \"#4a4a4a\", lty = 2)\nlines(disp_seq, log_seq, col = 2, lwd = 2)\n\nplot(exp(mpg) ~ disp, data = mtcars,\n    main = \"Exponential Transfomation\")\nexp_seq &lt;- lm(exp(mpg) ~ disp, data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, exp(lm_seq), col = \"#4a4a4a\", lty = 2)\nlines(disp_seq, exp_seq, col = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\nI also added the linear model, transformed to the scale of the data. Notice how the linear model is curved on these non-linear scales!\nIn the end, the log-transform is actually pretty good! Let’s evaluate that one!\nI’m going to add log_mpg as a column in mtcars because we’re only going to be working on that scale. With transformations, all of our assumptions about the residuals are on the transformed scale!!! This is important!!!\n\n\nShow the code\nmtcars$log_mpg &lt;- log(mtcars$mpg)\n\nlog_lm &lt;- lm(log_mpg ~ disp, data = mtcars)\nraw_lm &lt;- lm(mpg ~ disp, data = mtcars)\n\npar(mfrow = c(3,2), mar = c(3,3,2,2))\nplot(raw_lm, which = 1)\nplot(log_lm, which = 1)\nplot(raw_lm, which = 2)\nplot(log_lm, which = 2)\nplot(raw_lm, which = 3)\nplot(log_lm, which = 3)\n\n\n\n\n\n\n\n\n\n\nResids versus fitted looks better for log_lm!\nNormal Q-Q looks about the same\nScale-location looks better for log_lm!\n\nIt’s worth noting that we always use “fitted” rather than, say, disp. When using a polynomial model, the fitted values go from the lowest to highest. For a positive coefficient for disp^2, this means that it starts from the lowest point in the parabola and goes upward in both directions! Keep that in mind when interpreting residual plots of polynomial functions!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "Lb15-Transformations.html#the-box-cox-transformation",
    "href": "Lb15-Transformations.html#the-box-cox-transformation",
    "title": "Appendix H — Transformations",
    "section": "H.2 The Box-Cox Transformation",
    "text": "H.2 The Box-Cox Transformation\n\n\nShow the code\nlibrary(MASS)\nboxcox(lm(mtcars$mpg ~ mtcars$disp), data = mtcars)\n\n\n\n\n\n\n\n\n\nSince 0 is the the top 5% of log-Likelihood values, the log transform is reasonable according to Box-Cox!\n\n\nShow the code\nbc &lt;- boxcox(lm(mtcars$mpg ~ mtcars$disp), data = mtcars, plotit = FALSE)\nprint(bc)\n\n\n$x\n [1] -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6\n[16] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9\n[31]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0\n\n$y\n [1] -3.57059866 -2.41311198 -1.29583952 -0.22262913  0.80247262  1.77526399\n [7]  2.69144571  3.54669319  4.33674248  5.05748822  5.70509010  6.27608329\n[13]  6.76748690  7.17690379  7.50260505  7.74359272  7.89963620  7.97127900\n[19]  7.95981574  7.86724092  7.69617369  7.44976444  7.13158953  6.74554135\n[25]  6.29571958  5.78632931  5.22158959  4.60565519  3.94255266  3.23613075\n[31]  2.49002447  1.70763143  0.89209868  0.04631829 -0.82707014 -1.72567168\n[37] -2.64732452 -3.59008589 -4.55221639 -5.53216368 -6.52854602\n\n\nShow the code\nprint(paste0(\"Optimal value of lamba is: \", bc$x[which.max(bc$y)]))\n\n\n[1] \"Optimal value of lamba is: -0.3\"\n\n\n\n\nShow the code\nmtcars$opt_mpg &lt;- (mtcars$mpg^(-0.3) - 1)/0.3\n\noptimal_lm &lt;- lm(opt_mpg ~ disp, data = mtcars)\nraw_lm &lt;- lm(mpg ~ disp, data = mtcars)\n\npar(mfrow = c(3,2), mar = c(3,3,2,2))\nplot(log_lm, which = 1)\nplot(optimal_lm, which = 1)\nplot(log_lm, which = 2)\nplot(optimal_lm, which = 2)\nplot(log_lm, which = 3)\nplot(optimal_lm, which = 3)\n\n\n\n\n\n\n\n\n\nIt does look a little bit better, but the log is simpler to interpret and should probably be used.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html",
    "href": "Lb16-Regression_with_Dummies.html",
    "title": "Appendix I — Regression with Categorical Predictors",
    "section": "",
    "text": "I.1 Regression as a t-test\nWhen we do regression against a dummy variable, we are actually just doing a t-test.\nShow the code\n# Note that I need the var.equal=TRUE to match the assumptions of regression.\nt.test(mpg ~ am, data = mtcars, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  mpg by am\nt = -4.1061, df = 30, p-value = 0.000285\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -10.84837  -3.64151\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231\nShow the code\nlm(mpg ~ am, data = mtcars) |&gt; summary()\n\n\n\nCall:\nlm(formula = mpg ~ am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3923 -3.0923 -0.2974  3.2439  9.5077 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   17.147      1.125  15.247 1.13e-15 ***\nam             7.245      1.764   4.106 0.000285 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.902 on 30 degrees of freedom\nMultiple R-squared:  0.3598,    Adjusted R-squared:  0.3385 \nF-statistic: 16.86 on 1 and 30 DF,  p-value: 0.000285\nThe two outputs have the exact same test statistic and p-value!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regression with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html#regression-as-a-t-test",
    "href": "Lb16-Regression_with_Dummies.html#regression-as-a-t-test",
    "title": "Appendix I — Regression with Categorical Predictors",
    "section": "",
    "text": "A regression models the mean of \\(y\\) for each value of \\(x\\).\nA one-unit increase in \\(X\\) is the difference between 0 and 1.\n\nThe slope is the difference in means.\n\nWe assume constant variance\n\nSame variance at \\(x=0\\) and \\(x=1\\).\n\nA t-test has a df of \\(n-1\\), so does a regression with just one predictor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regression with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html#categorical-predictors-as-anova",
    "href": "Lb16-Regression_with_Dummies.html#categorical-predictors-as-anova",
    "title": "Appendix I — Regression with Categorical Predictors",
    "section": "I.2 Categorical Predictors as ANOVA",
    "text": "I.2 Categorical Predictors as ANOVA\nIn order for R to recognize a variable as a dummy variable, we need to tell it that the values should be interpreted as a factor. This is a very particular data type:\n\nEvery observation is one level of the categorical variable.\nEvery observation can only be one level of that categorical variable\n\nE.g., a car can’t have 4 cylinders and 6 cylinders\n\nThere are a finite number of possible categories.\n\nUnless we specify, R assumes that the unique values that it sees constitutes all the possible categories. In other words, it won’t let us ask about cars with 5 cylinders because we’re making a factor variable with 4, 6, and 8 as the observed values.\n\n\n\n\nShow the code\nanova(aov(mpg ~ factor(cyl), data = mtcars))\n\n\nAnalysis of Variance Table\n\nResponse: mpg\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nfactor(cyl)  2 824.78  412.39  39.697 4.979e-09 ***\nResiduals   29 301.26   10.39                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nShow the code\nsummary(lm(mpg ~ factor(cyl), data = mtcars))\n\n\n\nCall:\nlm(formula = mpg ~ factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   26.6636     0.9718  27.437  &lt; 2e-16 ***\nfactor(cyl)6  -6.9208     1.5583  -4.441 0.000119 ***\nfactor(cyl)8 -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,    Adjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n\n\nThe overall test of significance has an F-value of 39.7 and a p-value of 4.979x10\\(^{-9}\\)\nBut what is up with the summary.lm() output? Why do we have a row labelled factor(cyl)6??",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regression with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html#coding-dummy-variables-what-factor-is-actually-doing",
    "href": "Lb16-Regression_with_Dummies.html#coding-dummy-variables-what-factor-is-actually-doing",
    "title": "Appendix I — Regression with Categorical Predictors",
    "section": "I.3 Coding Dummy Variables (what factor is actually doing)",
    "text": "I.3 Coding Dummy Variables (what factor is actually doing)\nWhen we have multiple variables, we set one aside as the “reference” category. For a factor variable with \\(k\\) categories, we set up \\(k-1\\) new variables to denote category membership. For example:\n\n4 is the reference category for cyl. If all the new dummy variables that we create are all 0, then the car must be 4 cylinders.\nWe set up a dummy variable for 6 cylinders, which is a column with a 1 if the car has 6 cylinders and a 0 otherwise. This is what’s labelled as factor(cyl)6 in the output.\n\nThis can be denoted \\(I(cyl == 6)\\).\n\nSimilarly, we have factor(cyl)8 = \\(I(cyl == 8)\\).\n\nConsider the model \\(y = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8)\\).\nWith this setup, the intercept represents the estimate for 4 cylinder cars, \\(\\beta_1\\) represents the difference in 4 and 6 cylinder cars, while \\(\\beta_2\\) represents the difference in 4 and 8 cylinder cars (you can find the difference between 6 and 8 by doing the right math).\nFrom model.matrix(), we can see this in action:\n\n\nShow the code\ncbind(model.matrix(mpg ~ factor(cyl), data = mtcars), cyl = cbind(mtcars$cyl)) |&gt; head(12)\n\n\n                  (Intercept) factor(cyl)6 factor(cyl)8  \nMazda RX4                   1            1            0 6\nMazda RX4 Wag               1            1            0 6\nDatsun 710                  1            0            0 4\nHornet 4 Drive              1            1            0 6\nHornet Sportabout           1            0            1 8\nValiant                     1            1            0 6\nDuster 360                  1            0            1 8\nMerc 240D                   1            0            0 4\nMerc 230                    1            0            0 4\nMerc 280                    1            1            0 6\nMerc 280C                   1            1            0 6\nMerc 450SE                  1            0            1 8\n\n\nThe model can be written as:\n\\[\ny_i = \\begin{cases}\\beta_0 & \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1 & \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2 & \\text{if }\\; cyl == 8\\end{cases}\n\\]\nAnd indeed we can show that this is equivalent to fitting three separate models:\n\n\nShow the code\ncyl4 &lt;- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 4)))\ncyl6 &lt;- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 6)))\ncyl8 &lt;- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 8)))\nallcyl &lt;- coef(lm(mpg ~ factor(cyl), data = mtcars))\nprint(c(cyl4 = unname(cyl4), beta0=unname(allcyl[1])))\n\n\n    cyl4    beta0 \n26.66364 26.66364 \n\n\nShow the code\nprint(c(cyl6 = unname(cyl6), beta0_plus_beta1 = unname(allcyl[1] + allcyl[2])))\n\n\n            cyl6 beta0_plus_beta1 \n        19.74286         19.74286 \n\n\nShow the code\nprint(c(cyl8 = unname(cyl8), beta0_plus_beta2 = unname(allcyl[1] + allcyl[3])))\n\n\n            cyl8 beta0_plus_beta2 \n            15.1             15.1 \n\n\nThe advantage of having all three in one is that we can test for significance easily!\nOne way (a bad way) to visualize this is to treat \\(I(cyl ==6)\\) and \\(I(cyl==8)\\) as separate variables:\n\n\nShow the code\npar(mfrow = c(1,2))\nplot(mpg ~ I(as.numeric(factor(cyl)) - 1), data = subset(mtcars, cyl %in% c(4,6)),\n    xlab = \"Cyl (0 = 4, 1 = 6)\", main = \"I(cyl == 6)\")\nabline(lm(mpg ~ factor(cyl), data = subset(mtcars, cyl %in% c(4,6))))\nplot(mpg ~ I(as.numeric(factor(cyl)) - 1), data = subset(mtcars, cyl %in% c(4,8)),\n    xlab = \"Cyl (0 = 4, 1 = 8)\", main = \"I(cyl == 8)\")\nabline(lm(mpg ~ factor(cyl), data = subset(mtcars, cyl %in% c(4,8))))\n\n\n\n\n\n\n\n\n\nFrom this, we can see that the slope of the line is indeed looking at the difference in means.\n\nCategorical and Continuous Variables\nIf we have cyl and disp in the model, we get the following:\n\\[\ny = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8) + \\beta_3 disp\n\\] which is equivalent to: \\[\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1  + \\beta_3 disp& \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2  + \\beta_3 disp& \\text{if }\\; cyl == 8\\end{cases}\n\\] This is three different models of mpg versus disp, but with a different intercept depending on the value of cyl.\n\n\nShow the code\ncyldisp &lt;- coef(lm(mpg ~ factor(cyl) + disp, data = mtcars))\ncyldisp\n\n\n (Intercept) factor(cyl)6 factor(cyl)8         disp \n 29.53476781  -4.78584624  -4.79208587  -0.02730864 \n\n\nShow the code\nplot(mpg ~ disp, col = factor(cyl), data = mtcars)\nabline(a = cyldisp[1], b = cyldisp[4], col = 1, lty = 1)\nabline(a = cyldisp[1] + cyldisp[2], b = cyldisp[4], col = 2, lty = 1)\nabline(a = cyldisp[1] + cyldisp[3], b = cyldisp[4], col = 3, lty = 2)\n\n\n\n\n\n\n\n\n\nThe plot looks like it only has 2 lines, but that’s because \\(\\beta_1=\\beta_2\\), so one line is plotted on top of the other! I changed the linetypes so you can see this.\nIt looks like the red and green lines are fitting the red and green data, but the black line doesn’t look quite right.\n\n\nThree Different Models\nIf we have an interaction between cyl and disp, then we essentially get 3 models. \\[\ny = \\beta_0 + \\beta_1I(6) + \\beta_2I(8) + \\beta_3 disp + \\beta_4I(6)disp + \\beta_5I(8)disp\n\\] where \\(I(6)\\) is just shorthand for \\(I(cyl == 6)\\).\nThis is the same as: \\[\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ (\\beta_0 + \\beta_1)  + (\\beta_3 + \\beta_4) disp& \\text{if }\\; cyl == 6\\\\(\\beta_0  + \\beta_2)  + (\\beta_3 + \\beta_5) disp& \\text{if }\\; cyl == 8\\end{cases}\n\\]\nIn R, we cans use the fanciness of the formula notation. R interprets * as interaction as well as lower order terms.\n\n\nShow the code\ncoef(lm(mpg ~ disp * factor(cyl), data = mtcars))\n\n\n      (Intercept)              disp      factor(cyl)6      factor(cyl)8 \n       40.8719553        -0.1351418       -21.7899679       -18.8391564 \ndisp:factor(cyl)6 disp:factor(cyl)8 \n        0.1387469         0.1155077 \n\n\nShow the code\ncoef(lm(mpg ~ disp, data = subset(mtcars, cyl == 4))) # Others will be similar\n\n\n(Intercept)        disp \n 40.8719553  -0.1351418 \n\n\nggplot2 makes it super easy to plot this.\n\n\nShow the code\nlibrary(ggplot2); theme_set(theme_bw())\nggplot(mtcars) +\n    aes(x = disp, y = mpg, colour = factor(cyl)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\")\n\n\n\n\n\n\n\n\n\nRecall that the model without interaction terms (different intercepts, same slopes) had practically the same intercept for 6 and 8. The slopes look different here, but there isn’t a lot of data in the 6 category and I suspect that we could force it to have the same intercept and slope as the 8 category.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regression with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html#comparison-to-other-models",
    "href": "Lb16-Regression_with_Dummies.html#comparison-to-other-models",
    "title": "Appendix I — Regression with Categorical Predictors",
    "section": "I.4 Comparison to Other Models",
    "text": "I.4 Comparison to Other Models\nIn previous lectures, we’ve looked at this same relationship using polynomial model for displacement and a transformation for mpg. Let’s see how these stack up!\nHow do we compare such models? There isn’t a statistical test for which one fits best, but, as always, we want to know about the residuals!\n\n\nShow the code\npoly_lm &lt;- lm(mpg ~ poly(disp, 2), data = mtcars)\nlog_lm &lt;- lm(log(mpg) ~ disp, data = mtcars)\ninteract_lm &lt;- lm(mpg ~ factor(cyl)*disp, data = mtcars)\n\n\n\n\nShow the code\npar(mfrow = c(1,3))\n\nresid_plot &lt;- 1 # Re-run with different values to check other plots\n# Use similar colours to the ggplot.\nmycolours &lt;- c(\"red\", \"green\", \"blue\")[as.numeric(factor(mtcars$cyl))]\n\nplot(poly_lm, which = resid_plot, main = \"Polynomial\", col = mycolours)\nplot(log_lm, which = resid_plot, main = \"Log Transform\", col = mycolours)\nplot(interact_lm, which = resid_plot, main = \"Interaction\", col = mycolours)\n\n\n\n\n\n\n\n\n\n\nResiduals versus fitted looks quite a bit better for the interaction model.\nNormal Q-Q also looks better for interaction model.\nScale-Location is slightly better for interaction model, although still not perfect.\nResiduals versus leverage indicates the Hornet 4 Drive car has somewhat high leverage. I’m guessing this is the largest green dot in the plot up above - the green line would be very different without it.\n\nThis highlights the importance of carefully interpreting Cook’s Distance. In this case, the interaction model is a combination of three different models.\n\n\nThe plots are quite similar, but for the most part the interaction model seems to work best.\nANOVA can be used to compare the residual variance across non-nested models, but is not appropriate for one of the three models we just saw. See if you can guess which !\n\n\nShow the code\nanova(poly_lm, log_lm, interact_lm)\n\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ poly(disp, 2)\nModel 2: mpg ~ factor(cyl) * disp\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     29 233.39                                \n2     26 146.23  3    87.159 5.1655 0.006199 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe models have a significantly different fit, so the one with the lowest residual variance probably fits better.\n\n\nShow the code\nc(summary(poly_lm)$sigma, summary(interact_lm)$sigma)\n\n\n[1] 2.836907 2.371581\n\n\nThere are two important points here:\n\nThe interaction model fits the context of the problem. It absolutely makes sense that an engine with 4 cylinders is different from an engine with 6 cylinders, and part of that difference is the relationship between mpg and displacement. There are no two engines which are equivalent except someone added two cylinders to it; the cylinder values represent fundamentally different groups.\n\nIn other words, the theory supports an interaction model. There’s no theory that I know of that states there should be a quadratic relationship between mpg and displacement. If we want to say something about cars in general (inference), it’s best to go with the context of the problem.\n\nThe interaction model should fit better since it contains more information - it has the displacement and the number of cylinders, the other models only had displacement.\n\nAlso note that the interaction model takes up more degrees of freedom. This can be a negative, especially with small samples.\n\n\n\nThis is an ANCOVA Model\n\n\nShow the code\nanova(interact_lm)\n\n\nAnalysis of Variance Table\n\nResponse: mpg\n                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(cyl)       2 824.78  412.39 73.3221 2.05e-11 ***\ndisp              1  57.64   57.64 10.2487 0.003591 ** \nfactor(cyl):disp  2  97.39   48.69  8.6574 0.001313 ** \nResiduals        26 146.23    5.62                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value for the ANCOVA test is 0.001313, indicating that there is a significantly different covariance between mpg and displacement depending on the number of cylinders.\n\nThe lower-order terms factor(cyl) and disp must be present in order for this test to make sense.\n\nIt is technically possible to fit a model with just the interaction term, but it’s slightly better to have extra predictors with a 0 coefficient than be missing predictors with a non-0 coefficient.\n\nFor the curious, the syntax for this model would be lm(mpg ~ factor(cyl):disp, data = mtcars), where the : indicates multiplication. This isn’t used often, since it’s almost always incorrect to include interaction terms without the individual effects.\nIt’s not clearly better in all cases! There may be some contextual reason why it makes sense to only have an interaction.\n\n\nRecall that the anova() function reports the sequential sum-of-squares. In this situation, we do not care about the p-values for factor(cyl) and disp, so we do not care which order they enter the model in. We only care about the p-value for inclusion of the interaction term factor(cyl):disp.\n\nR’s formula notation is clunky, but leads to a lot of great situations like this. By using factor(cyl)*disp), it added the lower order terms first and then the interaction, thus making the sequential sum-of-squares useful!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regression with Categorical Predictors</span>"
    ]
  },
  {
    "objectID": "Lb17-Multico.html",
    "href": "Lb17-Multico.html",
    "title": "Appendix J — Multicollinearity",
    "section": "",
    "text": "J.1 The problem with multicollinearity\nConsider the model \\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i\n\\] with the added assumption that \\(x_{i1} = a + bx_{i2} + z_i\\), where \\(z_i\\) is some variation.\n(As a technical note, none of this is assumed to be random; just uncertain. We’ll use the language of probability in this section, but it’s just to quantify uncertainty rather than make modelling assumptions.)\nShow the code\nlibrary(plot3D)\nn &lt;- 100\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- 2 + x1 + runif(n, -1, 1)\ny &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n\nscatter3D(x1, x2, y, phi = 30, theta = 15, bty = \"g\",\n    xlab = \"x1\", ylab = \"x2\", zlab = \"y\")\nThe 3D plot looks like a tube! By “tube”, we can think of the 3D plot as being a 2D plot that’s in the wrong coordinate system. In other words, the data define a single axis, which is different from the x1, x2, and y axes. When fitting multiple linear regression, we’re fitting a hyperplane. If the relationship is multicollinear, then we might imagine rotating a hyperplane around the axis defined by the “tube” in the plot above. Since any rotation of the plane around fits the line of data pretty well, the coefficients that define that plane are not well-estimated.\nIt’s hard to imagine 3D sets of points, so let’s do a 2D analogy. In the 3D example, we actually had a 2D relationship, so let’s consider a 2D example that’s almost 1D.\nIn the plot below, I’ve made \\(x\\) almost one-dimensional. That is, I’ve given it a vary small range. I intentionally changed the x limits of the plot to emphasize this.\nShow the code\nx &lt;- c(rep(-0.025, 10), rep(0.025, 10))\ny &lt;- x + rnorm(20, 0, 3)\nplot(y ~ x, xlim = c(-2, 2))\nIf we were to fit a regression line to this, there are many many slopes that would make this work!\nShow the code\nplot(y ~ x, xlim = c(-1, 1))\nfor(i in 1:20) abline(a = mean(y), b = runif(1, -20, 20))\nAll of the lines I added will fit the data pretty well, even though they all have completely different slopes! Yes, there’s one with a “best” slope, but slightly different data would have given us a very different slope:\nShow the code\n# Set up empty plot\nplot(NA, xlab = \"x\", ylab = \"y\",\n    xlim = c(-2, 2), ylim = c(-6,6))\n# Generate data from same dgp as before\nfor(i in 1:100) {\n    y &lt;- x + rnorm(20, 0, 3)\n    abline(lm(y ~ x))\n}\nAll of these lines would have worked for our data!\nThis can easily be seen the variance of the parameters:\nShow the code\nsummary(lm(y ~ x))$coef\n\n\n              Estimate Std. Error    t value  Pr(&gt;|t|)\n(Intercept) -0.6205096  0.6779798 -0.9152331 0.3721680\nx            6.0310840 27.1191916  0.2223917 0.8265128",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Multicollinearity</span>"
    ]
  },
  {
    "objectID": "Lb17-Multico.html#exact-multicollinearity",
    "href": "Lb17-Multico.html#exact-multicollinearity",
    "title": "Appendix J — Multicollinearity",
    "section": "J.2 Exact Multicollinearity",
    "text": "J.2 Exact Multicollinearity\nIn the example above, the line was very sensitive to a slight change in the data because we essentially had one dimension. If we actually had one dimension, then any line that cgoes through \\(\\bar y\\) has the exact same error, regardless of the slope. We can see this in the design matrix: \\(X\\) has a column of 1s for the intercept (which is good), but also has a column for \\(x_1\\) that has zero variance. This means that it’s a linear combination of the first column, and thus is rank-deficient.\n\n\nShow the code\nX &lt;- cbind(rep(1, 20), rep(0, 20))\ntry(solve(t(X) %*% X))\n\n\nError in solve.default(t(X) %*% X) : \n  Lapack routine dgesv: system is exactly singular: U[2,2] = 0\n\n\nSince one column is a linear combination of the other, \\(X^TX\\) cannot be inverted. In this case, the variance fo \\(\\hat\\beta_1\\) is infinite!\nThis can also happen if one column is “close” to being a linear combination of the others, such as:\n\n\nShow the code\n# Change the first element in the second column to 10^-23\nX[1, 2] &lt;- 1e-23\ntry(solve(t(X) %*% X))\n\n\nError in solve.default(t(X) %*% X) : \n  system is computationally singular: reciprocal condition number = 4.75e-48\n\n\nThe rows are mathematically different, but the difference is so small that computers cannot tell.\n\n\nShow the code\nx &lt;- runif(20, 0, 1) / 1e7\nX &lt;- cbind(1, x)\ntry(solve(t(X) %*% X))\n\n\n                            x\n   1.697904e-01 -2.406244e+06\nx -2.406244e+06  4.833450e+13\n\n\nIn the next code chunk, try playing around with the power of 10 (i.e., try 10^50, 10^-50, etc., for both positive and negative powers). At some point, the matrix is not invertible and the coefficient table stops reporting the slope! At the other end, the line is a near perfect fit (why?).\n\n\nShow the code\nx &lt;- runif(20, 0, 1) / (10^350)\ny &lt;- x + rnorm(20, 0, 3)\nsummary(lm(y ~ x))$coef\n\n\n             Estimate Std. Error   t value  Pr(&gt;|t|)\n(Intercept) 0.1092976  0.6173655 0.1770387 0.8613516",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Multicollinearity</span>"
    ]
  },
  {
    "objectID": "Lb17-Multico.html#back-to-regression",
    "href": "Lb17-Multico.html#back-to-regression",
    "title": "Appendix J — Multicollinearity",
    "section": "J.3 Back to Regression",
    "text": "J.3 Back to Regression\nWe can kinda detect multicollinearity mainly using the standard error of the estimates:\n\n\nShow the code\nlibrary(palmerpenguins)\npeng_lm &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = penguins)\nsummary(peng_lm)\n\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n    bill_depth_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1054.94  -290.33   -21.91   239.04  1276.64 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -6424.765    561.469 -11.443   &lt;2e-16 ***\nflipper_length_mm    50.269      2.477  20.293   &lt;2e-16 ***\nbill_length_mm        4.162      5.329   0.781    0.435    \nbill_depth_mm        20.050     13.694   1.464    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 393.4 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7615,    Adjusted R-squared:  0.7594 \nF-statistic: 359.7 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nIt looks like bill_depth_mm has a large standard error! Of course, this might be because:\n\nThe variance of bill_depth_mm is high to begin with.\nThe other predictors have explained most of the variance and any estimate for bill_depth_mm will do.\nMulticollinearity\n\nMultico. is just one of the possible reasons why the SE might be high, we need to look into it more to be sure.\n\n\nShow the code\nlibrary(car)\nvif(peng_lm)\n\n\nflipper_length_mm    bill_length_mm     bill_depth_mm \n         2.673338          1.865090          1.611292 \n\n\nIt actually has quite a small VIF!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Multicollinearity</span>"
    ]
  },
  {
    "objectID": "Lb17-Multico.html#centering-and-scaling",
    "href": "Lb17-Multico.html#centering-and-scaling",
    "title": "Appendix J — Multicollinearity",
    "section": "J.4 Centering and Scaling",
    "text": "J.4 Centering and Scaling\n\n\nShow the code\nX &lt;- model.matrix(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = penguins)\n\nZ &lt;- cbind(1, apply(X[, -1], 2, scale))\nprint(\"correlation of X\")\n\n\n[1] \"correlation of X\"\n\n\nShow the code\nround(cor(X), 4)\n\n\n                  (Intercept) flipper_length_mm bill_length_mm bill_depth_mm\n(Intercept)                 1                NA             NA            NA\nflipper_length_mm          NA            1.0000         0.6562       -0.5839\nbill_length_mm             NA            0.6562         1.0000       -0.2351\nbill_depth_mm              NA           -0.5839        -0.2351        1.0000\n\n\nShow the code\nprint(\"Also correlation of X\")\n\n\n[1] \"Also correlation of X\"\n\n\nShow the code\nround(t(Z) %*% Z/ (nrow(Z) - 1), 4)\n\n\n                         flipper_length_mm bill_length_mm bill_depth_mm\n                  1.0029            0.0000         0.0000        0.0000\nflipper_length_mm 0.0000            1.0000         0.6562       -0.5839\nbill_length_mm    0.0000            0.6562         1.0000       -0.2351\nbill_depth_mm     0.0000           -0.5839        -0.2351        1.0000\n\n\nBy simulation:\n\n\nShow the code\nn &lt;- 100\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- 2 + x1 + runif(n, -3, 3)\n\nreps_1 &lt;- replicate(1000, {\n    y &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))\n}) |&gt; t()\ncor(reps_1)\n\n\n            (Intercept)         x1         x2\n(Intercept)   1.0000000  0.1764180 -0.6112093\nx1            0.1764180  1.0000000 -0.8609998\nx2           -0.6112093 -0.8609998  1.0000000\n\n\n\n\nShow the code\nx1 &lt;- scale(x1)\nx2 &lt;- scale(x2)\n\nreps_2 &lt;- replicate(1000, {\n    y &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))\n}) |&gt; t()\ncor(reps_2)\n\n\n            (Intercept)          x1          x2\n(Intercept)  1.00000000 -0.01364783  0.03646404\nx1          -0.01364783  1.00000000 -0.85526575\nx2           0.03646404 -0.85526575  1.00000000\n\n\nThe correlation is… slightly lower? It’s much lower for the intercept, but it doesn’t make much of a difference for the correlation between the slopes. (It will generally be lower, and should be with a large enough number of simulations.)\nJust for fun, here’s the VIF for these data. I’ve added a parameter x1_around_x2 to allow you to play around with the correlation of x1 and x2.\n\n\nShow the code\nx2_around_x1 &lt;- 3\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- 2 + x1 + runif(n, -x2_around_x1, x2_around_x1)\ny &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n\nvif(lm(y ~ x1 + x2))\n\n\n      x1       x2 \n4.494537 4.494537",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Multicollinearity</span>"
    ]
  },
  {
    "objectID": "Lb18-Backwards_Selection_is_Bad.html",
    "href": "Lb18-Backwards_Selection_is_Bad.html",
    "title": "Appendix K — Backwards p-values",
    "section": "",
    "text": "L Algorithmic model selection\n\nDescription\nBackwards and forward model selection are sequential model selection techniques where predictors are sequentially removed or added until all predictors are significant and no other predictors would be significant if they were added.\n\n\nDistribution of a p-value\nUnder the null hypothesis, a p-value is uniform. That is, all p-values are equally likely. This is not a coincidence, this is how p-values are defined. The p-value is the area of the sampling distribution further away from the hypothesized mean than the observed data. The area “further away” means we are looking at the CDF of a distribution, and the CDF of any continuous distribution follows a normal distribution.\nTo demonstrate, let’s simulate!\n\n\nShow the code\nlibrary(magrittr) # pipes and `extract` function\nlibrary(dplyr)\nlibrary(ggplot2)\nx &lt;- runif(30, 0, 10)\ny &lt;- rnorm(30, 0, 1) # uncorrelated with x - null is true.\n\nsummary(lm(y ~ x))$coef[2, 4] # the p-value for x\n\n\n[1] 0.93823\n\n\nShow the code\nN &lt;- 10000 # Increase for better results\np_vals &lt;- double(N)\nfor (i in 1:N){\n    y &lt;- rnorm(30, 0, 1)\n    p_vals[i] &lt;- summary(lm(y ~ x))$coef[2, 4]\n}\n\n\n\n\nShow the code\nhist(p_vals, freq = FALSE)\nabline(h = 1) # theoretical pdf\n\n\n\n\n\n\n\n\n\n\n\n\nM Backwards Selection\nWe’ve established that P(p.value &lt; 0.05) = 0.05, i.e. that p-values follow a uniform distribution. What about the probability of at least one significant value out of 2?\n\\[\\begin{align*}\nF(z) &= P(\\text{at least one of }X_1\\text{ and }X_2\\text{ is less than }z)\\\\\n&= 1 - P(\\text{both }X_1\\text{ and }X_2\\text{ are greater than }z)\\\\\n&= 1 - P(X_1 &gt; z, X_2 &gt; z)\\\\\n&= 1 - P(X_1 &gt; z)(X_2 &gt; z)\\\\\n&= 1 - (1-z)^2\n\\end{align*}\\]\nSo the pdf is the derivative of this, which is \\(f(z) = 2 - 2z\\).\n\n\nShow the code\nN &lt;- 10000\nmin_p_vals &lt;- double(N)\nfor (i in 1:N){\n    yxx &lt;- data.frame(x1 = runif(30), x2 = runif(30), y = rnorm(30))\n    p_vals &lt;- summary(lm(y ~ ., data = yxx))$coef[-1, 4]\n    max_p &lt;- which.max(p_vals) # to be removed\n    min_p_vals[i] &lt;- summary(lm(y ~ ., data = yxx[, -max_p]))$coef[2, 4]\n}\n\n\n\n\nShow the code\nhist(min_p_vals, freq = FALSE)\nabline(a = 2, b = -2) # Theoretical pdf\n\n\n\n\n\n\n\n\n\nSo when we remove the largest p-value, we end up with a much larger chance that the remaining p-value is below 0.05 - even though the null hypothesis is true. This bears repeating - when we make a change in the model based on the p-value, the rest of the p-values must be interpreted carefully. They are no longer the probability of data at least as extreme under the null hypothesis - they have been artificially shrunk.\n\nWhat about AIC?\nIn R, step() performs stepwise model selection (by default, backwards) until the AIC no longer decreases. I’m going to simulate under the null hypothesis: none of the predictors are actually related to the response.\n\n\nShow the code\nN &lt;- 1000 # This will take a while\nmin_p &lt;- double(N)\nmax_p &lt;- double(N)\np1 &lt;- double(N)\nnum_signif &lt;- double(N)\n\nt0 &lt;- Sys.time()\nfor (i in 1:N){\n    # First columns is y, the rest are x1...x20\n    bigdf &lt;- matrix(data = rnorm(30 * 11), nrow = 30, ncol = 11)\n    colnames(bigdf) &lt;- c(\"y\", paste(\"x\", 1:10))\n    bigdf &lt;- as.data.frame(bigdf)\n\n    # Regress y against everything\n    newmod &lt;- step(lm(y ~ ., data = bigdf), trace = 0)\n\n    # Extract the p-values\n    mycoefs &lt;- summary(newmod)$coef[-1, 4]\n    num_signif[i] &lt;- length(mycoefs)\n    min_p[i] &lt;- min(mycoefs)\n    max_p[i] &lt;- max(mycoefs)\n    p1[i] &lt;- mycoefs[1] # should be representative of p-values\n}\nSys.time() - t0\n\n\nTime difference of 19.71106 secs\n\n\n\n\nShow the code\npar(mfrow = c(2, 2))\nhist(min_p, breaks = seq(0, 0.25, 0.01))\nabline(v = 0.05, col = \"red\")\nhist(max_p, breaks = seq(0, 0.55, 0.01))\nabline(v = 0.05, col = \"red\")\nbarplot(table(num_signif), main = \"Number of remaining predictors\")\nhist(p1, breaks = seq(0, 0.45, 0.01))\nabline(v = 0.05, col = \"red\")\n\n\n\n\n\n\n\n\n\nIn the plots above, keep in mind that none of the predictors were related to the response. We see that the p-values are far from being uniform, and sometimes we took all of the predictors.\nThe bottom right plot shows the distribution of the p-value for the first predictor. There’s nothing special about this predictor, so this plot is representative of the distribution of p-values resulting from stepwise AIC model selection with 20 predictors, none of which are actually related to the response.\n\n\n\nN So what’s the right way?\nBest: Domain knowledge. Start with the model that you think will be the final model, then experiment with adding/removing predictors. For instance, if your model has weather predictors, start with the ones that are most meaningful (e.g. daily high temperature), then look at what changes when you switch it out for something else (e.g., daily average or low temperature). Check the residual diagnostic plots each time - ignore the p-values until the very end!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Backwards p-values</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAR Notes",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "STAR Notes",
    "section": "About This Course",
    "text": "About This Course\nThis book contains the course notes for the Spring 2024 offering of ST362 Regression Analysis, based on the following sources:\n\nApplied Regression Analysis, 3rd edition, by Draper and Smith\n\nA PDF of this textbook is available through the WLU library\n\nIntroduction to Linear Regression Anaysis, 2nd edition, by Montgomery and Peck\n\nThis textbook is excellent but expensive, and I am striving to use free and OER materials.\n\nThe online course notes from Stat 501 at Penn State.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "STAR Notes",
    "section": "About This Book",
    "text": "About This Book\nThis book is a living document. Expect changes throughout each semester that I teach!\nSome features:\n\nThe GitHub logo takes you to the repo for this book. Feel free to fork and adapt as you please (under the CC BY-SA 4.0 license).\nThe little toggle next to the logo puts this into night mode. Try it out!\nEach lecture had a “Jam”, where I played music at the start of class that related to a particular slide. When that slide showed up, a student would say “That’s my Jam!” and I would throw chocolate at them.\n\nThe jams are still there, and you may wish to listen to them while reading!\n\n\nThis book is very much a work in progress. There are missing sections and typos. I am working on adding speaker notes to the slides, which will show up as text in this book.\nI am also working on a major re-write of the first few chapters to walk through the concepts in a better order. In particular, I would like to stay in simple linear regression for a lot longer, demonstrating correlation, Cook’s distance, correlation between \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), etc., then moving into binary and categorical predictors as a first step into multiple regression, polynomial as a second step, then a lecture demonstrating that all of these concepts generalize into multiple dimensions.\nThis is a quarto book based on my lecture slides. The “Lectures” are quarto files that were rendered into beamer PDF slides. I have included the configs to render the slides. To re-create my slides, you can use the code:\nquarto render L01-Introduction.qmd\nAlternatively, you can hit Cmd-Shift-K (Ctrl-Shift-K on Linux and other operating systems) inside VSCode or Rstudio to render the slides into a presentation.\nTo render the whole book (as html), use:\nquarto render --profile book\nThe --profile argument tells Quarto to use the configuration in the file _quarto-slides.yml. I have added speaker notes in a notes environment, which means that the notes will appear in the book but not the pdf slides.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 Unported License.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "L01-Introduction.html",
    "href": "L01-Introduction.html",
    "title": "1  Statistics Review",
    "section": "",
    "text": "1.1 Introduction",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics Review</span>"
    ]
  },
  {
    "objectID": "L01-Introduction.html#introduction",
    "href": "L01-Introduction.html#introduction",
    "title": "1  Statistics Review",
    "section": "",
    "text": "Today’s Learning Outcomes\n\nValidating theory with simulations\nCIs and t-tests.\nImportant facts about distributions.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics Review</span>"
    ]
  },
  {
    "objectID": "L01-Introduction.html#simulating-distributions-from-linear-models",
    "href": "L01-Introduction.html#simulating-distributions-from-linear-models",
    "title": "1  Statistics Review",
    "section": "1.2 Simulating Distributions from Linear Models",
    "text": "1.2 Simulating Distributions from Linear Models\n\nLinear Models 101\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\] where\n\n\\(x_i\\) is assumed to be fixed\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)\n\n“iid” = “independent and identically distributed”\n\n\n\n\nSimulating from a linear model\n\nChoose values for \\(\\beta_0\\) and \\(\\beta_1\\)\nFix \\(x\\)\nSimulate \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\nEstimate \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\sigma^2\\).\n\n\n\nSampling Distributions of Estimates\nThe following code will simulate from a linear model 1000 times. This is not the most efficient way to do this, but it’s more clear what’s going on and we’re working with some pretty small numbers.\n\n\nShow the code\nset.seed(2112)\nn &lt;- 40\nbeta0 &lt;- 10\nbeta1 &lt;- -5\nsigma &lt;- 3\nx_vals &lt;- runif(n, 0, 10)\n\n# Sets up an empty vector\n# (double means double precision, so it allows non-integers;\n# it is not doubling the value.)\nbeta0_ests &lt;- double(n)\nbeta1_ests &lt;- double(n)\nsigma_ests &lt;- double(n)\nfor (i in 1:1000) {\n    y &lt;- beta0 + beta1 * x_vals + rnorm(n, 0, sigma)   \n    mylm &lt;- lm(y ~ x_vals)\n    beta0_ests[i] &lt;- coef(mylm)[1]\n    beta1_ests[i] &lt;- coef(mylm)[2]\n    sigma_ests[i] &lt;- summary(mylm)$sigma\n}\n\npar(mfrow = c(1, 3))\nhist(beta0_ests, breaks = 30)\nabline(v = beta0, col = \"red\", lwd = 3)\n\nhist(beta1_ests, breaks = 30)\nabline(v = beta1, col = \"red\", lwd = 3)\n\nhist(sigma_ests^2, breaks = 30)\nabline(v = sigma^2, col = \"red\", lwd = 3)\n\n\n\n\n\n\n\n\n\nEvery time we generate new data, we get a different model! On average we’re getting the right model, but it’s never exactly equal. In addition, there’s a nice bell-shaped curve centered at the true value. In this class, we’re interested in both of these things - whether each model is approximating the true value, as well as how much it varies around that value.\nIt’s really nice to know that the average value of the estimates is equal to the true value. This is great! But we’re statisticians - at the very least we want variance, if not a full distribution!\n\n\nBut what’s the distribution?\n\n\nShow the code\npar(mfrow = c(1, 3))\nhist(beta0_ests, breaks = 30, freq = FALSE)\nabline(v = beta0, col = \"red\", lwd = 3)\nSxx &lt;- sum((x_vals - mean(x_vals))^2)\ncurve(dnorm(x, 10, sigma * sqrt(sum(x_vals^2)/ (n * Sxx))), from = 5, to = 15, add = TRUE, col = 2, lwd = 3)\n\nhist(beta1_ests, breaks = 30, freq = FALSE)\nabline(v = beta1, col = \"red\", lwd = 3)\ncurve(dnorm(x, -5, sigma/sqrt(Sxx)), from = -10, to = 0, add = TRUE, col = 2, lwd = 3, n = 500)\n\nhist(sigma_ests^2, breaks = 30, freq = FALSE)\nabline(v = sigma^2, col = \"red\", lwd = 3)\ncurve(dchisq(x * (n - 1) / sigma^2, df = n - 1) * (n - 1) / sigma^2, from = 1, to = 20, add = TRUE, col = 2, lwd = 3, n = 500)\n\n\n\n\n\n\n\n\n\nI was able to draw a curve that perfectly matches the distribution of the simulated data based on theory. You won’t yet know what each of the distributions are, but you can see that all of the curve() function calls rely only on the true values.\nThe \\(x\\) values are also used in the calculation, which is not as important as it sounds. In regression, we assume that the \\(x\\) values are fixed (or could have been known ahead of time). You may have noticed that I simulated \\(x\\) values, but this was done once and it was outside the for loop.\n\n\nTake-Home Lessons\n\nAll of the parameters have sampling distributions!\n\nThe variance of the sampling distributions is important\n\nThe distributions are well known to statisticians\n\nSoon you’ll know them too!\n\nSimulations let us “touch” theory!\n\nSimulations cannot “prove” anything!\n\n\nIn this course, we’ll generally follow the pattern of learning theory, verifying via simulation, then applying it to real-world data. The theory is the point of the course, with the hopes that you’ll either learn more advanced theory or have the tools you need to apply it. Simulation is an intermediate step that helps you fully understand the assumptions that you’re making!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics Review</span>"
    ]
  },
  {
    "objectID": "L01-Introduction.html#linear-model-example",
    "href": "L01-Introduction.html#linear-model-example",
    "title": "1  Statistics Review",
    "section": "1.3 Linear Model Example",
    "text": "1.3 Linear Model Example\n\nmtcars data\nWe’ll make great use of the mtcars data set in this course! We’ll just focus on what’s immediately relevant, and get to know it better a little later on.\nFor now, here’s the important bits:\n\n\n\nmpg: Miles per Gallon (fuel efficiency)\nhp: The horsepower of the car\n\nMeasures how many horses the car would be able to take in a fight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the plot, it looks like a linear model might be appropriate (although it might not be perfect).\nIn the simulations, we had a perfect situation: the data were simulated from a perfectly linear relationship and the residuals were perfectly iid normal. In practice, we have to check this before we can do any sort of analysis! In other words, we need our real-world data to come to us as if they were simulated from a perfect model.\n\n\nAssumptions and Residual Diagnostics\nAll statistical models require assumptions. Without assumptions, all we know is that the data we got have a set of properties, such as the mean and variance. With assumptions, we are able to say something about the population! Of course, this opens us up to the risk of being wrong!\nThe assumptions in statistics exist because of the calculations we do. We don’t just arbitrarily assume that the residuals are normal, we use this when we find the line of best fit; we can’t find the line of best fit without making some sort of assumption about the residuals.\nIn linear regression, the assumptions are as follows:\n\n\nAssumptions:\n\nThere’s actually a linear relationship\n\nCheck “Residuals versus Fitted”\n\nThe residuals are normally distributed\n\nCheck “QQ Norm”\n\nThe variance is constant\n\nCheck “Scale-Location”\n\nThe residuals are independent\n\nCheck the sampling procedure!\n\n\n\n\n\nShow the code\npar(mfrow = c(2, 2))\nplot(lm(mpg ~ hp, data = mtcars))\n\n\n\n\n\n\n\n\n\n\n\nEverything else in this semester is a direct result of these four assumptions. It’s kind of amazing, isn’t it? These four sentences, none of which are longer than six words, can form the basis of an entire semester!\nTo assess the assumptions, the first thing to know is this: There should be no patterns in the residual plots. The word “residual” means “what’s left behind”. If there’s a pattern left behind after fitting a linear model, then there was some violation of linearity in the model!\nFrom the data, it looks like there’s a pattern in the residuals versus fitted plot, but the other plots all look fine to me. We’ll explore this in great detail throughout the semester!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics Review</span>"
    ]
  },
  {
    "objectID": "L01-Introduction.html#confidence-intervals",
    "href": "L01-Introduction.html#confidence-intervals",
    "title": "1  Statistics Review",
    "section": "1.4 Confidence Intervals",
    "text": "1.4 Confidence Intervals\n\nGeneral Idea: Terminology\nConsider the statistic \\(t = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)}\\) (a statistic is any number that can be calculated from data alone).\n\n\\(\\theta\\) is the “““true”“” value of the parameter.\n\nUnknown, unknowable, not used in formula - hypothesize \\(\\theta = \\theta_0\\) instead.\n\n\\(\\hat\\theta\\) is our estimator of \\(\\theta\\).\n\nEstimator is a function, like \\(\\hat\\mu = \\bar X =(1/n)\\sum_{i=1}^nX_i\\).\n\n\\(X\\) is a random variable.\n\nEstimate is a number, like the calculated mean of a sample.\n\n\\(se(\\hat\\theta)\\) is the standard error of \\(\\hat\\theta\\).\n\nIf we had a different sample, the value of \\(\\hat\\theta\\) would be different. It has variance.\nStandard error: the standard deviation of the sampling distribution.\n\n\n\n\nGeneral Idea: Distributional Assumptions\nConsider the quantity (not statistic) \\(t = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)}\\).\nIf we assume \\(X\\sim N(\\mu, \\sigma^2)\\), then \\(\\hat\\theta = \\bar X\\) is an unbiased estimate of \\(\\theta\\) on \\(\\nu=n-1\\) degrees of freedom and \\(t \\sim t_\\nu\\)\nFrom this, we can find the lower and upper \\(\\alpha/2\\)% of the \\(t\\) curve.\n\n\\(t_\\nu(\\alpha/2)\\) is the lower \\(\\alpha/2\\)%.\n\ni.e., if \\(\\alpha = 0.11\\), then it’s \\(t(\\nu, 0.055)\\), the lower 5.5% area.\n\n\\(t_\\nu(1 - \\alpha/2)\\) is the upper \\(\\alpha/2\\)%.\nSince \\(t\\) is symmetric, \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\).\n\n\n\nShow the code\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Tools/pvalues\")\n\n\n\n\nGeneral Idea: Distribution to Quantiles\nUnder the null hypothesis, \\(\\theta = \\theta_0\\), so \\[\nt = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)} = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\sim t_\\nu\n\\]\nWe know that \\(\\bar X\\) is a random variable, and we want everything in between its 5.5% and 94.5% quantiles. \n\nThis is a confidence interval!\n\n\n\nGeneral Idea: Distribution to CI\nWe can do this easily for the \\(t_\\nu\\) distribution: we want all values \\(t_0\\) such that\n\\[\\begin{align*}\nt_\\nu(5.5) &\\le t_0 \\le t_\\nu(94.5)\\\\\nt_\\nu(5.5) &\\le \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\le t_\\nu(94.5)\\\\\nse(\\hat\\theta)t_\\nu(5.5) &\\le \\hat\\theta - \\theta_0 \\le se(\\hat\\theta)t_\\nu(94.5)\\\\\n\\hat\\theta + se(\\hat\\theta)t_\\nu(5.5) &\\le \\theta_0 \\le \\hat\\theta + se(\\hat\\theta)t_\\nu(94.5)\n\\end{align*}\\]\nThe CI is all values \\(\\theta_0\\) that would not be rejected by the null hypothesis \\(\\theta = \\theta_0\\) at the \\(\\alpha\\)% level.\nSince \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\), our 89% CI is \\(\\hat\\theta \\pm se(\\hat\\theta)t_\\nu(5.5)\\).\n\n\nWhat is the Standard Error?\nIf we’re estimating the mean, \\(\\hat\\theta = (1/n)\\sum_{i=1}^nX_i\\), where we assume \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma)\\). \\[\nE(\\hat\\theta) = E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n\\mu}{n} = \\mu\n\\] From this, we see that the mean is an unbiased estimator! (This is nice, but not required.)\n\\[\nV(\\hat\\theta) = V\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}V\\left(\\sum_{i=1}^nX_i\\right) \\stackrel{indep}{=} \\frac{1}{n^2}\\sum_{i=1}^nV(X_i) = \\sigma^2/n \\stackrel{plug-in}{=} s^2/n\n\\] where \\(s^2\\) is the estimated variance since we cannot know the true mean.\nNote that \\(s\\) is a biased estimator for \\(\\sigma\\).\nHere’s a quick proof, taken from StackExchange:\nLet \\(S_n\\) be a random variable denoting the sample standard deviation. Note that \\(E(S_n^2) = \\sigma^2\\). What’s \\(E(S_n)\\)?\nA tricky way to find \\(E(S_n)\\) is to start with \\(V(S_n)\\). We know that \\(V(S_n) = E(S_n^2) - (E(S_n))^2\\), therefore \\(E(S_n) = \\sqrt{E(S_n^2) - V(S_n)}\\). Note that \\(E(S_n^2) = \\sigma^2\\), so this becomes \\(E(S_n) = \\sqrt{\\sigma^2 - V(S_n)}\\). Already, we can see that \\(E(S_n) \\ne \\sigma\\).\nAssuming that the variance is strictly larger than 0, then removing it from the equation makes the right hand side larger, i.e. \\(\\sqrt{\\sigma^2 - V(S_n)} &lt; \\sqrt{\\sigma^2} = \\sigma\\). If we replace \\(\\sqrt{\\sigma^2 - V(S_n)}\\) by \\(\\sigma\\), the right hand side is larger than the left, i.e. \\(E(S_n) &lt; \\sigma\\).\n\n\nPutting it all together: t CI for a mean\nAssuming \\(X\\sim N(\\mu, \\sigma^2)\\), we get that \\(\\bar X\\sim N(\\mu, s^2/n)\\).\nA \\((1 - \\alpha)\\)%CI for \\(\\mu\\) is: \\[\n\\bar x \\pm t_{n-1}(\\alpha/2)s/\\sqrt{n}\n\\]\n\n\nCI for Variance\nFrom before: \\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_n\\).\nLet \\(\\chi^2_n(0.055)\\) be the lower 5.5% quantile, \\(\\chi^2_n(0.945)\\) be the upper.\nFor homework, find the CI from: \\[\n\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\n\\] Note that \\(\\chi^2_n(0.055)\\ne-\\chi^2_n(0.945)\\).\nIn the standard normal distribution, \\(\\Phi(0.055)\\) is the usual notation for the CDF evaluated at 0.055, i.e. you tell it that you want 5.5% of the data below a certain point on the x-axis, and it tells you that point. For the standard normal distribution, it is always true that \\(\\Phi(\\alpha) = - \\Phi(1 - \\alpha)\\) since it is symmetric around 0. This is not the case for the Chi-Square distribution.\n\n\nTake-Home Lessons\n\nCIs are all values that would not be rejected by a hypothesis test.\n\nThe null hypothesis determines the distribution of the test statistic, which allows us to find the CI.\n\nCIs are found based on distributional assumptions\n\nMathematical Statistics is a different course, unfortunately.\n\nStandard errors are everywhere!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics Review</span>"
    ]
  },
  {
    "objectID": "L01-Introduction.html#distributions-for-reference",
    "href": "L01-Introduction.html#distributions-for-reference",
    "title": "1  Statistics Review",
    "section": "1.5 Distributions (for reference)",
    "text": "1.5 Distributions (for reference)\nIn this section, we’ll just lay out some basic facts about some distributions. I try to also justify why these facts are important for this course! Otherwise, the following examples should act as a reference, and won’t be tested on an exam or assignment.\n\nNormal\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right);\\;-\\infty&lt;x&lt;\\infty\n\\]\nShiny: https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory\n\nCompletely determined by \\(\\mu\\) and \\(\\sigma\\).\nIf \\(X \\sim N(\\mu,\\sigma^2)\\), then \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\).\n\nOften called “standardizing”\n\nYou won’t need to memorize the pdf, but it’s useful!\n\n\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\nThe normal distribution is the foundation for pretty much everything that we’re going to do in this class. In general, things aren’t normally distributed, but the assumption is very robust and works out in a lot of cases.\nIn this lecture we’re going to build up an important result that we’ll use frequently. In particular, we want some background into why the F and \\(t\\) distributions show up so often!\n\n\nThe \\(t\\) Distribution\n\\[\nf(x; \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu + 1)/2}\n\\]\n\nThe notation \\(f(x; \\nu)\\) means “function of \\(x\\), given a value of \\(\\nu\\).”\nCompletely determined by \\(\\nu\\)\nAs \\(\\nu\\rightarrow\\infty\\), this becomes \\(N(0,1)\\).\n\n\\(\\nu&gt;60\\) is pretty much normal already.\nFor \\(\\nu&lt;\\infty\\), \\(t\\) has wider tails than normal.\n\n\n\n\nThe \\(\\chi^2\\) distribution - variances\nIf \\(Z_1, Z_2, ..., Z_k\\) are iid \\(N(0,1)\\), then \\(\\chi^2_k = \\sum_{i=1}^kZ_i^2\\) has a chi-square distribution on \\(k\\) degrees of freedom. \\[\nf(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\n\\]\n\nAs \\(k\\rightarrow\\infty\\), \\((\\chi^2_k-k)/\\sqrt{2k}\\stackrel{d}{\\rightarrow} N(0,1)\\).\n\nThat is, for large \\(k\\) this can be approximated by a normal distribution.\n\nVery related to the variance\n\n\\((n-1)s^2/\\sigma^2\\) follows a \\(\\chi^2_n\\) distribution.\n\n\nNote that, if \\(X_i\\sim N(\\mu, \\sigma)\\), then \\(\\frac{X_i - \\mu}{\\sigma} = Z = N(0,1)\\) and:\n\\[\n\\dfrac{(n-1)S^2}{\\sigma^2} = \\dfrac{(n-1)\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar X)^2}{\\sigma^2} = \\sum_{i=1}^n\\left(\\dfrac{X_i - \\bar X}{\\sigma}\\right) = \\sum_{i=1}^nZ_i^2\n\\]\nSo \\(n-1\\) times the estimated variance divided by the true variance does follow a Chi-square distribution. There’s some advanced math with distributions going on here; the takeaway message is that the variance follows a Chi-Square distribution because \\(X_i\\) are assumed to be normal!\n\n\nThe \\(t\\) distribution in practice\nIf the sampling distribution of the sample mean is normal, i.e. \\(\\bar X\\sim N(\\mu, \\sigma)\\), then:\n\n\\(\\dfrac{S^2(n-1)}{\\sigma^2} = \\chi^2_{n-1} \\implies S / \\sigma = \\sqrt{\\chi^2_{n-1}/(n-1)}\\), and\n\\(\\dfrac{\\bar X - \\mu}{\\sigma / \\sqrt{n}} = N(0,1)\\)\n\nThe following math will not be on the exam! This is graduate level math. The rest of this course is going to focus on much simpler ideas! The takeaway message is that \\((\\bar x - \\mu)/(\\sigma/\\sqrt{n})\\) follows a \\(t\\) distribution because the \\(X_i\\) are assumed to be normal.\nWith a little trickery,\n\\[\n\\dfrac{\\bar X - \\mu}{S / \\sqrt{n}} = \\dfrac{\\bar X - \\mu}{S / \\sqrt{n}}\\cdot\\dfrac{\\sigma}{\\sigma} = \\dfrac{\\bar X - \\mu}{\\sigma / \\sqrt{n}}\\cdot\\dfrac{1}{S / \\sigma} = \\dfrac{Z}{\\sqrt{\\chi^2_{n-1}/(n-1)}} = t_{n-1}\n\\]\nIn other words, if the sampling distribution is normal, then \\(\\dfrac{\\bar x - \\mu}{s / \\sqrt n}\\) is from a \\(t_{n-1}\\) distribution!!!\n\n\nThe \\(F\\) distribution - ratio of variances\nIf \\(S_1\\) and \\(S_2\\) are independent \\(\\chi^2\\) distributions with degrees of freedom \\(\\nu_1\\) and \\(\\nu_2\\), then \\(\\frac{S_1/\\nu_1}{S_2/\\nu_2}\\) follows an \\(F_{\\nu_1,\\nu_2}\\) distribution. \\[\nf(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\n\\]\n\nIf \\(s_1^2\\) and \\(s_2^2\\) are the sample-based estimates of the true values \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), then \\(\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2}\\) follows an \\(F_{\\nu_1, \\nu_2}\\) distribution.\n\nUnder \\(H_0\\), \\(\\sigma_1=\\sigma_2\\), so we don’t need the true values.\nNote: \\(s_1^2\\) is calculated from \\(S_1^2/\\nu_1\\).\n\n\nOne last time: We assume that the \\(X_i\\) are normal, and all of the other distributions fall into place.\nIn the rest of the course, we’re assuming that the residuals of a linear model follow a normal distribution. Every other distribution we see is a result of this, we are not making further assumptions. When we do an F test for the ratio of variances, we’re not assuming an F distribution - we’re assuming normal residuals, and this results in an F distribution!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics Review</span>"
    ]
  },
  {
    "objectID": "L01-Introduction.html#homework-problems",
    "href": "L01-Introduction.html#homework-problems",
    "title": "1  Statistics Review",
    "section": "1.6 Homework Problems",
    "text": "1.6 Homework Problems\nTextbook questions for chapters 1-3 are grouped together at the end of chapter 3. There are no relevant excercises yet. However, it’s a good time to review some distributions with simulations!\n\nFind the following values using R:\n\nThe probability of a standard normal value less than -1.\nThe probability of a standard normal value less than -1 or greater than 1.\nThe 0.055 percentile of a standard normal distribution.\nThe 0.945 percentile of a standard normal distribution.\nThe 0.945 percentile of a Chi-Square distribution of 30 degrees of freedom.\nThe 0.945 percentile of an F distribution of 30 and 32 degrees of freedom.\nThe probability of an \\(F_{30, 32}\\) value larger than 1.5\n\n\n\n\nSolution\n\n\n# a.\npnorm(-1)\n\n[1] 0.1586553\n\n# b.\npnorm(-1) + (1 - pnorm(1))\n\n[1] 0.3173105\n\n# b. (alternative solution)\n2 * pnorm(-1)\n\n[1] 0.3173105\n\n# c.\nqnorm(0.055)\n\n[1] -1.598193\n\n# d.\nqnorm(0.945)\n\n[1] 1.598193\n\n# e.\nqchisq(0.945, df = 30)\n\n[1] 43.31057\n\n# f. \nqf(0.945, df1 = 30, df2 = 32)\n\n[1] 1.78572\n\n# g.\n1 - pf(1.5, df1 = 30, df2 = 32)\n\n[1] 0.1310077\n\n\n\n\n\nCalculate an 89% CI for the mean eruption time of the Old Faithful geyser.\n\nThe data are stored in the built-in R data set faithful, shown below. The eruption times are faithful$eruptions\nAlso make a plot - is a CI meaningful in this case?\nAlso find a CI for the variance of the eruption times.\n\n\n\ndata(faithful)\n\n\n\nSolution\n\n\nxbar &lt;- mean(faithful$eruptions)\ns &lt;- sd(faithful$eruptions)\nn &lt;- length(faithful$eruptions)\n\n# 89% CI (one liner)\nxbar + c(1, -1) * qt(0.055, df = n - 1) * s / sqrt(n)\n\n[1] 3.376815 3.598751\n\nplot(faithful)\n\n\n\n\n\n\n\n\nNote that t.test(faithful$eruptions, conf.level = 0.89) includes the exact same CI in its output.\nFrom the plot, it looks like the mean is not a reasonable measure of the average eruption times! There are clearly two groups, i.e. the distribution is bimodal. It would be better to separate the two clusters and make CIs for the means in each of those.\nHowever, the sampling distribution of the sample mean is likely normal because the sample size is large enough, so even though the mean is not a valid measure because of what we know from the context of the data, there’s no issue with the CI. If we were to repeat these measurements many many times, 89% of the sample means that we get will be in our 89% CI. The CI is valid, the mean is just not meaningful here.\nThe CI for the variance can be found as follows:\n\nxbar &lt;- mean(faithful$eruptions)\ns &lt;- sd(faithful$eruptions)\nn &lt;- length(faithful$eruptions)\n\n# I've included the estimate for reference.\nsd_ci &lt;- c(\"low\" = qchisq(0.055, df = n - 1) * s^2 / (n - 1),\n    \"estimate\" = s^2,\n    \"high\" = qchisq(0.945, df = n - 1) * s^22 / (n - 1))\nsd_ci\n\n      low  estimate      high \n 1.129010  1.302728 20.925488 \n\nsqrt(sd_ci)\n\n     low estimate     high \n1.062549 1.141371 4.574439 \n\n\nWe are 89% confident that the true population variance is between 1.1290 and 20.925. This corresponds to standard deviations of 1.0625 and 4.5745, respectively.\n\n\n\nThe code below demonstrates one of the topics covered in this lecture. Which is it?\n\n\nn &lt;- 60\nsigma1 &lt;- 4\nsigma2 &lt;- 8\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- runif(n, 0, 5)\n\nsim_vals &lt;- double(1000)\nfor (i in 1:1000) {\n    e1 &lt;- rnorm(n, 0, sigma1)\n    e2 &lt;- rnorm(n, 0, sigma2)\n    y1 &lt;- x1 + 2 * x2 + e1\n    y2 &lt;- x1 - 2 * x2 + e2\n\n    val1 &lt;- summary(lm(y1 ~ x1 + x2))$sigma\n    val2 &lt;- summary(lm(y2 ~ x1 + x2))$sigma\n\n    numerator &lt;- val1^2 / sigma1^2\n    denominator &lt;- val2^2 / sigma2^2\n    sim_vals[i] &lt;- numerator / denominator\n}\n\nhist(sim_vals, freq = FALSE, breaks = 40)\ncurve(df(x, n - 2, n - 2), add = TRUE, col = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\n\nSolution\n\nIt actually demonstrates a couple:\n\nThe x-values are assumed to be fixed; the errors are drawn at random to produce our sample.\nThe F distribution is the ratio of two variances, divided by their respective true variances.\n\n\n\n\nTrue or False: As \\(n\\) approaches infinity, the sampling distribution of the sample variance approaches a normal distribution?\n\n\n\nSolution\n\nLet’s start small. If \\(n = 3\\), then the variance clearly does not follow a normal distribution. Instead, it is a transformation of a Chi-Square distribution. What happens as n gets larger? What if the true sd gets smaller? What if we use a uniform distribution, or something else?\n\n# Change me to a different number!\nn &lt;- 3\n# Change me to a different number!\ntrue_sd &lt;- 1\n\nvals &lt;- double(100000)\nfor (i in seq_len(length(vals))) {\n    # Change me to a different distribution!\n    new_sample &lt;- rnorm(n, 0, true_sd) \n    vals[i] &lt;- var(new_sample)\n}\n\nhist(vals, freq = FALSE, breaks = 40)\ncurve(dnorm(x, mean(vals), sd(vals)), add = TRUE)\n\n\n\n\n\n\n\n\nWith a little investigation, the sample variance does indeed approach normality! The sample variance is something like an average - it’s the average of the squared deviations - so the CLT applies.\n\n\n\nWrite a simulation to show that \\((n-1)s^2/\\sigma^2\\sim \\chi^2_n\\). You may use the simulation from the previous question as a starting point.\n\n\n\nSolution\n\nYou will know you’ve got the right answer when the line added by curve(dchisq(x, n), add = TRUE, col = 2, lwd = 3) fits the histogram of your simulated values!\n\n\n\nShow that the square of a \\(t_\\eta\\) distribution is equal to an F distribution on 1 and \\(\\eta\\) degrees of freedom. Use the following facts:\n\n\\(t_\\eta\\) is defined as \\(\\frac{Z}{\\sqrt{\\chi^2_\\eta/\\eta}}\\).\n\\(F_{\\eta_1,\\eta_2} = \\frac{\\chi^2_{\\eta_1}/\\eta_1}{\\chi^2_{\\eta_2}/\\eta_2}\\)\nFor any variable \\(X\\), \\(X^2 = \\sum_{i = 1}^1X^2 / 1\\)\n\n\nNote that this sort of question will not be on the test, but the result is important for later!\n\n\nSolution\n\nFirst note that \\(Z^2\\) can be written as \\(Z^2 = \\sum_{i=1}^1Z^2/1 = \\chi^2_1/1\\), that is, a squared normal distribution is equal to the sum of a single squared normal distribution divided by it’s degrees of freedom (which is 1), which is exactly a Chi-Square distribution.\n\\[\nt^2_\\eta = \\left(\\frac{Z}{\\sqrt{\\chi^2_\\eta/\\eta}}\\right)^2\n= \\frac{Z^2}{\\chi^2_\\eta/\\eta}\n= \\frac{\\chi^2_1/1}{\\chi^2_\\eta/\\eta}\n= F_{1, \\eta}\n\\]\n\n\n\nInterpret the following plot.\n\nWhat concept does it demonstrate?\nWhat does it mean for Confidence Intervals?\n\n\n\nnorm_quantile &lt;- qnorm(p = 0.055)\nt_quantile &lt;- qt(p = 0.055, df = (2:200) - 1)\nplot(x = (2:200) - 1, y = t_quantile,\n    ylab = \"'p' such that P(X &lt;= 0.055) = p\",\n    xlab = \"Sample Size\")\nabline(h = norm_quantile, col = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\n\nSolution\n\nAs the sample size increases, the \\(t\\) distribution approaches a normal distribution. The y axis shows the critical value for an 89% CI from a t distribution, and as we approach infinity (or even 60 or so) it becomes the same as the critical value of a normal CI. However, there is still a slight difference.\nWe could use a normal distribution to approximate the t distribution when n is large, but we may as well just keep using the t distribution. It’s the correct one and it’s still easy to use.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics Review</span>"
    ]
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html",
    "href": "L02-Fitting_Straight_Lines.html",
    "title": "2  Fitting Straight Lines",
    "section": "",
    "text": "2.1 Why Fit Models?",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting Straight Lines</span>"
    ]
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "href": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "title": "2  Fitting Straight Lines",
    "section": "",
    "text": "The Quote.\n“All models are wrong, some are useful.” - George Box\nThis quote is something that you should tattoo on the inside of your eyelids so that you see it every time you blink.\n… okay that’s a little extreme, but this quote perfectly encapsulates the required mindset for modelling. No model is ever going to perfectly describe complex phenomena. However, a good model will at least say something useful about the world!\nOf course, it’s not a guarantee that a model is going to be useful. There are an infinite amount of useless models out there, and only a finite number of useful ones.\n\n\nAll Models Are Wrong, But Some Are Useful\nModel: A mathematical equation that explains something about the world.\n\n\n\nGravity: 9.8 \\(m/s^2\\) right?\n\nThis is a model - it’s an equation that explains something in the world.\nVaries across the surface of the Earth.\nVaries according to air resistance.\n\nEvery additional cigarette decreases your lifespan by 11 minutes.\n\nVery, very much wrong.\nVery, very useful for health communication.\n\n\n\n\n\n\n\n\nAll Linear Models are Wrong, Too!\n\n\n\n\nShow the code\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(palmerpenguins)\npenguins &lt;- penguins[complete.cases(penguins),]\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"A line fits reasonably well\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(mtcars) +\n    aes(x = disp, y = mpg) + \n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y~poly(x, 2)\", colour = 2) +\n    labs(title = \"A straight line misses the pattern\",\n        subtitle = \"A polynomial might fit better\",\n        x = \"Engine Displacement (1000 cu.in)\", \n        y = \"Fuel Efficiency (mpg)\")\n\n\n\n\n\n\n\n\n\n\n\nThe plot on the left shows penguins’ body mass against their flipper length. A straight line seems to fit pretty well. There’s still a fair bit of variance above and below the line so we can’t perfectly say exactly how much a penguin weighs if we only know their height, but the overall pattern is pretty straight.\nOn the right, we see the mtcars data again (definitely not the last time!), showing the fuel efficiency versus the engine displacement. The straight blue line is clearly missing important features of the data. We might be able to say that the fuel efficiency decreases with larger engines, but the line we’ve fit isn’t going to be a useful description of the true pattern. The red curve seems to fit better, but we’ll learn about a more useful model later in this course.\n\n\nSome Linear Models are Useful\n\n\nShow the code\nxpred &lt;- 200\nypred &lt;- predict(\n    lm(body_mass_g ~ flipper_length_mm, data = penguins),\n        newdata = data.frame(flipper_length_mm = xpred))\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"The height of the line is an okay prediction for Body Mass\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\") +\n    annotate(geom = \"point\", shape = \"*\", size = 30, colour = 2,\n        x = xpred, \n        y = ypred) + \n    annotate(geom = \"segment\", colour = 2, linewidth = 2, \n        linetype = \"dashed\",\n        x = xpred, xend = xpred, \n        yend = -Inf, y = ypred) + \n    annotate(geom = \"segment\", colour = 2, linewidth = 2, \n        linetype = \"dashed\",\n        x = -Inf, xend = xpred, \n        yend = ypred, y = ypred)\n\n\n\n\n\n\n\n\n\n\n\nA Model is an Equation\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i;\\; E(y_i|x_i) = \\beta_0 + \\beta_1 x_i;\\; V(y_i|x_i)=\\sigma^2\n\\]\n\nFor a one unit increase in \\(x_i\\), \\(y_i\\) increases by \\(\\beta_1\\).\n\nIf our model fits well and this is statistically significant, we can apply this to the population.\n\nThe estimated value of \\(y_i\\) when \\(x_i=0\\) is \\(\\beta_0\\).\n\nNot always interesting, but usually\\(^*\\) necessary.\n\n\nOur prediction for \\(y_i\\) at any given value of \\(x_i\\) is calculated as: \\[\n\\hat y_i = \\hat\\beta_0 + \\hat \\beta_1x_i\n\\] where the “hats” mean we’ve found estimates for the \\(\\beta\\) values.\nSide note: there are many models that are systems of equations - not just a single equation!\n\n\nNotes on Notation\nI will make mistakes, but in general:\n\n\\(Y\\) is definitely a random variable (usually a vector) representing the response.\n\n\\(Y_i\\) is also a random variable (not a vector)\nI also use \\(y_i\\) as a random variable sometimes - context should make this clear!\nNote that \\(V(\\underline y)\\) may refer to the variance of a r.v. or the empirical variance of observed data. Context!\n\n\\(X\\) is a matrix of covariates\n\nNot a random variable!!! Capital letters could be either random or matrix (or both). I will specify when necessary.\nI will avoid the notation \\(X_i\\).\nEntries of \\(X\\) are labelled \\(x_{ij}\\).\n\n\n\n\nThe Mean (Expectation) of \\(Y\\) at any value of \\(X\\)\n\\[\nE(Y|X) = X\\underline\\beta\n\\]\n\n\nShow the code\npeng &lt;- penguins[complete.cases(penguins), ]\nx &lt;- peng$flipper_length_mm\ny &lt;- peng$body_mass_g\n\nx_range &lt;- seq(min(x), max(x), by = 0.5)\ny_mean &lt;- double(length(x_range))\nbin_width &lt;- 2\nfor (i in seq_along(x_range)) {\n    y_mean[i] &lt;- mean(y[x &gt; x_range[i] - bin_width & x &lt; x_range[i] + bin_width])\n}\n\nplot(x, y, \n    xlab = \"Flipper Length (mm)\", ylab = \"Body Mass (g)\")\nlines(x_range, y_mean, col = 2, lwd = 2)\nabline(lm(y ~ x), col = 3, lwd = 2)\nlegend(\"topleft\", legend = c(\"data\", \"mean of y at x +/- 2\", \"linear model\"), pch = c(\"o\", \"-\", \"-\"), col = c(1, 2, 3), lwd = c(NA, 2, 2), bty = \"n\")\n\n\n\n\n\n\n\n\n\nThe plot above shows the height of a child against the average height of their parents.\nThe yellow dots are caclulated as follows: a “bin” of x-values is defined - in this case, a bin is all values between, say, 60 to 63, and the next bin might be 61 to 64, and the next is 62 to 65, etc. The average height of the y-values was found within each bin. The yellow dot is the average y height, plotted at the x-axis value in the middle of the bin.\nThe takeaway message of this plot is just how well the red line follows the yellow dots. A linear model can be seen as the average y-value for each x-value. This is a very useful way to think of linear models - they’re just fancy averages!\n\n\nA Linear Model is Linear in the Parameters\nThe following are linear:\n\n\\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\nThe following are not linear:\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0\\beta_1x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\sin(\\beta_1)x_i +\\epsilon_i\\)\n\nIn the “not linear” list, note that it’s the \\(\\beta\\)s that are non-linear.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting Straight Lines</span>"
    ]
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#estimation",
    "href": "L02-Fitting_Straight_Lines.html#estimation",
    "title": "2  Fitting Straight Lines",
    "section": "2.2 Estimation",
    "text": "2.2 Estimation\n\nGoal: Find \\(\\beta\\) values which minimize the error\nModel: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\nError: \\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\nTo fit the model, why not just find the one that minimizes the sum of the errors?\nAnswer: the errors can be negative, and the negatives can just cancel out the positives. If we just add the errors, then we can always get the sum to be 0 even if the model isn’t actually fitting well.\nHere’s an example:\n\n\nShow the code\nset.seed(2112)\nx &lt;- runif(50, 0, 10)\ny &lt;- 4 + 5 * x + rnorm(50, 0, 4)\n\npar(mfrow = c(1, 2))\nplot(x, y)\n\nsum_of_errors &lt;- c()\nbeta0s &lt;- seq(24, 68, 4)\nfor (beta0 in beta0s) {\n    abline(a = beta0, b = -4, lty = 2,col = 2)\n    text(x = 5, y = beta0 - 4 * 5,\n        labels = bquote(beta[0] ~ \"=\" ~ .(beta0)))\n    errors &lt;- y - beta0 + 4 * x\n    sum_of_errors &lt;- c(sum_of_errors, sum(errors))\n}\n\nplot(beta0s, sum_of_errors)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nIn the plot above, the value of \\(\\hat\\beta_0\\) is varied from 24 to 68 by 4 (24, 28, 32, …), with the value of \\(\\hat\\beta_1\\) fixed at -4. These lines are shown on the left.\nThe right plots the sum of the errors in these lines. The sum starts above 0, then at some point goes below 0. This means that we can have a slope of \\(\\hat\\beta_1 = -4\\), but still find an intercept that makes the sum of the errors equal to 0!\n\n\nLeast Squares\nGoal: Minimize \\(\\sum_{i=1}^n\\hat\\epsilon_i^2=\\sum_{i=1}^n(y_i - \\hat y_i)^2\\), the sum of squared errors.\nThis ensures errors don’t cancel out. It also penalizes large errors more.\nCould we have used \\(|\\epsilon_i|\\), or some other function that removes negatives? \\(|ly|\\)!\nThe expression \\(|ly|\\) represents the absolute value of “ly”. It’s a short form for “absolutely” that I’m trying to get the youths to use. It’s not going well.\nWe’ll see in other lectures and homework problems that we can define different errors, which prioritizes different aspects of the data. Squaring the errors means that large errors get even larger, so outliers are penalized more. Absolute errors penalize all values according to the size. You could also use exponential errors (\\(\\exp(\\hat\\epsilon_i)\\)) if you really wanted to.\nWe use squared errors for one main reason: because the math is easy. (This is not true, but it’s a big selling point of squared errors.)\n\n\nLeast Squares Estimates - Start\nTo find the minimum of a function, we take the derivative and set it to 0! \\[\\begin{align*}\nR(\\underline \\beta) \\stackrel{def}{=}\\sum_{i=1}^n\\hat\\epsilon_i^2 &= \\sum_{i=1}^n(y_i - \\hat y_i)^2 = \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\\\\n\\implies \\frac{dR(\\underline\\beta)}{d\\beta_0} &= -2\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)\\\\\n\\text{and }\\frac{dR(\\underline\\beta)}{d\\beta_1} &= -2\\sum_{i=1}^n(y_i - \\beta_0- \\beta_1x_i)x_i = -2\\sum y_ix_i + 2\\beta_0\\sum x_i + 2\\beta_1\\sum x_i^2\n\\end{align*}\\]\nHomework: complete the derivation.\n\n\nLeast Squares Estimates\nI will assume that you can do the Least Squares estimate in your sleep (be ready for tests).\nThe final results are: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x\\\\\n\\hat\\beta_1 &= \\frac{\\sum_{i=1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i=1}^n(x_i - \\bar x)^2}\\stackrel{def}{=}\\frac{S_{XY}}{S_{XX}}\n\\end{align*}\\]\nThe textbook was written in 1998 and gives formulas to make the calculation easier to do on pocket calculators. You will not need a pocket calculator for this course.\nIf you’ve never done this before, try it! Make sure you get the same final answer, and then do it again without looking at your notes. This sort of derivation is a major part of any regression course.\nAn important point about these estimates is that we have not yet assumed normality. These are the values that minimize the squared errors. That’s it; we did nothing else, assumed nothing else, and apply this to nothing else.\nNote: We’ll use these definitions of \\(S_{XX}\\) and \\(S_{YY}\\) a lot! Get used to them!\n\n\nLet’s Add Assumptions\nAssumptions allow great things, but only when they’re correct!\n\n\\(E(\\epsilon_i) = 0\\), \\(V(\\epsilon_i) = \\sigma^2\\).\n\\(cov(\\epsilon_i, \\epsilon_j) =0\\) when \\(i\\ne j\\).\n\nImplies that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and \\(V(y_i) = \\sigma^2\\).\n\n\\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\)\n\nThis is a strong assumption, but often works!\n\n\nInterpretation: the model looks like a line with completely random errors. (“Completely random” doesn’t mean “without structure”!)\n\n\nMean of \\(\\hat\\beta_1\\)\nIt is easy to show that \\[\n\\hat\\beta_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i=1}^n(x_i - \\bar x)^2} = \\frac{\\sum_{i=1}^n(x_i - \\bar x)y_i}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n\\] since \\(\\sum(x_i - \\bar x) = 0\\) (show this).\nHomework: show that \\(E(\\hat\\beta_1) = \\beta_1\\) (*Hint: use the fact that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) - don’t mix up the estimated values \\(\\hat\\beta\\) and the true values \\(\\beta\\)!).\n\n\nVariance of \\(\\hat\\beta_1\\)\n\\[\n\\hat\\beta_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar x)y_i}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n\\] can be re-written as \\[\n\\hat\\beta_1 = \\sum_{i=1}^na_iy_i\\text{, where }a_i = \\frac{x_i - \\bar x}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n\\]\nThus the variance of \\(\\hat\\beta_1\\) is \\[\nV(\\hat\\beta_1) = \\sum_{i=1}^na_i^2V(y_i) = ... = \\frac{\\sigma^2}{S_{XX}} \\stackrel{plug-in}{=} = \\frac{s^2}{S_{XX}}\n\\]\n\n\nConfidence Interval and Test Statistic for \\(\\hat\\beta_1\\)\nThe test statistic can be found as: \\[\nt = \\frac{\\hat\\beta_1 - \\beta_1}{se(\\hat\\beta_1)} = \\frac{(\\hat\\beta_1 - \\beta_1)}{s/\\sqrt{S_{XX}}} \\sim t_\\nu\n\\]\nSince this follows a \\(t\\) distribution, we can get the CI: \\[\n\\hat\\beta_1 \\pm t_\\nu(\\alpha/2)\\sqrt{\\frac{s^2}{S_{XX}}}\n\\]",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting Straight Lines</span>"
    ]
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#exercises",
    "href": "L02-Fitting_Straight_Lines.html#exercises",
    "title": "2  Fitting Straight Lines",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\nGood textbook questions: A, K, O, P, T, U, X (using data(anscombe) in R), Z, AA, CC. Note that all data sets in the textbook are available in an R package found here.\n\nFind the OLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for the model \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\).\nProve that the point \\((\\bar x, \\bar y)\\) is always on the estimated line for simple linear regression.\nFind the OLS estimate of \\(\\beta_1\\) for the model \\(y_i = \\beta_1x_i + \\epsilon_i\\).\n\nComment on these estimates compared to the previous question. What happened to \\(\\bar x\\) and \\(\\bar y\\)?\nInvestigate what happens to \\(\\hat\\beta_1\\) if the true intercept is not 0.\n\n\n\n\nSolution\n\nWe’re trying to minimize: \\[\n\\sum_{i=1}^n(\\epsilon_i)^2 = \\sum_{i=1}^n(y_i - \\beta_1x_i)^2\n\\] Setting the derivative to 0 (and putting on our hats, since we’re now working with the value of \\(\\beta_1\\) that minimizes the equation, not the true value of \\(\\beta_1\\)): \\[\n0 = -2\\sum(y_i -\\hat\\beta_1x_i)x_i = -2\\sum(y_ix_i) + 2\\hat\\beta_1\\sum x_i^2\\implies\\hat\\beta_1 = \\frac{\\sum x_iy_i}{\\sum x_i^2}\n\\]\nNote that this is the exact same formula as the estimate for \\(\\hat\\beta_1\\) in the model with an intercept, except with \\(\\bar x = \\bar y = 0\\). In the intercept model, the line is guaranteed to pass through the point \\((\\bar x, \\bar y)\\) (prove this!). In the intercept model, the line is guaranteed to pass through the point \\((0, 0)\\).\nIn the following simulation, the true slope is the exact same no matter what. However, the true intercept varies. When \\(\\beta_0 = 0\\), we get the true value for \\(\\beta_1\\). When the true intercept is negative, the estimate for the slope is too low. When \\(\\beta_0\\) is positive, the slope is too high. The plots on the right show why this is the case.\nFor further homework, change the range of x (to be far from 0 or to cover 0), the value of beta1 (try small, large, and negative values), and the standard deviation of e (small and large) and see if you can guess the pattern before you run the code.\n\nn &lt;- 75\nx &lt;- runif(n, 0, 10)\nbeta1 &lt;- 5\n\nbeta_hats &lt;- c()\nbeta0s &lt;- seq(-10, 10, by = 1)\nfor (beta0 in beta0s) {\n    e &lt;- rnorm(n, 0, 3)\n    y &lt;- beta0 + beta1*x + e\n    beta1_est &lt;- sum(x * y) / sum(x^2)\n    beta_hats &lt;- c(beta_hats, beta1_est)\n}\n\nlayout(matrix(c(1,1,2,1,1,3,1,1,4), ncol = 3, byrow = TRUE))\n\nplot(beta0s, beta_hats)\nabline(h = 5, col = 2, lwd = 3)\n\nbeta0 &lt;- -10\ne &lt;- rnorm(n, 0, 3)\ny &lt;- beta0 + beta1*x + e\nplot(y ~ x, main = \"True b0 = -10\")\nabline(a = 0, b = sum(x * y) / sum(x^2), col = 2, lwd = 3)\n\nbeta0 &lt;- 0\ne &lt;- rnorm(n, 0, 3)\ny &lt;- beta0 + beta1*x + e\nplot(y ~ x, main = \"True b0 = 0\")\nabline(a = 0, b = sum(x * y) / sum(x^2), col = 2, lwd = 3)\n\nbeta0 &lt;- 10\ne &lt;- rnorm(n, 0, 3)\ny &lt;- beta0 + beta1*x + e\nplot(y ~ x, main = \"True b0 = 10\",\n    ylim = c(0, max(y)))\nabline(a = 0, b = sum(x * y) / sum(x^2), col = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\n\n\nGiven that \\(\\hat\\beta_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i=1}^n(x_i - \\bar x)^2}\\), find \\(E(\\hat\\beta)\\).\nComplete the derivation of \\(V(\\hat\\beta_1)\\).\nSuppose that we fit the model without an intercept, but there actually is an intercept term. That is, suppose \\(y_i = \\beta_0 + \\beta_1x_i +\\epsilon_i\\), but our estimator is what was found in Question 2, and find \\(E(\\hat\\beta_1)\\). Hint: it’s not \\(\\beta_1\\)!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fitting Straight Lines</span>"
    ]
  },
  {
    "objectID": "L03-Residuals.html",
    "href": "L03-Residuals.html",
    "title": "3  Assessing Fit",
    "section": "",
    "text": "3.1 Analysis of Variance",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assessing Fit</span>"
    ]
  },
  {
    "objectID": "L03-Residuals.html#analysis-of-variance",
    "href": "L03-Residuals.html#analysis-of-variance",
    "title": "3  Assessing Fit",
    "section": "",
    "text": "Statistics is the Study of Variance\n\nGiven a data set with variable \\(\\underline y\\), \\(V(\\underline y) = \\sigma^2_y\\).\n\nThis is just the variance of a single variable.\n\nOnce we’ve incorporated the linear relationship with \\(\\underline x\\), \\(V(\\hat\\beta \\underline x + \\underline\\epsilon)=0+V(\\underline\\epsilon) = \\sigma^2\\)\n\nMathematically, \\(\\sigma^2 \\le \\sigma_y^2\\).\n\nHomework: when are they equal?\n\n\n\nThe variance in \\(Y\\) is explained by \\(X\\)!\nThis intuition is going to be very important: there’s variance in \\(y\\), but incorporating \\(x\\) explains some of that variability. Instead of variance around the mean of \\(y\\), we’re looking at the variance around the line.\nWhy is \\(\\sigma^2 &lt; \\sigma^2_y\\)? Imagine a horizontal line at \\(\\bar y\\) (also show that \\(\\hat\\beta_0 = \\bar y\\) is the least squares estimator of the model \\(y_i = \\beta_0 + \\epsilon\\)). The variance above and below the line is simply the variance of \\(y\\), i.e. \\(\\sigma^2_y\\). This is the absolute worst case scenario. If that line is moved to fit the data as well as possible, then the variance above and below the line will be smaller!\n\n\nA Useful Identity, and its Interpretations\n\\[\\begin{align*}\n\\hat\\epsilon_i &= y_i - \\hat y_i \\\\&= y_i - \\bar y - (\\hat y_i - \\bar y)\\\\\n\\implies ... \\implies \\sum_{i=1}^n\\hat\\epsilon_i^2 &= \\sum_{i=1}^n(y_i-\\bar y)^2 - \\sum_{i=1}^n(\\hat y_i - \\bar y)^2\n\\end{align*}\\] where we’ve simply added and subtracted \\(\\bar y\\). The final line skips a few steps (try them yourself, or see textbook page 28)!\nNote that the “…” is actually fairly difficult to prove. You will need to plug in the least squares estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) in order to get to the final answer.\nThe last line is often written as: \\(SS_E = SS_T - SS_{Reg}\\).\n\n\nSums of Squares\n\\[\nSS_E = SS_T - SS_{Reg}\n\\]\n\n\\(SS_E\\): Sum of Squared Errors\n\\(SS_T\\): Sum of Squares Total (i.e., without considering \\(\\underline x\\))\n\\(SS_{Reg}\\): Sum of Squares due to the regression.\n\nIt’s the variance of the line (calculated at observed \\(\\underline x\\) values) around the mean of \\(\\underline y\\)???\n\nThis is incredibly useful, but weird.\n\nI use \\(SS_{Reg}\\) instead of \\(SS_R\\)\n\nSome textbooks use \\(SS_R\\) as SS Residuals (same as \\(SS_E\\)), which is confusing.\n\n\n\n\n\nAside: Degrees of Freedom\nDef: The number of “pieces of information” from \\(y_1, y_2, ..., y_n\\) to construct a new number.\n\nIf I have \\(x = (1,3,2,1,3,???)\\) and I know that \\(\\bar x = 2\\), I can recover the missing piece.\n\nThe mean “uses” (accounts for) one degree of freedom\n\nIf I have \\(x = (1,2,3,1,???,???)\\) and I know \\(\\bar x = 2\\) and \\(s_x^2=1\\), I can recover the two missing pieces.\n\nThe variance accounts for two degrees of freedom.\n\nOne \\(df\\) is required to compute it.\n\n\n\nEstimating one parameter takes away one degree of freedom for the rest!\n\nCan find \\(\\bar x\\) when \\(x = (1)\\), but can’t find \\(s_x^2\\) because there aren’t enough \\(df\\)!\n\n\n\nSums of Squares in an ANOVA Table\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegress of Freedom \\(df\\)\nSum of Squares (\\(SS\\))\nMean Square (\\(MS\\))\n\n\n\n\nRegression\n1\n\\(\\sum_{i=1}^n(\\hat y_i - \\bar y)^2\\)\n\\(MS_{Reg}\\)\n\n\nError\n\\(n-2\\)\n\\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\)\n\\(MS_E=s^2\\)\n\n\nTotal (corrected)\n\\(n-1\\)\n\\(\\sum_{i=1}^n(y_i - \\bar y)^2\\)\n\\(s_y^2\\)\n\n\n\n\nNotice that \\(SS_T = SS_{Reg} + SS_E\\), which is also true for the \\(df\\) (but not \\(MS\\)). \nWhy is \\(df_E = n-2\\)? What two parameters have we estimated?\n\n\\(df_{Reg}\\) is trickier to explain. It suffices to know that \\(df_{Reg} = df_T-df_E\\).\n\n“Corrected”: We estimated the mean of \\(\\underline y\\), rather than just \\(\\sum y_i^2\\).\n\nThis used a degree of freedom!\n\n\n\n\nUsing Sums/Means of Squares\n\nIf \\(\\hat y_i = \\bar y\\) for all \\(i\\), then we have a horizontal line!\n\nThat is, there is no relationship between \\(\\underline x\\) and \\(\\underline y\\).\nIn this case, \\(SS_{reg} = \\sum_{i=1}^n(\\hat y_i - \\bar y)^2 = 0\\).\n\n\nOkay, so, just test for \\(SS_{Reg} = 0\\)?\nBut how??? We need some measure of how far from 0 is statistically significant!!!\nRecall that \\(SS_E = \\sum_{i=1}^n(y_i - \\hat y_i)^2\\).\n\nWe can compare the variation around the line to the variation of the line.\n\nThis is \\(MS_{Reg}/MS_E\\), and it follows an \\(F\\) distribution!!\n\n\nStatistics is the process of adding context to numbers. We expect a model in which there’s no relationship to have an \\(SS_{Reg}\\) of 0. Due to random sampling, we’re never actually going to get this.\n\n\nShow the code\nx &lt;- runif(50, -5, 5)\nssreg_vals &lt;- double(1000)\nfor (i in 1:1000) {\n    # There is no relationship here!\n    y &lt;- 4 + 0*x + rnorm(50, 0, 3)\n    mylm &lt;- lm(y ~ x)\n    yhat &lt;- predict(mylm)\n    ybar &lt;- mean(y)\n    ssreg_vals[i] &lt;- sum((yhat - ybar)^2)\n}\nhist(ssreg_vals)\n\n\n\n\n\n\n\n\n\nShow the code\nsum(ssreg_vals == 0)\n\n\n[1] 0\n\n\nThere’s no relationship, but none of the estimated values of \\(SS_{Reg}\\) are 0! Some of them were as high as 80, even though there’s truly no relationship!\nSo, what value of \\(SS_{Reg}\\) counts as “too large”? We actually don’t test this directly. Because of the way we chose to do statistics, it’s actually quite difficult to create a test at the boundary of possible values (in this case, testing if \\(SS_{Reg} = 0\\) when \\(SS_{Reg}\\) can only be greater than or equal to 0).\n\n\nThe F-test for Significance of Regression\nThe whole point of an ANOVA table is to get down to:\n\\[\nF_{df_{Reg}, df_E} = \\dfrac{MS_{Reg}}{MS_E} = \\dfrac{MS_{Reg}}{s^2}\n\\]\n\nRecall that \\(SS_{Reg} = SS_T - SS_E\\), but the \\(df\\) make a difference.\nFor homework, show that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2\\)\nThis implies that \\(E(MS_{Reg}) &gt; E(MS_E) = \\sigma^2\\).\n\nUNLESS \\(\\beta_1 = 0\\)\n\n\n\n\nExercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)\n\n\nA study was made on the effect of temperature on the yield of a chemical process. The data are shown to the right.\n\nAssuming \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), what are the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\)?\nConstruct the analysis of variance table and test the hypothesis \\(H_0: \\beta_1=0\\) at the 0.05 level.\nWhat are the confidence limits (at \\(\\alpha\\) = 0.05) for \\(\\beta_1\\)?\n\n\n\n\nShow the code\nlibrary(aprean3) # Data from textbook\nhead(dse03a) # Data Set Exercise Ch03A\n\n\n   x  y\n1 -5  1\n2 -4  5\n3 -3  4\n4 -2  7\n5 -1 10\n6  0  8\n\n\nShow the code\nnrow(dse03a)\n\n\n[1] 11\n\n\n\n\nAnswers:\n\n# 1.\ncoef(lm(y ~ x, data = dse03a))\n\n(Intercept)           x \n   9.272727    1.436364 \n\n# 2. \nanova(lm(y ~ x, data = dse03a))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 226.945  226.94   96.18 4.207e-06 ***\nResiduals  9  21.236    2.36                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3.\nconfint(lm(y ~ x, data = dse03a), conf.level = 0.95)\n\n               2.5 %    97.5 %\n(Intercept) 8.225007 10.320447\nx           1.105045  1.767682\n\n\nAs homework, calculate all of these using the formulas in R!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assessing Fit</span>"
    ]
  },
  {
    "objectID": "L03-Residuals.html#rule-number-1-always-plot-everything",
    "href": "L03-Residuals.html#rule-number-1-always-plot-everything",
    "title": "3  Assessing Fit",
    "section": "3.2 Rule Number 1: Always Plot Everything",
    "text": "3.2 Rule Number 1: Always Plot Everything\n\nAnscombe’s Quartet\nConsider the following four data sets\n\n\n\n\n\ndata_set\nxbar\nsigma_x\nybar\nsigma_y\ncorr\n\n\n\n\n1\n9\n3.316625\n7.500909\n2.031568\n0.8164205\n\n\n2\n9\n3.316625\n7.500909\n2.031657\n0.8162365\n\n\n3\n9\n3.316625\n7.500000\n2.030424\n0.8162867\n\n\n4\n9\n3.316625\n7.500909\n2.030578\n0.8165214\n\n\n\n\n\nCan you guess what the plots look like?\n\n\nAnscombe’s Quartet",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assessing Fit</span>"
    ]
  },
  {
    "objectID": "L03-Residuals.html#the-residual-whats-left-over",
    "href": "L03-Residuals.html#the-residual-whats-left-over",
    "title": "3  Assessing Fit",
    "section": "3.3 The residual: what’s left over",
    "text": "3.3 The residual: what’s left over\n\n\\(R^2\\): percent of variance explained by the regression model\n\n\n\\[\\begin{align*}\nR^2 &= \\frac{SSReg}{SST}\\\\& = \\frac{S_{XY}^2}{S_{XX}S_{YY}}\n\\end{align*}\\]\n\n\n\nShow the code\nlayout(mat = matrix(c(1,2,3), nrow = 1), widths = c(0.5,1,1))\nset.seed(18)\nx &lt;- runif(25, 0, 10)\ny &lt;- rnorm(25, 2 + 5*x, 6)\n\nplot(rep(1, 25), y, xlab = \"y\", ylab = \"y has variance\", xaxt = \"n\")\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nplot(x, y, ylab = \"There's variance around the line\")\nabline(lm(y~x))\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nmids &lt;- predict(lm(y~x))\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(y[i], mids[i]), col = 1)\n}\n\nmids &lt;- predict(lm(y~x))\nplot(mids ~ x, type = \"n\", ylab = \"The line varies around the mean of y!\")\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(mean(y), mids[i]))\n}\naxis(2, at = mean(y), labels = bquote(bar(y)), las = 1)\nabline(h = mean(y))\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidual Assumptions\n\nResidual: what’s left over\n\n\\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\n\nAssumptions:\n\n\\(E(\\epsilon_i) = 0\\)\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(\\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nWe must check our assumptions!\n\nThere are statistical tests, but they’ll never tell you as much as a plot!\n\nThe statistical tests try to give a p-value for the hypothesis that the residuals are normal, but looking at the residual plot will always be superior!\n\n\nResiduals versus fitted values: \\(\\hat{\\underline\\epsilon}\\) versus \\(\\hat{\\underline{y}}\\)\n\n\nWhy \\(\\hat{\\underline{y}}\\) instead of \\(\\underline y\\)?\n\nSee text. Try a regression of \\(\\hat{\\underline\\epsilon}\\) versus \\(\\underline{y}\\) yourself (mathematically and with code).\n\nWhy not \\(\\underline x\\)?\n\nFor simple linear regression, \\(\\hat{\\underline{y}}\\) is like a unit change for \\(\\underline x\\), so it doesn’t matter.\n\nFor multiple linear regression, it’s easier to have one variable for the \\(x\\) axis.\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\nlibrary(broom)\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins[complete.cases(penguins),]\n\ng1 &lt;- ggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\",\n        title = \"y versus x\")\n\nplm &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\np2 &lt;- augment(plm)\n\ng2 &lt;- ggplot(p2) + \n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\n\ng1 / g2\n\n\n\n\n\n\n\n\n\n\n\nI just want to add a little more context to the “unit change” idea. In a linear model, the estimated height of the line is \\(\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1x_i\\). To go from Celcius to Fahrenheit, we use the equation \\(F_i = 32 + \\frac{9}{5}C_i\\), where \\(C_i\\) is the Celcius value that we have and \\(F_i\\) is the Fahrenheit value that we get. A line is literally a change of units!\n\n\nResidual Plots and Assumption Checking\nMathematics is the process of making assumptions and seeing if we can break them.\n\n\\(E(\\epsilon_i) = 0\\) is a given since \\(\\sum_{i=1}^n\\hat\\epsilon_i=0\\).\n\\(V(\\epsilon_i) = \\sigma^2\\) (regardless of \\(i\\))\n\nCheck if the variance looks stable.\n\n\\(\\epsilon_i \\sim N(0,\\sigma^2)\\) is harder to see on a residuals versus fitted plot\n\nExpect more points close to 0, fewer further away, no outliers\n\n\n\n\nResidual plots: unstable error, a.k.a. Heteroscedasticity\n\n\nShow the code\nx &lt;- runif(200, 0, 10)\ny0 &lt;- 2 - 3*x\ny &lt;- y0 + rnorm(length(x), 0, 2 * x)\n\ng1 &lt;- ggplot() + \n    aes(x = x, y = y) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = NULL, y = NULL, title = \"y versus x\")\n\nxydf &lt;- augment(lm(y ~ x))\n\ng2 &lt;- ggplot(xydf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\ng1 + g2\n\n\n\n\n\n\n\n\n\nThe idea of heteroscedastic data is that \\(V(\\epsilon_i) = \\sigma^2_i\\), which depends on \\(i\\). The most obvious case is shown above, where the variance increases along the values of \\(x\\). This pattern is obviously present in the plot of \\(y\\) versus \\(x\\), but this can be very hard to see in higher dimensions!\n\n\nResidual plots: non-linear trend?\n\n\nShow the code\n## fig-height: 4\n## fig-width: 8\ng1 &lt;- ggplot(mtcars) + \n    aes(x = disp, y = mpg) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = \"Engine Displacement\", y = \"Miles per Gallon\", title = \"y versus x\")\n\nmtdf &lt;- augment(lm(mpg ~ disp, data = mtcars))\n\ng2 &lt;- ggplot(mtdf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_smooth(se = FALSE) +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted - non-linear trend?\")\n\ng1 + g2\n\n\n\n\n\n\n\n\n\nThe plots above show a (mild) violation of the assumption that \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), i.e., that the true trend is linear. This can be seen in the plot of \\(y\\) against \\(x\\), but it’s much more obvious in the residual plot!\nNote that, in this course, the phrase “The Residual Plot” always refers to a plot of residuals against fitted. There are other residual plots, but the residual plot is resids versus fitted.\n\n\nTesting Normality: Quantile-Quantile Plots\n\n\nConsider the data (2,3,3,4,5,5,6).\n\n50% of the data is below the median.\n\nFor a \\(N(0,1)\\) distribution, 50% of the data is below 0.\nPut the median on the y axis, 0 on the x axis.\n\n25% of the data is below Q1.\n\nFor a \\(N(0,1)\\) distribution, 25% is below qnorm(0.25) = -0.67\nPut a point at x = -0.67, y = Q1.\n\n75% of the data is below Q3.\n\nFor a \\(N(0,1)\\) distribution, 75% is below qnorm(0.75) = 0.67\nPut a point at x = 0.67, y = Q3.\n\n… and so on for the rest of the quantiles\n\nSee this link for an example.\n\n\n\nIf perfectly normal, expect a straight line!\n\n\nShow the code\nmydata &lt;- c(2, 3, 3, 4, 5, 5, 6)\nquants &lt;- qnorm(c(\n    0.125, 0.25, 0.375, 0.5, \n    0.625, 0.75, 0.875))\nplot(mydata ~ quants)\n\n\n\n\n\n\n\n\n\n\n\nThe example link is a fun thing to play with! It’s written in Python, so don’t worry about the code. Change the sample sizes to get an idea of just how hard it is to definitively show that data are normal.\n\n\nOther Residual Plots:\nScale-Location\n\nScale: Standardized residual\nLocation: Fitted value\nMore on standardized residuals in Ch08\n\nCook’s Distance\n\nBasically, an outlier detection method.\nMore in Ch08\n\nLeverage\n\nMore in Ch08",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assessing Fit</span>"
    ]
  },
  {
    "objectID": "L03-Residuals.html#maximum-likelihood",
    "href": "L03-Residuals.html#maximum-likelihood",
    "title": "3  Assessing Fit",
    "section": "3.4 Maximum Likelihood",
    "text": "3.4 Maximum Likelihood\n\nMain Idea\nFind the values \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\sigma^2\\) that maximize the likelihood of seeing our data.\nUnder the assumptions that \\(X\\) is fixed, \\(Y = \\beta_0 + \\beta_1X + \\epsilon_i\\), and \\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\), \\[\nY \\sim N(\\beta_0 + \\beta_1X, \\sigma^2)\n\\]\n\n\nThe Likelihood\nThe probability of observering a data point is: \\[\nf_Y(y_i|x_i, \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\nThe likelihood of the parameters, given the data, is: \\[\nL(\\beta_0, \\beta_1, \\sigma^2|x_i, y_i) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\n\nIt’s just a shift in perspective!\n\n\n\nSimple Coin Flip Example\nSuppose we flipped 10 bottle caps and got 6 “crowns”. Assume the probability of “crown” (\\(C\\)) is unknown, labelled \\(p\\).\n\nThe probability of one cap flip is \\(P(C|p) = p\\).\nThe probability of this is \\(P(C = 6|p) = p^6(1-p)^4\\).\n\nThis is just \\(P(\\underline y|p) = \\prod_{i=1}^nP(Y = y_i)\\).\n\nThe likelihood is \\(L(p|\\underline y) = \\prod_{i=1}^nP(Y = y_i)\\).\n\n\n\nMaximizing the Likelihood in LM\n\\[\nL(\\beta_0, \\beta_, \\sigma^2) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\n\nTo maximize w.r.t \\(\\beta_0\\), we set the derivative w.r.t \\(\\beta_0\\) to 0 and solve for \\(\\beta_0\\).\n\n\\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_0} = 0\\).\n\nRepeat for \\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_1} = 0\\) and \\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\sigma^2} = 0\\)\n\nHWK: Show that the estimates for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are the same as the OLS estimates. The estimate for \\(\\hat\\sigma^2\\) should come out to: \\[\n\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i\\right)^2\n\\]\n\n\nInterpreting the MSE\nThe basic question of statistics: “How big is this number?”\n\nCompare to previous studies - is MSE larger than \\(\\sigma^2\\)?\n\nImplies that Bias\\(^2\\) and Fitted Model Variance are larger than expected.\nF-test\n\nCompare to “pure error” - direct estimate of \\(\\sigma^2\\).\n\ni.e. the variance in repeated trials on the same covariate values\nTextbooks devotes a lot to this, but it’s often not plausible.\n\nWon’t be on tests!\n\n\nCompare to another model\n\nWe’ll focus on this (later)!\n\n\n\n\nCompare to Previous Studies\nHypothesis test for \\(\\sigma^2 = \\sigma_0^2\\) versus \\(\\sigma^2 &gt; \\sigma_0^2\\), where \\(\\sigma_0\\) is the value from a previous study.\n\nIf significant, some of your error is coming from the study design!\n\n\n\nCompare to other models\nIt can be shown that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1S_{XX}\\).\nConsider the Null hypothesis \\(\\beta_1 =0\\) (why is this a good null?).\n\nUnder this null, \\(\\frac{MS_{Reg}}{s^2}\\sim F_{1, n-2}\\).\n\nObvious CI from this. \n\nThis is exactly equivalent to the t-test for \\(\\beta_1\\)! (See text.)\n\n\n\nMSE of a Parameter: Bias of \\(s^2\\)\nFrom a previous class, we know that \\[\n\\frac{(n-2)s^2}{\\sigma^2}\\sim\\chi^2_{n-2}\n\\]\nFrom wikipedia, we know that the mean of a \\(\\chi^2_k\\) distribution is \\(k\\). Therefore, \\[\nE\\left(\\frac{(n-2)s^2}{\\sigma^2}\\right) = n-2 \\Leftrightarrow E(s^2) = \\sigma^2\n\\] and thus \\(s^2\\) is unbiased.\nThis does not necessarily mean that \\(s^2\\) is the best estimator for \\(\\sigma^2\\)!\n\n\nMSE of a Parameter: Bias of \\(s\\)\nEven though \\(s^2\\) is an unbiased estimator, \\(s = \\sqrt{s^2}\\) is biased! Specifically, \\(E(s) &lt; \\sigma\\)\nTo see why, first note that \\[\nV(s) = E(s^2) - (E(s))^2 \\Leftrightarrow E(s) = \\sqrt{E(s^2) - V(s)}\n\\] since \\(V(s) &gt; 0\\), \\(E(s^2) - V(s) &lt; E(s^2)\\), and therefore \\[\nE(s) &lt; \\sqrt{E(s^2)} = \\sqrt{\\sigma^2} = \\sigma\n\\]\n\n\nSummary\n\nMost of regression involves analysing the residuals.\n\nResiduals are regression.\nResidual plots will all be explained/explored thoroughly.\n\n\\(R^2 = \\frac{SSReg}{SST}\\) is the percent of variance explained.\n\nThis is a very important concept.\n\nMLE usually involves writing out a product of normals\n\nTake the ln(), then derivative, then set to 0 and solve.\n\nThe MSE is best interpreted relative to a hypothesis or other model.\n\nThe raw value is sometimes - but rarely - useful.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assessing Fit</span>"
    ]
  },
  {
    "objectID": "L03-Residuals.html#exercises",
    "href": "L03-Residuals.html#exercises",
    "title": "3  Assessing Fit",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\nRecommended textbook exercises: A, C, E, F, H, I, J, O, P, W\n\n\nSolution to A\n\n\nlibrary(aprean3) # install.packages(\"aprean3\")\ndata(dse03a)\nhead(dse03a)\n\n   x  y\n1 -5  1\n2 -4  5\n3 -3  4\n4 -2  7\n5 -1 10\n6  0  8\n\n\n\n# part 1\nlm(y ~ x, data = dse03a) |&gt; coef()\n\n(Intercept)           x \n   9.272727    1.436364 \n\n\nThe prediction equation is \\[\n\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1x_i =  9.27 + 1.436 * x_i\n\\]\n\n# part 2\nlm(y ~ x, data = dse03a) |&gt; anova() # Reject the null, beta_1 is sig. diff. from 0\n\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 226.945  226.94   96.18 4.207e-06 ***\nResiduals  9  21.236    2.36                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# part 3\ne3lm &lt;- lm(y ~ x, data = dse03a)\nX &lt;- cbind(1, dse03a$x)\nXTX1 &lt;- solve(t(X) %*% X)\nbetas &lt;- XTX1 %*% t(X) %*% dse03a$y\ndf &lt;- nrow(dse03a) - 2\npreds &lt;- X %*% betas\ns &lt;- sqrt(sum((dse03a$y - preds)^2) / df)\nXTX &lt;- t(X) %*% X\nbetas[2] + c(1, -1) * qt(0.025, df = df) * s / sqrt(XTX[2, 2])\n\n[1] 1.105045 1.767682\n\nconfint(e3lm) # check that it's correct\n\n               2.5 %    97.5 %\n(Intercept) 8.225007 10.320447\nx           1.105045  1.767682\n\n\n\n\n\nShow that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2\\). *Hint: \\(V(y_i) = E(y_i^2) - E(y_i)^2\\), and you’ll need to plug in the estimates for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\nUse R to calculate the values in the ANOVA table, i.e. the “Sum of Squares” section, for a linear regression of waiting versus eruptions in the faithful data. Compare the the anova() output, shown below.\n\n\ndata(faithful)\n\ny &lt;- faithful$waiting\ny_bar &lt;- mean(y)\nn &lt;- length(y)\n\nfaithful_lm &lt;- lm(waiting ~ eruptions, data = faithful)\ny_hat &lt;- predict(faithful_lm)\n\nanova(faithful_lm)\n\nAnalysis of Variance Table\n\nResponse: waiting\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \neruptions   1  40644   40644  1162.1 &lt; 2.2e-16 ***\nResiduals 270   9443      35                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nUse a simulation to demonstrate that, assuming \\(\\beta_1 = 0\\), \\(MS_{Reg} / MS_E \\sim F_{df_{Reg}, df_E}\\).\n\nExplain why this is only true when \\(\\beta_1 = 0\\)?\n\n\n\n\nSolution\n\n\nn &lt;- 100\nx &lt;- runif(n, 0, 10)\n\nf_vals &lt;- c()\nmsreg_vals &lt;-c()\nfor (i in 1:1000) {\n    e &lt;- rnorm(n, 0, 1)\n    y &lt;- -3 + 0.005*x + e\n    mylm &lt;- lm(y ~ x)\n    \n    y_hat &lt;- predict(mylm)\n    y_bar &lt;- mean(y)\n    ms_reg &lt;- sum((y_hat - y_bar)^2) / 1\n    msreg_vals[i] &lt;- ms_reg\n    ms_e &lt;- sum((y - y_hat)^2) / (n - 2)\n    f_vals &lt;- c(f_vals, ms_reg / ms_e)\n}\n\nhist(f_vals, freq = FALSE, breaks = 40)\ncurve(df(x, 1, n - 2), col = 2, lwd = 3, add = TRUE)\n\n\n\n\n\n\n\n\nWhy is this only true when \\(\\beta_1=0\\)? For intuition, this only should be true when the null is true: recall that p-values are always calculated assuming that the null is true, and the F-stat is no different.\nMathematically, this has to do with the fact that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1S_{XX}\\), not just \\(\\sigma^2\\). Since this is not equal to \\(\\sigma^2\\), this does not follow a Chi-Square distribution.\n\npar(mfrow = c(1, 2))\nhist(msreg_vals, breaks = 40, freq = FALSE)\ncurve(dchisq(x, 1), add = TRUE, col = 2, lwd = 3)\n\n# qqplot, just to check\nqqplot(x = qchisq(ppoints(length(msreg_vals)), 1), y = msreg_vals)\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\n\n… actually it kinda does follow a Chi-Square distribution. Not sure what to make of that.\n\n\n\nShow that \\(\\frac{SSReg}{SST} = \\frac{S_{XY}^2}{S_{XX}S_{YY}}\\).\nExplore why we use residuals versus fitted, rather than residuals versus observed.\n\nFit a regression, then plot \\(\\underline\\epsilon\\) versus \\(\\underline y\\). What do you notice?\nFind \\(cor(\\epsilon_i, y_i)\\) mathematically. Compare this with what you see in the plot.\n\n\n\n\nSolution\n\n\nn &lt;- 100\nx &lt;- runif(n, 0, 10)\ne &lt;- rnorm(n, 0, 3)\ny &lt;- 5 + 2*x + e\n\nmylm &lt;- lm(y ~ x)\nplot(mylm$residuals ~ y)\n\n\n\n\n\n\n\n\nNote that \\(y_i = \\hat y_i + \\hat \\epsilon_i\\)\n\\[\ncov(\\hat\\epsilon_i, y_i) = cov(\\epsilon_i, \\hat y_i + \\hat \\epsilon_i) = cov(\\epsilon_i, \\hat y_i) + cov(\\hat\\epsilon_i, \\hat \\epsilon_i) = 0 + \\sigma^2\n\\]\nEmpiricially:\n\ncov(mylm$residuals, y)\n\n[1] 7.987392\n\nvar(mylm$residuals)\n\n[1] 7.987392\n\n\nThis is why we use residuals versus fitted, not residuals versus observed!\n\n\n\nShow that the estimates for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are the same as the OLS estimates.\nFind the MLE estimate for \\(\\sigma^2\\).\n\nComment on the difference between this formula and the usual variance formula.\nComment on the variance formula from least squares\n\n\n\n\nSolution\n\nDerivation left as an exercise. Hint: You are allowed to take a derivative with respect to \\(\\sigma^2\\), not just \\(\\sigma\\).\n\nMLE versus usual formula: We’re dividing by \\(n\\), not \\(n - 1\\) or even \\(n - 2\\)! The mle is actually biased. We’ll explore this in the next question.\nThere is no variance formula in least squares! MLE allows us to incorporate our assumption about the distribution of the residuals, giving us more to work with!\n\n\n\n\nThe MLE of \\(\\sigma^2\\) divides by \\(n\\). Demonstrate via simulation that this is actually a biased estimate.\n\n\n\nSolution\n\n\nn &lt;- 100\ntrue_sigma &lt;- 3\n\nn_minus &lt;- seq(-3, 3, 1)\nvals &lt;- matrix(ncol = length(n_minus), nrow = 10000)\nfor (i in 1:10000) {\n    y &lt;- rnorm(n, 0, true_sigma)\n    vals[i, ] &lt;- sum((y - mean(y))^2) / (n - n_minus)\n}\n\napply(vals, 2, mean) |&gt; plot(x = n_minus, y = _)\nabline(h = true_sigma^2, lwd = 3, col = 2)\n\n\n\n\n\n\n\n\nFrom the plot, we get closest to the true value when we subtract 1 from \\(n\\). However, the MLE subtracts nothing from \\(n\\)! Why not? Let’s check the MSE.\n\napply(vals, 2, function(x) {\n    sum((x - true_sigma^2)^2)\n}) |&gt; plot(x = n_minus, y = _)\n\n\n\n\n\n\n\n\nThe MSE is lowest when we divide by… \\(n+1\\)???? Yes, this is just how it is. See here for a bit more discussion on this issue. Basically, the estimator with \\(n-1\\) is unbiased, and \\(n\\) is lower variance without too much bias, while \\(n+1\\) is much more bias than we’re comfortable with.\n\n\n\nProve that \\(\\sum_i=1^n\\hat\\epsilon_i = 0\\). Really try it yourself, then check the hint below only if your’re struggling.\n\n\n\nHint\n\n\\(\\hat\\epsilon_i = y_i - \\hat\\beta_0 - \\hat\\beta_1x_i\\), \\(\\sum_{i=1}^ny_i = n\\bar y\\), and \\(\\beta_0 = \\bar y - \\hat\\beta_1x_i\\). *****\n\n\nConsider the following plot, where the grey line is the mean model (where \\(\\beta_1 = 0\\)), the green line represents the simple linear regression of mpg versus displacement, and the red line represents a model where the intercept is forced to be 0 (I use this a lot because it’s easy to conceptualize and the math is easy, not because it’s usually relevant to applications). Using R, calculate the anova table for both models and interpret the results.\n\n\nggplot(mtcars) + \n    aes(x = disp, y = mpg) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ 1, colour = \"darkgrey\", linewidth = 2) +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, colour = \"green\", linewidth = 2) +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ 0 + x, colour = \"red\", linewidth = 2)\n\n\n\n\n\n\n\n# Model with an intercept (the usual model)\nlm_with &lt;- lm(mpg ~ disp, data = mtcars)\n# Using \"0 +\" forces the intercept to be 0\nlm_without &lt;- lm(mpg ~ 0 + disp, data = mtcars)\n\n\n\nSolution\n\n\nanova(lm_with)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm_without)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndisp       1 7599.9  7599.9   36.57 1.073e-06 ***\nResiduals 31 6442.4   207.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSome interpretations:\n\nFor lm_with:\n\n\\(MS_{Reg} = 808.89\\) and \\(MS_E = 10.57\\), which are clearly different numbers (even with a sample size of 32, which you can see from \\(df_E = 30 = n - 2\\)).\n\nThe model is clearly different from just using the mean!\nThe p-value confirms this.\n\n\nFor lm_without:\n\nAgain, the model is clearly “different” from just using the mean, but it’s not necessarily better than using it. The graph shows that this model fits very poorly, the variance is just significantly different!\nNote that the degrees of freedom is \\(n - 1\\), since we’re only estimating one parameter (the slope).\n\nComparison:\n\n\\(MS_{E}^{(with)} = 10.57\\) and \\(MS_E^{(without)} = 207.8\\). Clearly the model with an intercept is fitting much much better!\n\nNotice: we did not look at \\(MS_{E}^{(with)} = 10.57\\) and conclude that it was clearly a small value. The MSE is really hard to interpret on it’s own, but it’s quite useful for comparisons!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assessing Fit</span>"
    ]
  },
  {
    "objectID": "L04-Matrix_Form.html",
    "href": "L04-Matrix_Form.html",
    "title": "4  Regression in Matrix Form",
    "section": "",
    "text": "4.1 Putting Things in Boxes",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression in Matrix Form</span>"
    ]
  },
  {
    "objectID": "L04-Matrix_Form.html#putting-things-in-boxes",
    "href": "L04-Matrix_Form.html#putting-things-in-boxes",
    "title": "4  Regression in Matrix Form",
    "section": "",
    "text": "Matrix Forms of Things We’ve Seen\nUsing \\[\nY = \\begin{bmatrix}Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_n\\end{bmatrix};\\quad \\underline y = \\begin{bmatrix}y_1\\\\ y_2\\\\ \\vdots\\\\ y_n\\end{bmatrix};\\quad X = \\begin{bmatrix}1 & x_1\\\\1 &x_2\\\\ \\vdots & \\vdots\\\\ 1 & x_n\\end{bmatrix};\\quad \\underline\\beta = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix};\\quad\\underline\\epsilon = \\begin{bmatrix}\\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\end{bmatrix}\n\\]\n\\(Y = X\\underline\\beta + \\underline\\epsilon\\) is the same as \\[\\begin{align*}\nY_1 &= \\beta_0 + \\beta_1x_1 + \\epsilon_1\\\\\nY_2 &= \\beta_0 + \\beta_1x_2 + \\epsilon_2\\\\\n&\\vdots\\\\\nY_n &= \\beta_0 + \\beta_1x_n + \\epsilon_n\\\\\n\\end{align*}\\]\n\n\nSome Fun Matrix Forms\n\\[\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= \\epsilon_1^2 + \\epsilon_2^2 + ... + \\epsilon_n^2 = \\sum_{i=1}^n\\epsilon_i^2\\\\\nY^TY &= \\sum Y_i^2\\\\\n\\mathbf{1}^T\\underline y &= \\sum y_i = n\\bar y\\\\\nX^TX &= \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\;\\text{ (show this)}\\\\\n(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\\text{ (show this)}\\\\\nX^TY &= \\begin{bmatrix}\\sum Y_i\\\\ \\sum x_iY_i\\end{bmatrix}\n\\end{align*}\\]\n\n\nThe “Normal” Equations\n\n\nCopied from previous slide: \\[\\begin{align*}\nX^TX &= \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\\\\nX^T\\underline y &= \\begin{bmatrix}\\sum y_i\\\\ \\sum x_iy_i\\end{bmatrix}\n\\end{align*}\\]\n\nThe textbook included the following equations in Ch02. The estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) in OLS (same as MLE) are the solution to:\n\\[\\begin{align*}\nn\\hat\\beta_0 + \\hat\\beta_1\\sum x_i &= \\sum y_i\\\\\n\\hat\\beta_0\\sum x_i + \\hat\\beta_1\\sum x_i^2 &= \\sum x_iy_i\\\\\n\\end{align*}\\]\n\n\nPutting these together: \\[\nX^TX\\hat{\\underline\\beta} = X^T\\underline y\\quad \\Leftrightarrow\\quad \\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\n\\] Try this out using the definition of \\((X^TX)^{-1}\\) on the previous slide.\n\n\nVariance of a Vector: the Variance-Covariance Matrix\nIn general, for vector-valued random variable \\(Z = (Z_1, Z_2, \\dots, Z_n)\\), \\[\nV(Z) = \\begin{bmatrix}\nV(Z_1) & cov(Z_1, Z_2) & \\cdots & cov(Z_1, Z_n) \\\\\ncov(Z_2, Z_1) & V(Z_2) & \\cdots &  cov(Z_2, Z_n)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\ncov(Z_n, Z_1) & cov(Z_n, Z_2) & \\cdots & V(Z_n)\n\\end{bmatrix}\n\\]\n\n\n\\(V(\\hat{\\underline\\beta})\\)\n\n\nLet’s start with the covariance of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\): \\[\\begin{align*}\ncov(\\hat\\beta_0, \\hat\\beta_1) &= cov(\\bar y - \\hat\\beta_1\\bar x, \\hat\\beta_1)\\\\\n&= -\\bar x cov(\\hat\\beta_1, \\hat\\beta_1)\\\\\n&= -\\bar x V(\\hat\\beta_1)\\\\\n&= \\frac{-\\sigma^2\\bar x}{S_{XX}}\n\\end{align*}\\pause\\]\n\nThe var-covar matrix is: \\[\\begin{align*}\nV(\\hat{\\underline\\beta}) &= \\begin{bmatrix}\nV(\\hat\\beta_0) & cov(\\hat\\beta_0, \\hat\\beta_1)\\\\\ncov(\\hat\\beta_0, \\hat\\beta_1) & V(\\hat\\beta_1)\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\sigma^2\\sum x_i^2}{S_{XX}} & \\frac{-\\sigma^2\\bar x}{S_{XX}}\\\\\n\\frac{-\\sigma^2\\bar x}{nS_{XX}} & \\frac{\\sigma^2}{S_{XX}}\n\\end{bmatrix}\\\\\n&= \\frac{\\sigma^2}{nS_{XX}}\\begin{bmatrix}\n\\sum x_i^2 & -n\\bar x\\\\\n-n\\bar x & n\n\\end{bmatrix}\\\\\n&= (X^TX)^{-1}\\sigma^2\n\\end{align*}\\]\n\n\n\n\nVariance of \\(\\hat Y_0\\)\nLet \\(X_0 = [1, x_0]\\) be an arbitrary observation. Then the predicted value of the line at \\(X_0\\), labelled \\(\\hat Y_0\\), is: \\[\\begin{align*}\nV(\\hat Y_0) &= V(X_0\\hat{\\underline\\beta})\\\\\n&= X_0V(\\hat{\\underline\\beta})X_0^T\\\\\n&= \\sigma^2X_0(X^TX)^{-1}X_0\n\\end{align*}\\]\nHWK: Verify that \\(X_0(X^TX)^{-1}X_0\\) is a \\(1\\times 1\\) matrix.\n\n\nVariance of \\(\\hat Y_{n+1}\\)\nFor a new observation, we have the variance of the line plus a new unobserved error \\(\\epsilon_{n+1}\\). \\[\\begin{align*}\nV(\\hat Y_{n+1}) &= V(X_{n+1}\\hat{\\underline\\beta} + \\epsilon_{n+1})\\\\\n&= X_{n+1}V(\\hat{\\underline\\beta})X_{n+1}^T + V(\\epsilon_{n+1})\\\\\n&= \\sigma^2X_{n+1}(X^TX)^{-1}X_{n+1}^T + \\sigma^2\\\\\n&= \\sigma^2\\left(X_{n+1}(X^TX)^{-1}X_{n+1}^T + 1\\right)\n\\end{align*}\\]\nHWK: Verify (empirically or mathematically) that this is smallest when \\(x_0 = \\bar x\\).\n\n\nCI for \\(Y_i\\) (observed or new)\nAssuming \\(X\\) is fixed, \\[\n\\hat Y_i = X_{i,\\cdot}\\hat{\\underline\\beta}\n\\] follows a normal distribution.\nTherefore, \\[\n\\frac{\\hat Y_i - Y_i}{se(\\hat Y_i)}\\sim t_{n - 2}\n\\] and a confidence interval can be found as \\[\n\\hat Y_i \\pm t_{n-2}(\\alpha/2)se(\\hat Y_i)\n\\]\n\n\n(Corrected) ANOVA in Matrix Form\nOn tests, you will be given these formulas if they are relevant!\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\\(MS\\)\n\n\n\n\nRegression (corrected)\n\\(p - 1\\)\n\\(\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\\(SS/df\\)\n\n\nError\n\\(n-p\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\\(SS/df\\)\n\n\nTotal (corrected)\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\\(SS/df\\)\n\n\n\n\nThe “corrected” ANOVA is the ANOVA table for comparing the errors due to \\(\\hat\\beta_1\\) to the errors due to \\(\\hat\\beta_0\\).\nThis is different from comparing a model with \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) to a model with neither parameter.\n\nThe value of the “correction” is based on \\(\\bar y\\). If the slope is 0, then the estimate for \\(\\hat\\beta_0\\) is \\(\\bar y\\).\n\n\n\n\nSummary\nWhen we have one predictor, it is clear that: \\[\\begin{align*}\nY &= X\\hat{\\underline\\beta} + \\underline\\epsilon\\\\\n\\hat{\\underline\\beta} &= (X^TX)^{-1}X^T\\underline y\\\\\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2\\\\\nV(\\hat Y_0) &= \\sigma^2X_0(X^TX)^{-1}X_0^T\\\\\nV(\\hat Y_{n+1}) &= \\sigma^2(X_{n+1}(X^TX)^{-1}X_{n+1}^T + 1)\\\\\n\\end{align*}\\]\nThis will not change when we add predictors!\nSee the R Notes for some good examples! I’ll probably go over some of these in class.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression in Matrix Form</span>"
    ]
  },
  {
    "objectID": "L04-Matrix_Form.html#exercises",
    "href": "L04-Matrix_Form.html#exercises",
    "title": "4  Regression in Matrix Form",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\nSuggested textbook exercises: A, B, D, F.\n\nShow that \\(X^TX = \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\). Hint: the first column of \\(X\\) is all 1s, and this is always important. Also find the inverse of this 2x2 matrix.\nVerify that \\(X^TX\\hat{\\underline\\beta} = X^T\\underline y\\) is solved by \\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\\) (i.e., plug \\(\\hat{\\underline\\beta}\\) into the first equation and simplify).\nShow that \\(V(\\hat\\beta_0) = \\frac{\\sigma^2\\sum x_i^2}{S_{XX}}\\) and \\(V(\\hat\\beta_1) = \\frac{\\sigma^2}{S_{XX}}\\). Hint: Show \\(V(\\hat\\beta_1)\\) first, then use the fact that \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\nVerify that \\(X_0(X^TX)^{-1}X_0\\) is a scalar.\nVerify (empirically or mathematically) that \\(V(\\hat Y_{n + 1})\\) is smallest when \\(x_0 = \\bar x\\).\nExplain why \\(V(\\hat y_{n+1}) &gt; V(\\hat y)\\). In other words, explain why there’s less variance when predicting values in our data rather than predicting values outside our data.\n\n\n\nSolution\n\nThe values in our data set were used to fit the model, and thus we’ve already dealt with them as a source of variance. A new data point comes with a new source of variance, and this is accounted for in \\(V(\\hat y_{n+1})\\).\nSomewhat paradoxically, we get two different variances depending on whether we’re treating it as an observation or a new value!\n\n\nFor the next few questions, we’ll use the mtcars data. In the following block of code, I will set up some of the matrices for you!\n\ndata(mtcars)\ny &lt;- mtcars$mpg\nX &lt;- cbind(1, mtcars$wt)\n# (X^TX)^-1\nXTX1 &lt;- solve(t(X) %*% X) \n\n\nVerify that \\(\\hat{\\underline{\\beta}} = (X^TX)^{-1}X^T\\underline y\\) is the same as the output from R.\nVerify via matrix multiplication in R that \\(\\hat{\\underline{\\beta}}\\) is the solution to \\(X^TX\\underline{\\hat\\beta} = X^T\\underline y\\).\nusing the matrix forms, verify that \\(V(\\hat\\beta_0)\\) and \\(V(\\hat\\beta_1)\\) (using \\(s^2\\) rather than \\(\\sigma^2\\)) are what are found in the summary table of a linear model in R.\n\n\n\nSolutions to questions 7-9\n\n\n# 7.\ncoef(lm(mpg ~ wt, data = mtcars))\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\nXTX1 %*% t(X) %*% y\n\n          [,1]\n[1,] 37.285126\n[2,] -5.344472\n\n# 8.\nbeta &lt;- XTX1 %*% t(X) %*% y\nall.equal(\n    t(X) %*% X %*% beta,\n    t(X) %*% y\n)\n\n[1] TRUE\n\n# 9.\nsummary(lm(mpg ~ wt, data = mtcars))$coef[, \"Std. Error\"]\n\n(Intercept)          wt \n   1.877627    0.559101 \n\nsigma &lt;- summary(lm(mpg ~ wt, data = mtcars))$sigma\nSxx &lt;- sum((mtcars$wt - mean(mtcars$wt))^2)\nn &lt;- length(mtcars$mpg)\nc(\n    sqrt(sigma^2 * sum(mtcars$wt^2) / (n * Sxx)),\n    sqrt(sigma^2 / (Sxx))\n)\n\n[1] 1.877627 0.559101\n\n\n\n\n\nUsing simulation, verify that \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are negatively correlated. Show a plot of \\(\\hat\\beta_1\\) versus \\(\\hat\\beta_0\\) from each of the simulated data sets.\n\n\n\nSolution\n\n\nn &lt;- 100\nx &lt;- runif(n, 0, 10)\nX &lt;- cbind(1, x)\n\nbeta0s &lt;- beta1s &lt;- double(1000)\nfor (i in 1:1000) {\n    e &lt;- rnorm(n, 0, 3) # sigma^2 = 9\n    y &lt;- -2 + 3 * x + e\n    beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y\n    beta0s[i] &lt;- beta[1]\n    beta1s[i] &lt;- beta[2]\n}\n\nplot(beta0s, beta1s)\n\n\n\n\n\n\n\n# This is slightly off because cov(b0s, b1s) is based on s, not sigma\ncov(beta0s, beta1s)\n\n[1] -0.04772858\n\n-9 * mean(x) / sum((x - mean(x))^2)\n\n[1] -0.04864048\n\n\n\n\n\nIs it possible to change the simulation above to get a positive correlation?\n\n\n\nSolution\n\nYes! Make it so \\(\\bar x\\) is negative. (It does not matter if the signs of \\(\\beta_0\\) or \\(\\beta_1\\) are positive or negative, it only matters if \\(\\bar x &gt; 0\\).)\n\n\n\nIn the simple linear regression case, derive an estimate for \\(\\beta_1\\) when \\(\\beta_0\\) is a fixed value.\n\n\n\nSolution\n\n\\[\\begin{align*}\nR(\\beta_1) &= \\sum(y_i - \\beta_0 - \\beta_1x_i)^2\\\\\n\\implies \\frac{dR(\\beta_1)}{d\\beta_1} &= -2\\sum(y_ix_i - \\beta_0x_i - \\beta_1x_i^2)\\stackrel{set}{=}0\\\\\n\\implies &= \\sum y_ix_i - \\beta_0\\sum x_i - \\hat\\beta_1\\sum x_i^2\\\\\n\\implies \\hat\\beta_1 &= \\frac{\\sum y_ix_i - \\beta_0\\sum x_i}{\\sum x_i^2}\n\\end{align*}\\]\n\n\n\nUse your answer to question 12 to demonstrate how increasing the intercept will always lead to a decrease in the slope whenever \\(\\bar x &gt; 0\\), thus explaining the negative correlation between \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). Repeat for \\(\\bar x &lt; 0\\).\n\n\n\nSolution\n\n\nn &lt;- 100\nx &lt;- runif(n, 0, 10)\ntrue_beta0 &lt;- 2\ntrue_beta1 &lt;- 4\ny &lt;- true_beta0 + true_beta1 * x + rnorm(n, 0, 3)\nplot(y ~ x)\n\nfor (beta0 in c(-8, -3, 2, 7, 12, 17)) {\n    b1hat &lt;- (sum(x * y) - beta0 * sum(x)) / (sum(x^2))\n    abline(a = beta0, b = b1hat)\n}\n\n\n\n\n\n\n\n\nAs \\(\\beta_0\\) increases, \\(\\hat\\beta_1\\) is forced to have a smaller slope in order to still fit the data well. If \\(\\beta_0\\) were 1000, the line would have to have a very large negative slope in order to even come close to the data!\nConversely, if \\(\\beta_0\\) were very small, \\(\\hat\\beta_1\\) would need to be very large. They’re negatively correlated!\nAs a side note, all of the lines go through the same point. It isn’t \\((\\bar x, \\bar y)\\)! What is it? Hint: take any two lines and find the estimator of \\(\\beta_1\\), then set them equal to each other. Do this with the values, then do this using \\(\\beta_0^{(1)}\\) and \\(\\beta_0^{(2)}\\).\n\n\n\nWhat happens to the correlation of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) when \\(\\bar x = 0\\)? Use your answers above to explain this intuitively",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression in Matrix Form</span>"
    ]
  },
  {
    "objectID": "L05-General_Regression.html",
    "href": "L05-General_Regression.html",
    "title": "5  The General Regression Situation",
    "section": "",
    "text": "5.1 Chapter Summary",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The General Regression Situation</span>"
    ]
  },
  {
    "objectID": "L05-General_Regression.html#chapter-summary",
    "href": "L05-General_Regression.html#chapter-summary",
    "title": "5  The General Regression Situation",
    "section": "",
    "text": "The Normal Equations\n\\[\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= (Y - X\\underline\\beta)^T(Y - X\\underline\\beta)\\\\\n&= ... = Y^TY - 2\\underline\\beta^TX^TY + \\underline\\beta^TX^TX\\underline\\beta\n\\end{align*}\\] We then take the derivative with respect to \\(\\underline\\beta\\). Note that \\(X^TX\\) is symmetric and \\(Y^TX\\underline\\beta\\) is a scalar.. \\[\\begin{align*}\n\\frac{\\partial}{\\partial\\underline\\beta}\\underline\\epsilon^T\\underline\\epsilon &= 0 - 2X^TY + 2X^TX\\underline\\beta\n\\end{align*}\\]\n\nFor the 1 predictor case, verify that the equations look the same!\n\nSetting to 0, rearranging, and plugging in our data gets us the Normal equations: \\[\\begin{align*}\nX^TX\\underline{\\hat\\beta} &= X^T\\underline y\n\\end{align*}\\]\n\n\nFacts\nThe solution to the normal equations satisfies: \\[X^TX\\underline{\\hat\\beta} = X^T\\underline y\\]\n\nNo distributional assumptions.\nIf \\(X^TX\\) is invertible, \\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\\).\n\n\\(\\hat{\\underline\\beta}\\) is a linear transformation of \\(\\underline y\\)!\nThis is the same as the MLE.\n\n\\(E(\\hat{\\underline\\beta}) = \\underline\\beta\\) and \\(V(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\\).\n\nThis is the smallest variance amongst all unbiased estimators of \\(\\underline\\beta\\).\n\n\n\n\nExample Proof Problems\n\nProve that \\(\\sum_{i=1}^n\\hat\\epsilon_i\\hat y_i = 0\\).\nProve that \\(\\sum_{i=1}^n\\hat\\epsilon_i = 0\\).\nProve that \\(X^TX\\) is symmetric. Is \\(A^TA\\) symmetric in general?\n\n\n\nShow the code\n## Demonstration that they're true (up to a rounding error)\nmylm &lt;- lm(mpg ~ disp, data = mtcars)\nsum(resid(mylm) * predict(mylm))\n\n\n[1] -2.664535e-14\n\n\nShow the code\nmean(resid(mylm))\n\n\n[1] -1.12757e-16\n\n\nShow the code\nX &lt;- model.matrix(mpg ~ disp, data = mtcars)\nall.equal(t(X) %*% X, t(t(X) %*% X))\n\n\n[1] TRUE\n\n\n\n\nAnalysis of Variance (Corrected)\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression (corrected)\n\\(p - 1\\)\n\\(\\underline{\\hat{\\beta}}^TX^T\\underline y - n\\bar{y}^2\\)\n\n\nError\n\\(n - p\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal (corrected)\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{y}^2\\)\n\n\n\n\nNote that \\(p\\) is the number of parameters, not the index of the largest param.\n\n\\(\\underline\\beta = (\\beta_0, \\beta_1, ..., \\beta_{p-1})\\)\n\nWe’ll always be using corrected sum-of-squares.\n\nEspecially next chapter!\n\n\n\n\n\\(F\\)-test for overall significance\nIf SSReg is significantly larger than SSE, then fitting the model was worth it!\n\nThis is a test for \\(\\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0\\), versus any \\(\\beta_j\\ne 0\\).\n\nAs before, we find a quantity with a known distribution, then use it for hypothesis tests.\n\\[\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n\\]\nAgain, note that a regression with no predictors always has \\(\\hat\\beta_0 = \\bar y\\).\n\n\nExample: Significance of disp\n\n\n\n\nShow the code\nanova(lm(mpg ~ disp, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ndisp\n1\n808.8885\n808.88850\n76.51266\n0\n\n\nResiduals\n30\n317.1587\n10.57196\nNA\nNA\n\n\n\n\n\n\n\\(p = 2\\), \\(n = 32\\).\n\n\\(df_R' + df_E' = df_T'\\), where \\(df'\\) is the df for corrected SS.\n\nVerified these numbers in the last lecture\n\n\n\n\nShow the code\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: \\(F_{1,p-1} = t^2_{p-1}\\)\n\n\nShow the code\nanova(lm(mpg ~ 1, data = mtcars), lm(mpg ~ qsec, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n31\n1126.0472\nNA\nNA\nNA\nNA\n\n\n30\n928.6553\n1\n197.3919\n6.376702\n0.017082\n\n\n\n\n\nShow the code\nsummary(lm(mpg ~ qsec, data = mtcars))$coef |&gt;\n    knitr::kable()\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-5.114038\n10.0295433\n-0.5098974\n0.6138544\n\n\nqsec\n1.412125\n0.5592101\n2.5252133\n0.0170820\n\n\n\n\n\nWhat do you notice about these two tables?\n\n\nExample: Significance of Regression (\\(F_{2,p-1} \\ne t^2_{p-1}\\))\n\n\nShow the code\nanova(lm(mpg ~ 1, data = mtcars), lm(mpg ~ qsec + disp, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n31\n1126.0472\nNA\nNA\nNA\nNA\n\n\n29\n313.5368\n2\n812.5104\n37.57582\n0\n\n\n\n\n\nShow the code\nsummary(lm(mpg ~ qsec + disp, data = mtcars))$coef |&gt;\n    knitr::kable()\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n25.5045079\n7.1840940\n3.5501356\n0.0013359\n\n\nqsec\n0.2122880\n0.3667758\n0.5787951\n0.5671961\n\n\ndisp\n-0.0398877\n0.0052882\n-7.5428272\n0.0000000\n\n\n\n\n\nWe’ll learn more about the ANOVA table next lecture.\n\n\n\\(R^2\\) again\n\\[\nR^2 = \\frac{SS(Reg|\\hat\\beta_0)}{Y^TY - SS(\\beta_0)} = \\frac{\\sum(\\hat y_i - \\bar y)^2}{\\sum(y_i - \\bar y)^2}\n\\]\nWorks for multiple dimensions!… kinda.\n\n\n\\(R^2\\) is bad?\n\n\n\n\nShow the code\nnx &lt;- 10 # Number of uncorrelated predictores\nuncorr &lt;- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) &lt;- c(\"y\", paste0(\"x\", 1:nx))\nuncorr &lt;- as.data.frame(uncorr)\n\nrsquares &lt;- NA\nfor (i in 2:(nx + 1)) {\n    rsquares &lt;- c(rsquares,\n        summary(lm(y ~ ., \n            data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjusted (Multiple) \\(R^2\\)\n\\[\nR^2_a = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p}\\right)\n\\]\n\nPenalizes added predictors - won’t always increase!\n\nStill might increase by chance alone!\n\nF-test\n\n\\(R^2_a = R^2\\) when \\(p=1\\) (intercept model)\n\nStill not perfect!\n\nWorks for comparing different models on same data\nWorks (poorly) for comparing different models on different data.\n\nIn general you should use \\(R^2_a\\), but always be careful.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The General Regression Situation</span>"
    ]
  },
  {
    "objectID": "L05-General_Regression.html#prediction-and-confidence-intervals-again",
    "href": "L05-General_Regression.html#prediction-and-confidence-intervals-again",
    "title": "5  The General Regression Situation",
    "section": "5.2 Prediction and Confidence Intervals (Again)",
    "text": "5.2 Prediction and Confidence Intervals (Again)\n\n\\(R^2\\) and \\(F\\)\nRecall that \\[\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n\\]\nFrom the definition of \\(R^2\\), \\[\\begin{align*}\nR^2 &= \\frac{SS(Reg|\\hat\\beta_0)}{SST}\\\\\n&= \\frac{SS(Reg|\\hat\\beta_0)}{SS(Reg|\\hat\\beta_0) + SSE}\\\\\n&= \\frac{(p-1)F}{(p-1)F + (n-p)}\n\\end{align*}\\] Conclusion: Hypothesis tests/CIs for \\(R^2\\) aren’t useful. Just use \\(F\\)!\n\n\nCorrelation of \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_2\\), etc.\nWith a different sample, we would have gotten slightly different numbers!\n\nIf the slope changed, the intercept must change to fit the data\n\n(and )\nThe parameter estimates are correlated!\n\nSimilar things happen with multiple predictors!\nThis correlation can be a problem for confidence regions\n\n\n\nUncorrelated \\(\\hat\\underline{\\beta}\\)\n\\[\nV(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\n\\]\nIn simple linear regression, \\[\n(X^TX)^{-1} = \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\]\nso the correlation is 0 when \\(\\bar x = 0\\)!\n\nIt’s common practice to mean-center the predictors for this very reason!\nFor 2 or more predictors, \\((X^TX)^{-1}\\) has a more complicated expression.\n\nThe correlation of the \\(\\beta_j\\)s is a function of the correlations in the data.\n\n\n\n\nPrediction and Confidence Intervals for \\(Y\\)\n\\(\\hat Y = X\\hat\\beta\\).\n\nA confidence interval around \\(\\hat Y\\) is based on the variance of \\(\\hat\\beta\\).\n\\(\\hat Y \\pm t * se(X\\hat\\beta)\\)\n\n\\(Y_{n+1} = X\\beta + \\epsilon_{n+1}\\)\n\nA prediction interval around \\(Y_{n+1}\\) is based on the variance of \\(\\hat\\beta\\) and \\(\\epsilon\\)!\n\\(\\hat Y_{n+1} \\pm t * se(X\\hat\\beta + \\epsilon_{n+1})\\)",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The General Regression Situation</span>"
    ]
  },
  {
    "objectID": "L05-General_Regression.html#exercises",
    "href": "L05-General_Regression.html#exercises",
    "title": "5  The General Regression Situation",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\n\nProve that \\(\\sum_{i=1}^n\\hat\\epsilon_i\\hat y_i = 0\\).\nProve that \\(\\sum_{i=1}^n\\hat\\epsilon_i = 0\\).\nProve that \\(X^TX\\) is symmetric. Is \\(A^TA\\) symmetric in general?\nSimulate data with \\(n = 50\\), \\(p = 11\\), and \\(\\beta_1 = \\beta_2 = ... = \\beta_10 = 0\\). Check the individual t-tests for whether the slopes are 0, and the overall F-test. Do this multiple times. Do you find a significant result from one of the t-tests but not the overall F-test? What about vice-versa?\n\n\n\nSolution\n\n\nn &lt;- 50\np &lt;- 11\nX &lt;- matrix(\n    data = c(rep(1, n),\n        runif(n * (p - 1), -5, 5)),\n    ncol = p)\nbeta &lt;- c(p - 1, rep(0, p - 1)) # beta_0 = 10, will not affect results\ny &lt;- X %*% beta + rnorm(n, 0, 3)\n\nmydf &lt;- data.frame(y, X[, -1])\nmylm &lt;- lm(y ~ ., data = mydf)\nsummary(mylm)\n\n\nCall:\nlm(formula = y ~ ., data = mydf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7871 -1.9480  0.7211  1.5483  4.3626 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.44474    0.44502  23.470  &lt; 2e-16 ***\nX1          -0.24881    0.15095  -1.648  0.10733    \nX2          -0.04107    0.16778  -0.245  0.80789    \nX3           0.23421    0.18441   1.270  0.21159    \nX4          -0.07099    0.15537  -0.457  0.65026    \nX5           0.02299    0.17153   0.134  0.89407    \nX6          -0.05605    0.16334  -0.343  0.73333    \nX7          -0.12174    0.15332  -0.794  0.43198    \nX8           0.20164    0.15557   1.296  0.20254    \nX9          -0.48608    0.15305  -3.176  0.00292 ** \nX10         -0.28487    0.18491  -1.541  0.13149    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.859 on 39 degrees of freedom\nMultiple R-squared:   0.24, Adjusted R-squared:  0.04509 \nF-statistic: 1.231 on 10 and 39 DF,  p-value: 0.3023\n\n\nIn my run (this will be different each time you run it), one of the slopes was found to be significant! This is a Type 1 error - we found a significant result where we shouldn’t have.\nHowever, the F-test successfully found that the overall fit is not significantly better than a model without any predictors. F-tests exists to prevent Type 1 errors!\nAs a side note, check out the difference between the Multiple R-squared and Adjusted R-squared. Which one would you believe in this situation?\n\n\n\nPlot the confidence intervals and the prediction intervals for a simple linear regression of body mass versus flipper length for penguins. Do this for a lot of points, and interpret the difference between the two.\n\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins[complete.cases(penguins),]\npeng_lm &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\n\n\nSolution\n\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins[complete.cases(penguins),]\npeng_lm &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\n# confidence intervals along all flipper lengths\nflippers &lt;- seq(from = min(penguins$flipper_length_mm),\n    to = max(penguins$flipper_length_mm), length.out = 100)\n\nconfi &lt;- predict(peng_lm, newdata = data.frame(flipper_length_mm = flippers), interval = \"confidence\")\npredi &lt;- predict(peng_lm, newdata = data.frame(flipper_length_mm = flippers), interval = \"prediction\")\n\nplot(body_mass_g ~ flipper_length_mm, data = penguins)\nlines(x = flippers, y = confi[, \"upr\"], col = 3, lwd = 2, lty = 2)\nlines(x = flippers, y = confi[, \"lwr\"], col = 3, lwd = 2, lty = 2)\nlines(x = flippers, y = predi[, \"upr\"], col = 2, lwd = 2, lty = 2)\nlines(x = flippers, y = predi[, \"lwr\"], col = 2, lwd = 2, lty = 2)\nlegend(\"topleft\", legend = c(\"Confidence\", \"Prediction\"), col = 3:2, lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\nThe confidence “band” is much thinner than the prediction band! With this many points, the model is pretty sure about the slope and intercept (i.e. \\(\\hat y_i\\)); a new sample would be expected to produce a similar line.\nHowever, even though the model knows about the slope and intercept of the line, there’s still lots of uncertainty around an unobserved point \\(\\hat y_{n + 1}\\).\n\n\n\nUse a simulation to demonstrate a confidence interval for \\(\\hat y_i\\) (an observed data point). For a model \\(y_i = 1 + 4x_i + \\epsilon_i\\), \\(\\epsilon_i \\sim N(0, 16)\\), \\(n = 30\\), simulate 100 data sets, calculate the line, and add it to a plot. What do you notice about the shape? Why is this?\n\n\n\nSolution\n\n\nn &lt;- 30\nx &lt;- runif(n, -5, 5)\nplot(NA, xlim = c(-5, 5), ylim = c(-25, 25), xlab = \"x\", ylab = \"y\")\nfor (i in 1:1000) {\n    y &lt;- 1 + 4 * x + rnorm(n, 0, 8)\n    abline(lm(y ~ x))\n}\n\n\n\n\n\n\n\n\nIt looks kind of like a bowtie!\nAs mentioned in lecture, the model can be seen as fitting a point at \\((\\bar x, \\bar y)\\), then finding a slope that fits the data well. If we got the exact same value for \\(\\bar y\\) every time, every single line would converge at the point \\((\\bar x, \\bar y)\\)!\nWe also saw that the variance of \\(\\hat y_i\\) is minimized at \\(\\bar x\\), and this plot demonstrates this.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The General Regression Situation</span>"
    ]
  },
  {
    "objectID": "L06-Extra_Sums_of_Squares.html",
    "href": "L06-Extra_Sums_of_Squares.html",
    "title": "6  Extra Sum-of-Squares",
    "section": "",
    "text": "6.1 Introduction",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extra Sum-of-Squares</span>"
    ]
  },
  {
    "objectID": "L06-Extra_Sums_of_Squares.html#introduction",
    "href": "L06-Extra_Sums_of_Squares.html#introduction",
    "title": "6  Extra Sum-of-Squares",
    "section": "",
    "text": "Today’s Main Idea\nIf you add or remove predictors, the variance of the residuals changes!\n\nAs always, we ask if it’s a “big” change.\n\nAll sizes are relative to the standard error!\n\nDifferent predictors have a different effect on the residuals.\nA group of predictors can be tested for whether the change is “big”.\n\nWhich predictors have a meaningful (significant?) effect on the residuals?\n\nSince SSE = SST - SSReg and SST never changes, we’re focusing on SSReg.\nRecall: SSReg is the variance of the line itself!\n\n\\[\nSS_{Reg} = \\sum_{i=1}^n(\\hat y_i - \\bar y)\n\\]\n\n\nSSReg in two different penguin models\nIn the penguins data, we’re determining which predictors are associated with body mass.\n\n\\(SS_1\\) = SSReg for model 1\n\n\\(\\texttt{body\\_mass\\_g} = \\beta_0 + \\beta_1 \\texttt{flipper\\_length\\_mm} + \\beta_2 \\texttt{bill\\_length\\_mm} + \\beta_3 \\texttt{bill\\_depth\\_mm}\\)\n\n\\(SS_2\\) = SSReg for model 2\n\n\\(\\texttt{body\\_mass\\_g} = \\beta_0 + \\beta_1 \\texttt{flipper\\_length\\_mm} + \\beta_2 \\texttt{bill\\_length\\_mm}\\)\n\n\nNote: M2 is nested in M1; M1 has all the same predictors and then some.\n\n\n\n\n\n\n\\(\\beta_1\\) in the first model is different from \\(\\beta_1\\) in the second model.\n\n\n\n(Only exception: the correlation between predictors is exactly 0.)\n\n\n\n\nExtra Sum-of-Squares\nIf M2 is nested within M1, :\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength}\\)\n\nThen the Extra Sum of Squares is defined as: \\[\nSS(\\hat\\beta_3 | \\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = S_1 - S_2\n\\] where \\(S_1\\) is \\(SS_{Reg}\\) for M1, similar for \\(S_2\\).\nConvince yourself that \\(S_1 \\ge S_2\\).\n\n\nSpecial Case: Corrected Sum-of-Squares\nWe’ve already seen this notation: \\[\nSSReg(\\hat\\beta_0) = n\\bar{\\underline y}^2\n\\] and \\[\nSS_{Reg}(corrected) = SS_{Reg}(\\hat\\beta_1, ..., \\hat\\beta_{p-1}|\\hat\\beta_0)= \\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2\n\\] which can be written as \\(SS_{Reg}(corrected) = S_1 - S_2\\) where \\(S_2\\) is the sum-of-squares for the null model!\n\n\nUnspecial Case: Correction doesn’t matter!\nConsider \\(S_{1c}\\) and \\(S_{2c}\\), the corrected versions of \\(S_1\\) and \\(S_2\\). In symbols: \\[\nS_{1c} = SS_{Reg}(\\hat\\beta_1,\\hat\\beta_2,\\hat\\beta_3 | \\hat\\beta_0)\\text{ and }S_{1} = SS_{Reg}(\\hat\\beta_0,\\hat\\beta_1,\\hat\\beta_2,\\hat\\beta_3)\n\\] Then \\[\\begin{align*}\nS_{1c} - S_{2c} = (S_2 - n\\bar{\\underline y}^2) - (S_1 - n\\bar{\\underline y}^2) = S_1 - S_2\n\\end{align*}\\]\nIn other words, the correction term doesn’t matter.\n\nThis is useful because R outputs the corrected versions.\n\n\n\nUnspecial Case: SSReg versus SSE doesn’t matter!\nConsider \\(SSE_1\\) and \\(SSE_2\\). Since \\(SST\\) is the same for both models,\n\\[\\begin{align*}\nSSE_2 - SSE_1 = (SST - S_1) - (SST - S_2) = S_2 - S_1\n\\end{align*}\\]\nNotice that the order is switched - bigger variance in the line means smaller variance in the residuals!!!\n\n\nANOVA Tests for ESS\nConsider the models:\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\n\n\\(df_1 = 4\\)\n\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength}\\)\n\n\\(df_2 = 3\\)\n\n\nIf we choose \\(H_0: \\beta_3 = 0\\) in model 1, then \\[\n\\frac{S_1 - S_2}{(4 - 3)s^2} \\sim F_{1,\\nu}\n\\]\nwhere \\(s^2\\) is the error variance (MSE) in the larger model with degress of freedom \\(\\nu = df_1\\).\nThis is almost identical to the F-test for only one predictor (with one important difference).\n\n\nIn General\nIf M1 has \\(p\\) df, M2 has \\(q\\) df, and one is nested in the other, then \\(\\nu = \\max(p, q)\\) and\n\\[\n\\frac{S_1 - S_2}{(p - q)s^2} \\sim F_{|p-q|,\\nu}\n\\]\n\nNote that it doesn’t matter which is nested: \\(S_1 - S_2\\) has the same sign as \\(p-q\\), so the F-statistic always positive.\n\nThe df in F requires an absolute value.\n\n\n\n\nThe Big Idea: Omnibus Tests for Multiple Predictors\nSuppose we want to test if any bill measurement is useful.\n\nBill length and depth are highly correlated - marginal CIs won’t be valid.\nConfidence Regions are hard (and only work in 2D)\n\nTo provide a little bit more context, we’re going to consider the actual covariance of the predictors! Recall that the variance covariance matrix for \\(\\hat{\\underline\\beta}\\) (i.e. the variance of the joint sampling distribution) is defined as \\(V(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\\).\n\n\nShow the code\nlibrary(palmerpenguins)\npeng &lt;- penguins[complete.cases(penguins),]\n\nmylm &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = peng)\n\nX &lt;- model.matrix(mylm)\ns2 &lt;- summary(mylm)$sigma^2\n\nvar_beta &lt;- s2 * solve(t(X) %*% X)\nvar_beta\n\n\n                  (Intercept) flipper_length_mm bill_length_mm bill_depth_mm\n(Intercept)       320503.2363      -1211.206658     805.525086   -6528.69599\nflipper_length_mm  -1211.2067          6.236320      -8.786474      20.06738\nbill_length_mm       805.5251         -8.786474      28.793240     -17.85211\nbill_depth_mm      -6528.6960         20.067377     -17.852112     191.15678\n\n\nWe can see that there’s a lot of covariance going on in our data~ We’ll focus on bill length and bill depth for now. The code below is not testable.\n\n\nShow the code\nlibrary(mvtnorm)\n\nbeta_length &lt;- coef(mylm)[\"bill_length_mm\"]\nbeta_depth &lt;- coef(mylm)[\"bill_depth_mm\"]\nse_length &lt;- summary(mylm)$coef[\"bill_length_mm\", \"Std. Error\"]\nse_depth &lt;- summary(mylm)$coef[\"bill_depth_mm\", \"Std. Error\"]\nvar_bills &lt;- var_beta[3:4, 3:4]\nx &lt;- seq(beta_length - 2 * se_length, beta_length + 2*se_length,\n    length.out = 100)\ny &lt;- seq(beta_depth - 2 * se_depth, beta_depth + 2 * se_depth,\n    length.out = 100)\nz &lt;- outer(x, y, function(xi, yi) dmvnorm(cbind(xi, yi), mean = c(beta_length, beta_depth),\n    sigma = var_bills))\ncontour(x, y, z, main = \"Joint Normal Distribution of Bill Measurement Coefficients\",\n    xlab = \"Coefficient for bill length\",\n    ylab = \"Coefficient for bill depth\")\n\nconfints &lt;- confint(mylm)\nabline(v = confints[\"bill_length_mm\",], col = 2)\nabline(h = confints[\"bill_depth_mm\",], col = 2)\npoints(x = -6, y = -4, pch = 16, cex = 2, col = 2)\n\n\n\n\n\n\n\n\n\nThe plot above shows the estimated sampling distribution for the Bill Length and Bill Depth coefficients (this is similar to the normal distribution that we use for the sample mean, but it’s in two dimensions now and we have to deal with the correlation of the coefficients). Since the coefficients are correlated, we get an elliptical region.\nI’ve also added the “marginal” confidence intervals; that is, the confidence intervals you get if you only consider the coefficient for bill depth. There’s a red dot at (-6, -4), which represents the joint hypothesis \\(\\beta_3 = -6\\) and \\(\\beta_4 = 4\\) (that is, the hypothesis that both of these are true at the same time). If we simply test the hypothesis that \\(\\beta_3 = -6\\), we are well within the confidence limits. If we just test the hypothesis that \\(\\beta_4 = -4\\), we’re also within those confidence limits. However, the contour plot above shows that this is actually not a reasonable value - it’s far in the tail of the joint distribution! The marginal confidence intervals don’t tell the whole story.\nInstead, we can use the ESS to test for a subset of predictors!\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength}\\)\n\n\\(S_1 = S_2\\) is equivalent to \\(\\beta_2 = \\beta_3 = 0\\), and it accounts for their covariance!\nIf significant, then at least one of \\((\\beta_2, \\beta_3)\\) is not 0.\n\n\nESS In R\n\n\nShow the code\nlibrary(palmerpenguins)\npeng &lt;- penguins[complete.cases(penguins),]\nm1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = peng)\nm2 &lt;- lm(body_mass_g ~ flipper_length_mm,\n    data = peng)\nanova(m1, m2) |&gt; knitr::kable()\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n329\n50814912\nNA\nNA\nNA\nNA\n\n\n331\n51211963\n-2\n-397050.9\n1.285349\n0.2779392\n\n\n\n\n\n\nIncluding the bill measurements does not significantly change the variance of the regression model.\n\n\\(SS_{Reg}(\\hat\\beta_3,\\hat\\beta_4|\\hat\\beta_0,\\hat\\beta_1) = 0 \\Leftrightarrow \\beta_3 = \\beta_4 = 0\\).",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extra Sum-of-Squares</span>"
    ]
  },
  {
    "objectID": "L06-Extra_Sums_of_Squares.html#examples",
    "href": "L06-Extra_Sums_of_Squares.html#examples",
    "title": "6  Extra Sum-of-Squares",
    "section": "6.2 Examples",
    "text": "6.2 Examples\n\nmtcars\n\n\n\ndisp: Engine size (displacement)\nhp: Horse power\ndrat: Rear axel ratio\n\nHow much the axel must turn\n\nwt: Weight\nqsec: Quarter mile time (seconds)\n\n\n\n\nShow the code\nM1 &lt;- lm(mpg ~ disp + hp + drat + wt + qsec,\n    data = mtcars)\n\n\n\n\nSome potential questions:\n\nIs the car’s “power” important for the regression?\nIs the car’s “size” important?\nAre hp and wt enough to model the mpg of a car?\n\nSolutions:\n\n\nShow the code\nanova(M1, update(M1, ~ . - hp - qsec - drat)) # At least one is important!\n\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + hp + drat + wt + qsec\nModel 2: mpg ~ disp + wt\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1     26 170.13                             \n2     29 246.68 -3   -76.553 3.8998   0.02 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nShow the code\nanova(M1, update(M1, ~ . - wt - disp)) # At least one is important!\n\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + hp + drat + wt + qsec\nModel 2: mpg ~ hp + drat + qsec\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     26 170.13                                \n2     28 287.99 -2   -117.86 9.0057 0.001067 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nShow the code\nanova(M1, lm(mpg ~ hp + wt, data = mtcars)) # None are important!\n\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + hp + drat + wt + qsec\nModel 2: mpg ~ hp + wt\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     26 170.13                           \n2     29 195.05 -3   -24.919 1.2694 0.3055\n\n\nOf course, this raises more questions: If “at least one is important”, then which is it?\nWe’re slowly getting into the “art” territory of regression. The choice of which predictors to include is never going to have a right answer. If someone is trying to decide how heavy of a car to make, they probably want wt in the model no matter what. If the Ministry of Transportation wants to predict the mpg based on things that are easy to measure, we might choose the predictors that are easiest to measure.\nWe should not just check a whole bunch of p-values! A recurring and important theme in this class is that Type 1 error is a monster! The whole point of ESS (including overall F-tests) is that we can check a bunch of things all at once without having to calculate a bunch of p-values!\n\n\nNext time\n\nWhen to check ESS\nHow to check all ESS\nWhat is R’s anova() function even doing???",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extra Sum-of-Squares</span>"
    ]
  },
  {
    "objectID": "L06-Extra_Sums_of_Squares.html#exercises",
    "href": "L06-Extra_Sums_of_Squares.html#exercises",
    "title": "6  Extra Sum-of-Squares",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\nSuggested Ch06 textbook exercises: A, D.\n\nExplain why \\(SS_{Reg}\\) is always larger when predictors are added. (That is, if \\(M1\\) is nested within \\(M2\\), then \\(SS_{Reg}\\) for \\(M2\\) will be larger).\nSimulate a data set with 10 predictors, where only 5 of them have a non-zero coefficient. Show that an ESS test can detect this, even if some of the parameters with a 0 coefficient have a significant p-value (Type 1 error).\nDemonstrate that \\(\\hat\\beta_1\\) in the model \\(y_i = \\beta_0 + \\beta_1\\) is different \\(y_i = \\beta_0 + \\beta_1 + \\beta_2\\). Demonstrate this across any data we’ve seen before, new data, simulated data, or whatever you want!\nWhy does the estimate of one parameter change when you add other estimates? (Hint: see the bill length and bill depth example here in the course notes.)",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extra Sum-of-Squares</span>"
    ]
  },
  {
    "objectID": "L07-Exampless.html",
    "href": "L07-Exampless.html",
    "title": "7  ESS Exampless",
    "section": "",
    "text": "7.1 Review",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ESS Exampless</span>"
    ]
  },
  {
    "objectID": "L07-Exampless.html#review",
    "href": "L07-Exampless.html#review",
    "title": "7  ESS Exampless",
    "section": "",
    "text": "Extra Sum of Squares\nFrom last time, we basically learned what the following means:\n\\[\n\\frac{SS(\\hat\\beta_{q+1}, ..., \\hat\\beta_{p-1} | \\hat\\beta_0, ... \\hat\\beta_q)}{(p-q)s^2} =\\frac{S_1 - S(\\hat\\beta_0) - (S_2 - S(\\hat\\beta_0))}{(p-q)s^2}\\sim F_{p-q, \\max(p, q)}\n\\] where \\(s^2\\) is the MSE calculated from the larger model.\nThis allows us to do a test for whether \\(\\beta_{q+1} = \\beta_{q+2} = ... = \\beta_{p-1} = 0\\).\nRecall that this is accomplished by testing whether the variance of the line significantly changes when those predictors are included. A line with zero variance is horizontal; if adding one predictor to that line changes the variance significantly, then it’s no longer a horizontal line (and thus the slope is non-zero). This works when extending into arbitrary dimensions by adding more predictors - if a predictor is not included in a model, then the hyperplane defined by the linear regression is “horizontal” in that direction.\nMathematically, the regression equation \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\) defines a plane in 3 dimensions, with the slopes determined by \\(\\beta_1\\) and \\(\\beta_2\\) (and a slope of \\(1\\) in the y direction). If we have also recorded \\(x_3\\) but not included it in the model, we can write the hyperplane as \\(y = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 + 0x_3\\), i.e. the hyperplane has a slope of 0 in the \\(x_3\\) direction. Adding \\(x_3\\) as a predictor means we are allowing the slope in the \\(x_3\\) direction to be non-zero.\nThe big lesson here is that analysing the variance allows us to determine if the slope is 0! This is very important for understanding what’s happening with regression.\n\n\nPenguins Example\nThe R code to do this test is as follows. In this code, we believe that the bill length and bill depth are strongly correlated, and thus we cannot trust the CIs that we get from summary(lm()) (we saw “Confidence Regions” in the slides and code for L05).\n\nnrow(peng)\n\n[1] 333\n\n\n\nlm1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm, data = peng)\nlm2 &lt;- lm(body_mass_g ~ flipper_length_mm, data = peng)\nanova(lm2, lm1)\n\nAnalysis of Variance Table\n\nModel 1: body_mass_g ~ flipper_length_mm\nModel 2: body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm\n  Res.Df      RSS Df Sum of Sq      F Pr(&gt;F)\n1    331 51211963                           \n2    329 50814912  2    397051 1.2853 0.2779\n\n\n\n\nWhere do these values come from?\nLet’s try and calculate these values ourselves in a couple different ways!\n\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n                   Df    Sum Sq   Mean Sq   F value Pr(&gt;F)    \nflipper_length_mm   1 164047703 164047703 1062.1232 &lt;2e-16 ***\nbill_length_mm      1    140000    140000    0.9064 0.3418    \nbill_depth_mm       1    257051    257051    1.6643 0.1979    \nResiduals         329  50814912    154453                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this model, SSE is 50814912 on 329 degrees of freedom. This is the same as the SSE in the output of anova(lm2, lm1).\n\n\nCalculating the SSE\n\nanova(lm2)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n                   Df    Sum Sq   Mean Sq F value    Pr(&gt;F)    \nflipper_length_mm   1 164047703 164047703  1060.3 &lt; 2.2e-16 ***\nResiduals         331  51211963    154719                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, the SSE of 51211963 matches what we saw in anova(lm2, lm1), and we have 331 degrees of freedom (as expected, the differences in degrees of freedom is 2).\nNote that the F-value in anova() is just the ratio of the MSEs, but this is not the case here. Instead, we need to calculate \\(s^2\\).\n\n\nFinal Calculations\n\n\n\\(s^2\\) is the MSE for the larger model:\n\ns2 &lt;- 50814912 / 329\ns2\n\n[1] 154452.6\n\n\nAnd now we can calculate the F-value:\n\n(51211963 - 50814912) / (2 * s2)\n\n[1] 1.285349\n\n\n\n1 - pf(1.28539, 2, 329)\n\n[1] 0.2779278\n\n\n\n\nanova(lm2, lm1)\n\nAnalysis of Variance Table\n\nModel 1: body_mass_g ~ flipper_length_mm\nModel 2: body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm\n  Res.Df      RSS Df Sum of Sq      F Pr(&gt;F)\n1    331 51211963                           \n2    329 50814912  2    397051 1.2853 0.2779\n\n\n\n\n\nTry to calculate these values based on matrix multiplication.\n\nWith and without correction factors!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ESS Exampless</span>"
    ]
  },
  {
    "objectID": "L07-Exampless.html#ess-algorithms",
    "href": "L07-Exampless.html#ess-algorithms",
    "title": "7  ESS Exampless",
    "section": "7.2 ESS Algorithms",
    "text": "7.2 ESS Algorithms\n\nMain Idea\n\nESS: Test a subset of predictors for at least one significant coefficient.\nWe might want to check all predictors one-by-one.\n\nThis is much less common than the textbook may lead you to believe\n\n\nThere are 3 ways to calculate the ESS for all predictors. They are very helpfully labelled Types I, II, and III.\n\n\nType I: Sequential Sum-of-Squares (with interactions)\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1)\\)\nCheck \\(SS(\\hat\\beta_2:\\hat\\beta_1|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n\n\\(\\hat\\beta_2:\\hat\\beta_1\\) is an interaction term, which means we use a formula like y ~ x1 + x2 + x1*x2 (although we’ll learn why R uses different notation than this).\n\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\nCheck all interactions between x1, x2, and x3,\n…\n\nThis will give us every possible sum-of-squares. This is very very dubious, and can lead to a major multiple comparisons problem!\n\n\nType 2: Sequential Sum-of-Squares (R’s Default)\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1)\\)\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n…\n\n\nResults in an ordered sequence of “is it worth adding x1?”, “if we have x1, is it worth adding x2?”, etc.\nOnly meaningful if the predictors are naturally ordered (such as polynomial regression, see below).\n\n\n\nType 3: Last-entry sum-of-squares\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0, \\hat\\beta_2, \\hat\\beta_3)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_3)\\)\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n\n\nWhether adding predictor \\(x_i\\) is worth it, given that all other predictors are already in the model.\n\n\n\nBack to Type 2 ANOVA (Sequental Sum-of-Squares)\nBy default, R does sequential sum-of-squares. This is a very important fact to know!\nIn Types I and II, the order of the predictors matters. In fact, you cannot make any conclusions about the significance that doesn’t make reference to this fact.\n\n\nType 2 ANOVA in R\n\n## Try changing the order to see how the significance changes!\nmylm &lt;- lm(mpg ~ qsec + disp + wt, data = mtcars)\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nqsec       1 197.39  197.39  28.276 1.165e-05 ***\ndisp       1 615.12  615.12  88.116 3.816e-10 ***\nwt         1 118.07  118.07  16.914 0.0003104 ***\nResiduals 28 195.46    6.98                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mylm)$coef # No obvious connection to anova\n\n                 Estimate Std. Error     t value     Pr(&gt;|t|)\n(Intercept) 19.7775575655 5.93828659  3.33051585 0.0024420674\nqsec         0.9266492353 0.34209668  2.70873496 0.0113897664\ndisp        -0.0001278962 0.01056025 -0.01211109 0.9904228666\nwt          -5.0344097167 1.22411993 -4.11267686 0.0003104157\n\n\n\n\nSpecial Case: Polynomial Regression\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... + \\beta_{p-1}x_i^{p-1} + \\epsilon_i\n\\]\nWe only have one predictor \\(x\\), but we have performed non-linear transformations (HMWK: why is it important that the transformations are non-linear?).\nWhat order of polynomial should we fit?\n\n\nPolynomial Regression Data Context\n\nGiven we have a linear model, is it worth making it quadratic?\nGiven that we have a quadratic model, is it worth making it cubic?\nGiven that we have a cubic model…\n\nIn the code below, I use the I() function (the I means identity) to make the polynomial model. The “formula” notation in R, y ~ x + z, has a lot of options. Including x^2, rather than I(x^2), makes R think we want to do one of the more fancy things, but the I() tells it that we want to literally square it. In the future, we’ll use a better way of doing this.\n\nx &lt;- runif(600, 0, 20)\ny &lt;- 2 - 3*x + 3*x^2 - 0.3*x^3 + rnorm(600, 0, 100)\nplot(y ~ x)\n\n\n\n\n\n\n\n\n\n\nSequential SS for Polynomial Regression\n\nmylm &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: y\n           Df   Sum Sq  Mean Sq   F value Pr(&gt;F)    \nx           1 49560412 49560412 4726.8201 &lt;2e-16 ***\nI(x^2)      1 19661307 19661307 1875.1955 &lt;2e-16 ***\nI(x^3)      1  1150679  1150679  109.7459 &lt;2e-16 ***\nI(x^4)      1      661      661    0.0630 0.8018    \nI(x^5)      1      112      112    0.0107 0.9177    \nI(x^6)      1     6274     6274    0.5984 0.4395    \nResiduals 593  6217568    10485                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the table above, we can clearly see that this should just be a cubic model (which is the true model that we generated). Try changing things around to see if, say, it will still detect an order 5 polynomial if if there’s no terms of order 3 or 4.\n\n\nA note on calculations\nTake a moment to consider the following. Suppose I checked the following two (Type II) ANOVA tables:\n\nanova(lm(mpg ~ disp, data = mtcars))\nanova(lm(mpg ~ disp + wt, data = mtcars))\n\nBoth tables will have the first row labelled “disp” and include its sum-of-squares along with the F-value. Do you expect these two rows to be exactly the same?\nThink about it.\nThink a little more.\nWhat values do you expect to be used in the calculation?\nWhich sums-of-squares? Which variances?\nLet’s test it out.\n\n\nChange in ANOVA when adding predictors\n\nanova(lm(mpg ~ disp, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(mpg ~ disp + wt, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndisp       1 808.89  808.89 95.0929 1.164e-10 ***\nwt         1  70.48   70.48  8.2852  0.007431 ** \nResiduals 29 246.68    8.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThey’re different!\nWith both the polynomial and the disp example, we see that the interpretation of the anova table is highly, extremely, extraordinarily dependent on which predictors we choose to include AND the order in which we choose to include them. So, yeah. Be careful.\n\n\nType III SS in R\nThere isn’t a built-in function to do this. To create this, we can either use our math (my preferred method) or test each one individually.\n\nanova(\n    lm(mpg ~ disp + wt, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + wt\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     29 246.68                              \n2     28 195.46  1     51.22 7.3372 0.01139 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(\n    lm(mpg ~ disp + qsec, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + qsec\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     29 313.54                                  \n2     28 195.46  1    118.07 16.914 0.0003104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(\n    lm(mpg ~ wt + qsec, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ wt + qsec\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1     29 195.46                          \n2     28 195.46  1 0.0010239 1e-04 0.9904",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ESS Exampless</span>"
    ]
  },
  {
    "objectID": "L07-Exampless.html#modelling-strategies",
    "href": "L07-Exampless.html#modelling-strategies",
    "title": "7  ESS Exampless",
    "section": "7.3 Modelling Strategies",
    "text": "7.3 Modelling Strategies\n\nThe Art of Modelling\n\nIf you think to yourself “my predictors are logically ordered and I want to check for the significance of all of them one-by-one”, you want Type II.\n\nType 1 error through the roof.\n\nIf you think “they’re not ordered but I want to check significance”, check the overall F test for all predictors and then individual t-tests.\nIf you think “what would happen if each predictor were the last one I put in the model”, then you want Type III.\n\nI can’t think of a good for doing this - you’re pretty much guaranteed to have a multiple comparisons issue.\n\n\n\n\nChoosing Predictor Sets\n\nThese algorithms assume that you have a set of predictors that you already know you want to check.\nThere are other predictors in the mtcars dataset that we did not consider!\n\nWe only looked at continuous predictors - we’ll see categorical predictors later.\n\n\n\n\nAdvice\n\nStart with a lot of plots.\nBased on the plots and your knowledge of the context, create a candidate set of predictors that you think will be the final model.\nCheck the model fit (p-values, residuals, etc).\nBased on your knowledge of the context, check significance of groups of predictors that you think are highly correlated.\nYour final model will be based on the tests for groups of (or individual) predictors that you suspect would be relevant.\n\n\n\nThe Guiding Principles of Model Selection\nMinimize the number of p-values that you check.\n\nESS tests check a set of predictors, rather than each one individually.\n\nFewer p-values!\n\nThe Type I, II, and III algorithms are sometimes important, but intuition and knowledge is better!\n\nMinimize the number of degrees of freedom that you use\n\nMore degrees of freedom = more complexity.\n\nA simpler model that explains the same amount of variance is preferred!\n\nNon-linear trends (e.g. polynomial models) use more degrees of freedom.\n\nPolynomials are hard to interpret.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ESS Exampless</span>"
    ]
  },
  {
    "objectID": "L07-Exampless.html#exercises",
    "href": "L07-Exampless.html#exercises",
    "title": "7  ESS Exampless",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\nSuggested textbook Ch06 exercises: E, I, J, N, P, Q, R, X\n\nCalculate all ANOVA tables in this lesson using matrix multiplication.\nExplain why Type II SS would not be meaningful for the model \\(y_i = \\beta_{p-1}x_i^{p-1} + \\beta_{p-2}x_i^{p-2} + ... + \\beta_2x_i^2 + \\beta_1x_i + \\beta_0 + \\epsilon_i\\).\nI performed a Type 3 SS ANOVA for the model \\(bodymass_i = \\beta_0 + \\beta_1flipperlength + \\beta_2billlength\\). A colleague told me that I should have included \\(billdepth\\) in this analysis. Will the results from Type 3 SS ANOVA change when I add in this predictor?\nDescribe another situation (not polynomial regression) where Type 2 SS ANOVA would be appropriate. Simulate some relevant data and demonstrate.\n\n\n\nSolution Hints\n\nIn order for Type 2 SS to be appropriate, the predictors need to have a logical order in which to enter the model. Some examples might include a case where researchers have a particular ranking for the predictors in their study, and want to stop adding predictors to the model as soon as they find the first non-significant predictor (I am still struggling to think of context for this).\n\n\n\nFor the penguins data, fit a model with flipper length, bill length, and bill depth (in that order), and check the output of anova(). Reverse the order of predictors and repeat. Which values in the ANOVA table stay the same? Which values change? Why do they change? What does this tell you about interpreting anova() output?\nGive an example of a context where a collection of predictors might be tested for significance all at once.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ESS Exampless</span>"
    ]
  },
  {
    "objectID": "L08-Hat-Resid_Plots-Cook.html",
    "href": "L08-Hat-Resid_Plots-Cook.html",
    "title": "8  The Hat Matrix",
    "section": "",
    "text": "8.1 Effect of a Single Point",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Hat Matrix</span>"
    ]
  },
  {
    "objectID": "L08-Hat-Resid_Plots-Cook.html#effect-of-a-single-point",
    "href": "L08-Hat-Resid_Plots-Cook.html#effect-of-a-single-point",
    "title": "8  The Hat Matrix",
    "section": "",
    "text": "Leverage and Influence\n\n\nShow the code\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Apps/InfluentialPoint\")\n\n\n\nLeverage: affects the line.\n\nMeasured by the hat matrix.\nForcing the line to be what it would be anyway counts as leverage.\n\nInfluence: affects the line in a “meaningful” way\n\nMeasured by vibes (and Cook’s Distance).\nIf the point were removed the line would be different.\n\n\n\n\nLeverage \\(\\ne\\) Influence; Extreme Case\n\n\n\n\nShow the code\n# Generate n-1 data points\nx &lt;- runif(39, 0, 10)\ny &lt;- 4 + 10 * x + rnorm(39, 0, 5)\n\n# Generate nth data point - prefectly on line\nx[40] &lt;- 20\ny[40] &lt;- 4 + 10 * 20\n\nh &lt;- hatvalues(lm(y ~ x))\n\nplot(x, y)\nabline(lm(y ~ x))\ntext(x[40], y[40], labels = round(h[40], 4),\n    adj = c(1,1))\n\n\n\n\n\n\n\n\n\nIf the point above moves, the entire line moves with it! However, the point happens to be perfectly on the line.\n\n\n\n\n\nShow the code\n# Generate nth data point - prefectly on line\nx[40] &lt;- 20\ny[40] &lt;- 50\n\nh &lt;- hatvalues(lm(y ~ x))\n\nplot(x, y)\nabline(lm(y ~ x))\ntext(x[40], y[40], labels = round(h[40], 4),\n    adj = c(1,1))\n\n\n\n\n\n\n\n\n\nExact same leverage, but the line is now completely different!\n\n\nConclusion: Leverage measures whether the point affects the line, not how.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Hat Matrix</span>"
    ]
  },
  {
    "objectID": "L08-Hat-Resid_Plots-Cook.html#le-chapeau",
    "href": "L08-Hat-Resid_Plots-Cook.html#le-chapeau",
    "title": "8  The Hat Matrix",
    "section": "8.2 Le Chapeau",
    "text": "8.2 Le Chapeau\n\nThe Hat Matrix\n\\[\nH = X(X^TX)^{-1}X^T\n\\]\nThe hat matrix projects \\(Y\\) onto \\(\\hat Y\\), based on \\(X\\).\n\n\\(\\hat Y = HY\\)\n\n\\(\\hat Y_i = h_{ii} Y_i + \\sum_{j\\ne i}h_{ij}Y_j\\)\n\n\nIn other words, \\(h_{ii}\\) determines the leverage of the observed point \\(y_i\\) on it’s own prediction.\n\n\nVariance-Covariance Matrix of \\(\\hat{\\underline\\epsilon}\\)\nJust like \\(\\beta_0\\) and \\(\\beta_1\\), each sample results in different \\(\\underline{\\hat\\epsilon}\\).\nAcross samples, we have: \\[\n\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon}) = (I-H)(Y-X\\underline{\\beta}) = (I-H)\\underline{\\epsilon}\n\\] and therefore: \\[\\begin{align*}\nV(\\underline{\\hat\\epsilon}) &= E([\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon})][\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon})]^T)\\\\\n&= [I-H]E(\\underline{\\epsilon}\\underline{\\epsilon}^T)[I-H]^T\\\\\n&= [I-H]\\sigma^2[I-H]^T\\\\\n&= [I-H]\\sigma^2\n\\end{align*}\\] where we used the idempotency and symmetry of \\(I-H\\).\n\nNote: \\(h_{ii}\\in [0,1]\\) for all \\(i\\), so the variance is positive.\n\n\n\nThe variance of a residual\nGiven that \\(V(\\underline{\\hat\\epsilon}) = (I-H)\\sigma^2\\), \\[\nV(\\hat\\epsilon_i) = (1-h_{ii})\\sigma^2\n\\]\n\nThe variance of the residual depends on how much \\(Y_i\\) effects it’s own estimate.\n\nHigh leverage = low variance.\n\n\nThe correlation between residuals is: \\[\n\\rho_{ij} = \\frac{Cov(\\hat\\epsilon_i, \\hat\\epsilon_j)}{\\sqrt{V(\\hat\\epsilon_i)V(\\hat\\epsilon_j)}} = \\frac{-h_{ij}}{\\sqrt{(1-h_{ii})(1-h_{jj})}}\n\\]\n\nNote that \\(h_{ij}\\in [-1, 1]\\). Covariance can be negative or positive.\n\n\n\nMore H Facts\n\n\\(SS(\\hat{\\underline\\beta}) = \\hat{\\underline\\beta}^TX^TY = \\hat Y^TY = Y^TH^TY = Y^TH^THY = \\hat Y^T\\hat Y\\)\n\nWe used the facts \\(H^T= H^TH\\) and \\(\\hat Y = HY\\).\nIt is super weird that this proof implies \\(\\hat Y^TY = \\hat Y^T\\hat Y\\), even though \\(\\hat Y \\ne Y\\).\n\n\\(\\sum_{i=1}^nV(\\hat Y_i) = trace(H\\sigma^2) = p\\sigma^2\\)\n\n\\(trace(A)\\) is the sum of the diagonal elements of \\(A\\).\n\\(p\\) is the number of parameters.\nProof is homework\n\n\\(H1 = 1\\) if the model contains a \\(\\beta_0\\) term.\n\n\\(1\\) is a column of 1s, not identity matrix.\nProof on next slide.\n\\(h_{ii} = 1 - \\sum_{j\\ne i}h_{ij}\\)\n\nNote that \\(h_{ij}\\in[-1, 1]\\).\n\n\n\nThe first one is a little hard to believe! How can \\(\\hat Y^TY = \\hat Y^T\\hat Y\\) when \\(\\hat Y \\ne Y\\)???\nFirst, here’s a demonstration that each equality holds (at least for these particular data):\n\n\nShow the code\nmylm &lt;- lm(mpg ~ wt, data = mtcars)\nX &lt;- model.matrix(mylm)\nY &lt;- mtcars$mpg\nYhat &lt;- predict(mylm)\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\nbeta &lt;- as.numeric(coef(mylm))\nt(beta) %*% t(X) %*% Y\n\n\n         [,1]\n[1,] 13763.99\n\n\nShow the code\nt(Yhat) %*% Y\n\n\n         [,1]\n[1,] 13763.99\n\n\nShow the code\nt(Y) %*% H %*% Y\n\n\n         [,1]\n[1,] 13763.99\n\n\nShow the code\nt(Y) %*% t(H) %*% H %*% Y\n\n\n         [,1]\n[1,] 13763.99\n\n\nShow the code\nt(Yhat) %*% Yhat\n\n\n         [,1]\n[1,] 13763.99\n\n\nWhy \\(\\hat Y^TY = \\hat Y^T\\hat Y\\)? \\(\\hat Y\\) is trying to approximate \\(Y\\), so you can expect that they’re correlated. If \\(Y\\) were to be changed so that one of the values were higher, \\(\\hat Y\\) would make a corresponding change. If \\(\\hat Y\\) is too high in one place, it will be too low in another to compensate. This property is not true in general, it’s specifically because of the relationship between \\(\\hat Y\\) and \\(Y\\) that this works.\n\n\nProof that \\(H1 = 1\\)\nNote that \\(HX = X\\) (as proven on A1).\n\nThe first column of \\(X\\) is all ones (\\(\\beta_0\\) term).\n\n\\([X]_{i1} = 1\\)\n\nTherefore \\([HX]_{i1}\\) is a column of ones.\n\nEvery row of \\(H\\) times the column of 1s in \\(X\\) results in a column of ones.\n\n\\([HX]_{i1}\\) is every row of \\(H\\) times the first column of \\(X\\).\n\nThe first column of \\(X\\) is 1s, which is equal to the first column of \\(HX\\), which is \\(H\\) times a column of ones.\nIn other words, \\(H1 = 1\\)",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Hat Matrix</span>"
    ]
  },
  {
    "objectID": "L08-Hat-Resid_Plots-Cook.html#studentized-residuals",
    "href": "L08-Hat-Resid_Plots-Cook.html#studentized-residuals",
    "title": "8  The Hat Matrix",
    "section": "8.3 Studentized Residuals",
    "text": "8.3 Studentized Residuals\n\nInternally Studentized (not ideal)\nHow do you measure the size a residual?\nDivide by the variance, of course!\nWe know that \\(V(\\hat \\epsilon_i) = (1-h_{ii})\\sigma^2\\), and \\[\ns^2 = \\frac{\\sum_{y=1}^n(y_i-\\hat y_i)^2}{n-p} = \\frac{SSE}{df_E} = MSE\n\\] is an estimate of \\(\\sigma^2\\). Then, \\[\nr_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s^2(1-h_{ii})}}\n\\] is called the internally studentized residual.\n\n\n“Internally” “Studentized”\nNote that \\[\ns^2 = \\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2}{n-p} = \\frac{\\sum_{i=1}^n(\\hat\\epsilon_i)^2}{n-p}\n\\] and therefore \\[\nr_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s^2(1-h_{ii})}} = \\frac{\\hat\\epsilon_i}{\\sqrt{(\\hat\\epsilon_i^2 + \\sum_{j\\ne i}\\hat\\epsilon_j^2)(1-h_{ii})/(n-p)}}\n\\]\n\nIf \\(\\hat\\epsilon_i\\) is large, then \\(s^2\\) is large.\n\nIf \\(s^2\\) is large, then \\(r_i\\) is small!\n“Internally”: the variance includes the residual of interest.\n\n“Studentized” because “Student” (William Gosset) made it popular.\nOften called “standardized”.\n\n\n\nExternally Studentized Step 1\nLike adding/removing predictors and checking the change in SS, we can add/remove points!\n\nCalculate SS\nRemove the first point. Estimate the model again and calculate new SS.\nAdd the first point back, remove the second. Estimate the model again and check the SS.\n…\n\nFor each point, we have an estimate of the variance without itself.\n\n\nExternally Studentized Step 2\nSkipping the math, \\[\ns^2_{(i)} = \\frac{(n-p)s^2 - \\hat\\epsilon_i^2/(1-h_{ii})}{n-p-1}\n\\] is the variance of the residuals without observation \\(i\\).\n\nThe leverage tells us how much a point changed the model\n\nWe can see what happened without it\nNo need to re-estimate the model!!!\n\n\nLet’s demonstrate this quickly!\n\n\nShow the code\nmylm &lt;- lm(mpg ~ wt, data = mtcars)\n\n# s can be extracted as folows:\ns &lt;- summary(mylm)$sigma\n# Therefore s1^2 is:\nsummary(lm(mpg ~ wt, data = mtcars[-1, ]))$sigma^2\n\n\n[1] 9.409517\n\n\nShow the code\n# Now let's get it *without* re-fitting the model\n# The hat matrix\nh &lt;- hatvalues(mylm)\ne &lt;- residuals(mylm)\np &lt;- 2\nn &lt;- nrow(mtcars)\n((n - p) * s^2 - e[1]^2 / (1 - h[1])) / (n - p - 1)\n\n\nMazda RX4 \n 9.409517 \n\n\nThey’re exactly the same, except the second one did not require re-fitting the model! Math is truly astounding.\n\n\nExternally Studentized Residuals\nUse \\(s^2_{(i)}\\) in place of \\(s^2\\). \\[\nt_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s_{(i)}^2(1-h_{ii})}} \\sim t_{n-p-1}\n\\]\n\nFollows a \\(t\\) distribution!\n\nLarger than 2 is suspect, 3 is definitely an outlier!\n\nA large \\(t_i\\) is large relative to the other residuals\nUsually just called “Studentized”\n\nMost software uses Studentized residuals for plots/diagnostics!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Hat Matrix</span>"
    ]
  },
  {
    "objectID": "L08-Hat-Resid_Plots-Cook.html#cooks-distance",
    "href": "L08-Hat-Resid_Plots-Cook.html#cooks-distance",
    "title": "8  The Hat Matrix",
    "section": "8.4 Cook’s Distance",
    "text": "8.4 Cook’s Distance\n\nBetter Measures of Influence\nThe hat matrix is sometimes intepreted as influence, but it has problems.\n\n\\(y_i\\)’s “influence” on it’s own prediction,\n\ngiven all other points.\n\n\\(0 \\le h_{ii} \\le 1\\)\n\nWhat is a “big” “influence”?\n\nHow do you explain \\(h_{ii}\\) to nonstatisticians?\n\nA better measure is how much the predicted value changes with/without the obs.\n\n\nCook’s Distance: Change in \\(\\hat y_i\\).\n\\[\nD_i = \\frac{\\sum_{i=1}^n(\\hat y_i - \\hat y_{(i)})^2}{ps^2}\n\\]\n\n\\(\\hat y_i\\) is the predicted value of \\(y_i\\) when all data are considered.\n\\(\\hat y_{(i)}\\) is the predicted value of \\(y_i\\) when observation \\(i\\) is removed.\n\\(s^2\\) is the MSE of the model with all of the data.\n\\(p\\) is the number of parameters\n\n\\(D_i\\) decreases as \\(p\\) increases!\n\n\nAgain, this would involve re-fitting the model \\(n\\) times (one re-fit for each observation).\n\n\nCook’s Distance: Alternate Form\nAgain, we can use \\(H\\) to avoid re-fitting the model.\n\\[\nD_i = \\frac{\\sum_{i=1}^n(\\hat y_i - \\hat y_{(i)})^2}{ps^2} = \\left[\\frac{\\hat \\epsilon_i}{\\sqrt{s^2(1-h_{ii})}}\\right]^2\\frac{1}{p}\\left[\\frac{h_{ii}}{1 - h_{ii}}\\right] = r_i^2\\frac{1}{p}\\frac{\\text{variance of $i$th predicted value}}{\\text{variance of $i$th residual}}\n\\]\n\nCook’s distance is a modification of the internally studentized residual.\n\nVariances are based on same “deletion” idea as studentized.\n\nRatio of Variances!\n\n\\(F\\) distribution, mean approaches 1 for large values of \\(n\\)\n\nCooks Distance of larger than 1 is suspect.\n\n\n\n\n\nNext Class\nPlots!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Hat Matrix</span>"
    ]
  },
  {
    "objectID": "L08-Hat-Resid_Plots-Cook.html#exercises",
    "href": "L08-Hat-Resid_Plots-Cook.html#exercises",
    "title": "8  The Hat Matrix",
    "section": "8.5 Exercises",
    "text": "8.5 Exercises\nSuggested Ch08 Exercises: A, C.\n\nIn the proof that \\(V(\\underline{\\hat\\epsilon_i}) = (1-h_{ii})\\sigma^2\\), we skipped a few steps. Fill in those steps.\nExplain why “a large residual tells us there are small residuals”. It is useful to know that \\(\\sum_{i=1}^n\\hat\\epsilon_i = 0\\).\nExplain where each part of the formula for \\(\\rho_{ij}\\).\nProve that \\(\\sum_{i=1}^nV(\\hat Y_i) = trace(H\\sigma^2) = p\\sigma^2\\). You may use the fact that the “trace” function allows you to move the last term to the front (i.e. you can “cycle” the entries): \\(trace(ABCD) = trace(DABC) = trace(CDAB) = trace(BCDA) \\ne trace(BACD)\\).\nFor the following data, calculate \\(h_{11}\\), \\(\\hat\\epsilon_1\\), \\(s_1\\), \\(t_1\\), and \\(D_1\\) using R. Plot the data; comment on whether the first value is an outlier, and whether you would know this from each of the calculated values.\n\n\n\nShow the code\nset.seed(2112)\nx &lt;- c(10, runif(49, 0, 10))\ny &lt;- c(50, 2 + 3 * x[-1] + rnorm(49, 0, 3))\nmylm &lt;- lm(y ~ x)\nX &lt;- model.matrix(mylm)\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\nh11 &lt;- H[1,1]\n\n\n\nFor the data in the previous question, note that the mean of \\(x\\) should be around 5. Move the first point to this x-value, keeping the y-value the same. First, make a guess at how do each of \\(h_{11}\\), \\(\\hat\\epsilon_1\\), \\(s_1\\), \\(t_1\\), and \\(D_1\\) change, writing down your guess. Then calculate the values and see if you were right!\nThe covariance of the estimated residuals is a strange idea to consider. I consider a lot of strange ideas; let’s do this via simulation! Generate a single vector of x values which will be used throughout the simulations, and a preliminary simulation of y values to use for now. For 1000 times, generate new y values, noting the first and second residuals. At the end, find the correlation between these two vectors, and compare to the theoretical correlation found by the \\(\\rho_{ij}\\) formula.\n\n\n\nSolution\n\n\nx &lt;- runif(40, 0, 10)\ny &lt;- 4 + 10 * x + rnorm(40, 0, 10)\nX &lt;- cbind(1, x)\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\ncor12 &lt;- -H[1, 2] / sqrt((1 - H[1, 1]) * (1 - H[2, 2]))\n\nR &lt;- 10000\ne1 &lt;- c()\ne2 &lt;- c()\nfor (i in 1:R) {\n    y &lt;- 4 + 3 * x + rnorm(40, 0, 3)\n    e &lt;- residuals(lm(y ~ x))\n    e1[i] &lt;- e[1]\n    e2[i] &lt;- e[2]\n}\ncor(e1, e2)\n\n[1] 0.01971098\n\ncor12\n\n[1] 0.006790357\n\n\nIt’s a simulation and there’s a lot of variance, but I’m satisfied with that.\nThe important thing: The correlation of residuals is based on if we were to do many different samples (with the x-values the same each time).",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Hat Matrix</span>"
    ]
  },
  {
    "objectID": "L09-Analysing-Resids.html",
    "href": "L09-Analysing-Resids.html",
    "title": "9  The Hat Matrix 2",
    "section": "",
    "text": "See The Appendix (which is subject to change each semester).",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Hat Matrix 2</span>"
    ]
  },
  {
    "objectID": "L10-Extra_Topics.html",
    "href": "L10-Extra_Topics.html",
    "title": "10  Special Topics",
    "section": "",
    "text": "10.1 Standardizing \\(X\\)",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Special Topics</span>"
    ]
  },
  {
    "objectID": "L10-Extra_Topics.html#standardizing-x",
    "href": "L10-Extra_Topics.html#standardizing-x",
    "title": "10  Special Topics",
    "section": "",
    "text": "Mean-Centering\nConsider \\(y_i = \\beta_0 + \\beta_1 x'_i\\), where \\(x'_i\\) are the “centered” versions of \\(x_i\\): \\[\nx'_i = x_i - \\bar x\n\\]\nThen \\(\\bar{x'} = 0\\) and the coefficient estimates become: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar{x'} = \\bar y\\\\\n&\\text{and}\\\\\n\\hat\\beta_1 &= \\frac{S_{XY}}{S_{XX}} = \\frac{\\sum_{i=1}^n(x_i' - \\bar{x'})^2}{\\sum_{i=1}^n(x_i' - \\bar{x'})(y_i - \\bar{y})} = \\frac{\\sum_{i=1}^nx_i'^2}{\\sum_{i=1}^nx_i'(y_i - \\bar{y})}\n\\end{align*}\\]\n\n\nMean-Centering and Covariance\n\n\nFor un-centered data: \\[\\begin{align*}\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2,\\\\\n\\text{where }(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\end{align*}\\] Note also that \\(S_{x'x'} = \\sum_{i=1}^n{x'}_i^2\\), so \\[\\begin{align*}\nV(\\hat{\\underline\\beta}^c) &= \\frac{\\sigma^2}{nS_{X'X'}}\\begin{bmatrix}\\sum {x'}_i^2 & 0\\\\0 & n\\end{bmatrix}\\\\\n& = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\end{align*}\\]\n\\(\\implies\\) no covariance!!!\n\nSimulations with same data, but one uses centered data (code in L02 Rmd).\n\n\n\n\n\n\n\n\n\n\n\n\n\nComments on \\(V(\\hat{\\underline\\beta}^c)\\)\n\\[\nV(\\hat{\\underline\\beta}^c) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\]\n\n\\(V(\\hat\\beta_0^c) = \\sigma^2/n \\implies sd(\\hat\\beta_0) = \\sigma/\\sqrt{n}\\).\n\nThe t-test for significance of \\(\\beta_0\\) is just a hypothesis test for \\(\\bar y = 0\\). \n\nNote that \\(\\underline y\\) hasn’t changed, so \\(\\hat{\\underline\\epsilon}\\) and \\(\\sigma^2\\) are unchanged.\n\\(V(\\hat\\beta_0^c) = \\sigma^2/\\sum_{i=1}^n{x'}_i^2\\) isn’t all that interesting…\n\n\n\nStandardizing \\(\\underline x\\)\nIn addition to mean-centering, divide by the sd of \\(\\underline x\\): \\[\nz_i = \\frac{x_i - \\bar x}{\\sqrt{S_{XX}/(n-1)}}\n\\]\nThen \\(\\bar z = 0\\) and \\(sd(z) = 1 \\implies S_{ZZ} = n-1\\).\nIt can be shown that: \\[\nV(\\hat{\\underline\\beta}^s) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (n-1)^{-1}\\end{bmatrix}\n\\]\n\n\\(\\underline x\\) doesn’t matter!!!\n\n\n\nStandardizing in Multiple Linear Regression\nSuppose we standardize each column of \\(X\\) (except the intercept).\nSeveral things happen:\n\nAll predictors are now in units of standard deviations!!!\n\nCoefficients are directly comparable!\n\nCovariances disappear!!!\n\nStandardizing doesn’t hurt and can often help \\(\\implies\\) it’s almost always worth it!\nPossible downside:\n\nHave to “un-standardize” to interpret the results.\n\n\n\nStandardization Example\n\n\n\n\nShow the code\nx &lt;- mtcars$wt\ny &lt;- mtcars$mpg\ncoef(lm(y ~ x))\n\n\n(Intercept)           x \n  37.285126   -5.344472 \n\n\n\n\n\nShow the code\n# Standardize x:\nx_prime &lt;- (x - mean(x)) / sd(x)\ncoef(lm(y ~ x_prime))\n\n\n(Intercept)     x_prime \n  20.090625   -5.229338 \n\n\n\n\nTo get back our original estimates, we “un-standardize”. \\[\ny = \\beta_0' + \\beta_1'x' = \\beta_0' + \\beta_1'\\left(\\frac{x - \\bar x}{sd(x)}\\right) = \\left(\\beta_0' - \\beta_1'\\frac{\\bar x}{sd(x)}\\right) + \\frac{\\beta_1'}{sd(x)}x = \\beta_0 + \\beta_1x\n\\]\n\n\nShow the code\nc(20.090625 - (-5.229338) * mean(x)/sd(x), -5.229338 / sd(x))\n\n\n[1] 37.285126 -5.344472\n\n\n\n\nInterpretations\n\nRaw: a one unit increase in \\(x\\) leads to a \\(\\hat\\beta\\) unit increase in \\(y\\).\n\nSlopes are in the units of the predictors.\nAssuming all other predictors are held constant.\n\nStandardized: a one standard deviation increase in \\(x\\) leads to a \\(\\hat\\beta^c\\) unit increase in \\(y\\)\n\nSlopes are all in the same units!\n\nBigger coefficient \\(\\implies\\) bigger effect on \\(y\\).\n\nStill assumes other predictors are held constant.\n\n\n\n\nStandardizing in R\n\n\nShow the code\n# Use the RHS of the model formula for mtcars\n# But remove the column of 1s\nX &lt;- model.matrix(~ wt + disp + qsec, data = mtcars)[, -1]\nX &lt;- scale(X)\nlm(mtcars$mpg ~ X)\n\n\n\nCall:\nlm(formula = mtcars$mpg ~ X)\n\nCoefficients:\n(Intercept)          Xwt        Xdisp        Xqsec  \n   20.09062     -4.92596     -0.01585      1.65587  \n\n\nHomework: recover the un-standardized coefficients.\n\n\nSimulation Example\n\n\nShow the code\nn &lt;- 50\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- runif(n, 0, 10) + 0.3 * x1\n\nx1s &lt;- scale(x1)\nx2s &lt;- scale(x2)\n\nraw_ests &lt;- matrix(ncol = 3, nrow = 1000)\nstd_ests &lt;- matrix(ncol = 3, nrow = 1000)\nfor (i in 1:1000) {\n    y &lt;- -10 + 5 * x1 + 2 * x2 + rnorm(n, 0, 10)\n    lm_raw &lt;- lm(y ~ x1 + x2)\n    raw_ests[i, ] &lt;- coef(lm_raw)\n    lm_std &lt;- lm(y ~ x1s + x2s)\n    std_ests[i, ] &lt;- coef(lm_std)\n}\n\npar(mfrow = c(2, 2))\nplot(x = raw_ests[, 1], y = raw_ests[, 2],\n    xlab = \"beta0\", ylab = \"beta1\",\n    main = \"Raw\")\nplot(x = raw_ests[, 3], y = raw_ests[, 2],\n    xlab = \"beta2\", ylab = \"beta1\",\n    main = \"Raw\")\nplot(x = std_ests[, 1], y = std_ests[, 2],\n    xlab = \"beta0\", ylab = \"beta1\",\n    main = \"Standardized\")\nplot(x = std_ests[, 3], y = std_ests[, 2],\n    xlab = \"beta2\", ylab = \"beta1\",\n    main = \"Standardized\")\n\n\n\n\n\n\n\n\n\nIn the plots above, the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) has been removed, even though we get the same estimates!!!\nNote, however, that the correlation between \\(\\beta_1\\) and \\(\\beta_2\\) has not changed at all. If you look closely, they are exactly the same, but with different scales. As a homework problem, you’ll show exactly what this change of scale is.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Special Topics</span>"
    ]
  },
  {
    "objectID": "L10-Extra_Topics.html#general-linear-hypotheses",
    "href": "L10-Extra_Topics.html#general-linear-hypotheses",
    "title": "10  Special Topics",
    "section": "10.2 General Linear Hypotheses",
    "text": "10.2 General Linear Hypotheses\nNote that this section was skipped in class.\n\nDiet vs. Exercise\nWhich is more important for weight loss?\nWe can set this up in a linear regression framework: \\[\n\\text{Loss}_i = \\beta_0 + \\beta_1\\text{CaloriesConsumed}_i + \\beta_2\\text{ExercisesMinutes}_i\n\\] where we assume CaloriesConsumed and ExerciseMinutes are standardized.\nOur question about the importance of diet versus exercise becomes a hypothesis test: \\[\nH_0:\\; \\beta_1 = \\beta_2\\text{ vs. }H_a:\\; \\beta_1 \\ne \\beta_2\n\\] Alternatively, the null can be written as \\(\\beta_1 - \\beta_2 = 0\\).\n\n\nLinearly Independent Hypotheses\nIn some cases, we might have a collection of hypotheses. For ANOVA: \\[\nH_0:\\; \\beta_2 - \\beta_1 = 0,\\; \\beta_3 - \\beta_2 = 0,\\; \\beta_4 - \\beta_3 = 0,\\dots,\\; and\\; \\beta_{p-1} - \\beta_{p-2} = 0\n\\] These hypotheses are linearly indepenent. To see why, we can write them in matrix form: \\[\n\\begin{bmatrix}\n0 & -1 & 1 & 0 & 0 & ...\\\\\n0 & 0 & -1 & 1 & 0 & ...\\\\\n0 & 0 & 0 & -1 & 1 & ...\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & ...\\\\\n\\end{bmatrix}\\hat{\\underline\\beta} = \\underline 0\n\\] where none of the rows are linear combinations of the others.\nWe’ll use the notation \\(C\\underline\\beta = \\underline 0\\).\n\n\nLinearly Independent Hypotheses\nThe \\(C\\) matrix can be row-reduced to the hypotheses \\(\\beta_i=0\\;\\forall i&gt;0\\). In this case, our hypothesized model is: \\[\nY_i = \\beta_0 + \\underline\\epsilon\n\\]\nWe have reduced \\(Y = X\\underline\\beta + \\underline\\epsilon\\) to \\(Y = Z\\underline\\alpha + \\underline\\epsilon\\), where \\(\\underline\\alpha = (\\beta_0)\\) and \\(Z\\) is a column of ones.\n\n\nLinearly Dependent Hypotheses\nConsider the model \\(Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\underline\\epsilon\\) and the hypotheses: \\[\nH_0:\\; \\beta_{11} = 0,\\ \\beta_1 - \\beta_2 = 0,\\; \\beta_1 - \\beta_2 + \\beta_3 = 0,\\; 2\\beta_1 - 2\\beta_2 + 3\\beta_3 = 0\n\\] We can write this as: \\[\n\\begin{bmatrix}\n0 & 0 & 0  & 1\\\\\n0 & 1 & -1 & 0\\\\\n0 & 1 & -1 & 1\\\\\n0 & 2 & -2 & 3\n\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\beta_{11}\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix}\n\\] With a little work, we can show that this reduces to the model: \\[\nY = \\beta_0 + \\beta(x_1 + x_2) + \\underline\\epsilon \\Leftrightarrow Y = Z\\underline\\alpha + \\underline\\epsilon\n\\]\n\n\nTesting General Linear Hypotheses\nConsider an arbitrary matrix for \\(C\\) (not linearly dependent), such that we can row-reduce \\(C\\) to \\(q\\) linearly independent hypotheses.\n\nFull Model\n\n\\(SS_E = Y^TY - \\hat{\\underline\\beta}^TX^TY\\) on \\(n-p\\) df.\n\nHypothesized Model\n\n\\(SS_W = Y^TY - \\hat{\\underline\\alpha}^TZ^TY\\) on \\(n-p-q\\) df.\n\n\nFrom these, we get: \\[\n\\left(\\frac{SSW-SSE}{q}\\right)/\\left(\\frac{SSE}{n-p}\\right) \\sim F_{q, n-p}\n\\] In other words, we test whether the restrictions significantly change the \\(SS_E\\)!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Special Topics</span>"
    ]
  },
  {
    "objectID": "L10-Extra_Topics.html#generalized-least-squares",
    "href": "L10-Extra_Topics.html#generalized-least-squares",
    "title": "10  Special Topics",
    "section": "10.3 Generalized Least Squares",
    "text": "10.3 Generalized Least Squares\n\nMain Idea\nWhat if the variance of \\(\\epsilon_i\\) isn’t the same for all \\(i\\)?\nIn other words, \\(V(\\underline\\epsilon) = V\\sigma^2\\) for some matrix \\(V\\).\n\nThe structure of \\(V\\) changes how we approach this.\n\nWeighted least squares: \\(V\\) is diagonal.\nGeneralized: \\(V\\) is symmetric and positive-definite, but otherwise arbitrary.\n\n\n\n\nTransforming the Instability Away\nIn the model \\(Y = X\\underline\\beta + \\underline\\epsilon\\), we want \\(V(Y) = I\\sigma^2\\), but we have \\(V(Y) = V\\sigma^2\\)\nSince \\(V\\) is symmetric and positive-definite, we can find a matrix \\(P\\) such that: \\[\nP^TP = PP = P^2 = V\n\\]\nWe can pre-multiply the model by \\(P^{-1}\\) so that \\(V(P^{-1}Y) = V^{-1}V\\sigma^2 = I\\sigma^2\\): \\[\nP^{-1}Y = P^{-1}X\\underline\\beta + P^{-1}\\underline\\epsilon \\Leftrightarrow Z = Q\\underline\\beta + \\underline f\n\\]\n\n\nGeneralized Least Squares Results\n\\[\\begin{align*}\n\\underline f^T\\underline f &= \\underline\\epsilon^TV^{-1}\\underline\\epsilon = (Y - X\\underline\\beta)^TV^{-1}(Y - X\\underline\\beta)\\\\\n\\hat{\\underline\\beta} &= (X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSS_T &= \\hat{\\underline\\beta}^TQ^TZ = Y^TV^{-1}X(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSST &= Z^TZ = Y^TV^{-1}Y\\\\\n\\hat Y &= X\\hat{\\underline\\beta}\\\\\n\\hat{\\underline f} &= P^{-1}(Y-\\hat Y) = P^{-1}(I-X(X^TV^{-1}X)^{-1}X^TV^{-1})Y\n\\end{align*}\\]\nMost things are just switching \\(Y\\) with \\(P^{-1}Y\\), etc., except one…\n\n\nOLS when you should have used GLS\nSuppose the true model has \\(V(\\underline\\epsilon) = V\\sigma^2\\).\nLet \\(\\hat{\\underline\\beta}_O\\) be the estimate of \\(\\underline\\beta\\) if we were to fit with Ordinary Least Squares. Then:\n\n\\(E(\\hat{\\underline\\beta}_O) = \\underline\\beta\\)\n\\(V(\\hat{\\underline\\beta}_O) = (X^TX)^{-1}X^TVX(X^TX)^{-1}\\sigma^2\\)\n\nThe OLS estimate is still unbiased, but has a much higher variance!\n\n\nChoosing \\(V\\)\n\nFor serially correlated data, \\(V_{ii} = 1\\) and \\(V_{ij} = \\rho^{|i-j|}\\)\n\nThis is choosing \\(V\\) based on model assumptions!\n\\(\\rho\\) must be estimated ahead of time.\n\nIf we have repeteated \\(x\\)-values, we can use the estimated variance from there.\n\nChoosing \\(V\\) based on the data\n\nIn a controlled experiment, where we have known weights for different \\(x\\)-values\n\nE.g., more skilled surgeons, machine age.",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Special Topics</span>"
    ]
  },
  {
    "objectID": "L10-Extra_Topics.html#exercises",
    "href": "L10-Extra_Topics.html#exercises",
    "title": "10  Special Topics",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises\n\nRepeat (and verify)the “un-standardizing” process for multiple linear regression. Use the regression formula mpg ~ disp + wt + qsec compared to the centered version to verify.\n\nUse this to explain why the slopes are still correlated after standardizing.\nVerify that the predictions from a standardized linear model are the same as those for an unstandardized linear model.\n\nExplain in your own words why the correlation between the intercept and the slope disappears when mean centering.\nIn general, show that a linear transformation of \\(x\\), i.e. \\(x' \\leftarrow a + bx\\), results in a transformation of \\(\\beta_1\\) as well as \\(\\beta_0\\).",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Special Topics</span>"
    ]
  },
  {
    "objectID": "L11-Wrong_Model.html",
    "href": "L11-Wrong_Model.html",
    "title": "11  Getting the Wrong Model",
    "section": "",
    "text": "11.1 The Wrong Model",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting the Wrong Model</span>"
    ]
  },
  {
    "objectID": "L11-Wrong_Model.html#the-wrong-model",
    "href": "L11-Wrong_Model.html#the-wrong-model",
    "title": "11  Getting the Wrong Model",
    "section": "",
    "text": "The Right Model?\nRecall: All models are wrong, some are useful!\nBut how wrong can a model be while still being useful?\n\nThis is an extraordinarily challenging philosophical question.\nWe will touch on a very small part of it\n\n\n\nThe Wrong Predictors\nSo far, we’ve talked about a model of the form \\(Y=X\\underline\\beta + \\underline\\epsilon\\).\n\n\\(E(\\hat{\\underline\\beta}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TX\\underline\\beta = \\underline\\beta\\)\n\nHowever, what if we are missing some predictors?\nWhat if the true model is \\(Y=X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\)? \\[\\begin{align*}\nE(\\hat{\\underline\\beta}) &= E((X^TX)^{-1}X^TY)\\\\\n& = (X^TX)^{-1}X^T(X\\underline\\beta + X_2\\underline\\beta_2) \\\\\n& = (X^TX)^{-1}X^TX\\underline\\beta + (X^TX)^{-1}X^TX_2\\underline\\beta_2 \\\\\n&= \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\\\\\n&= \\underline\\beta + A\\underline\\beta_2\n\\end{align*}\\]\n\n\nWhat is \\(A\\)?\nRecall that \\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^TY\\).\nOn the previous slide, we had the equation: \\[\nE(\\hat{\\underline\\beta}) = \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\n\\] Thoughts?\n\n\nBias due to wrong predictors\nThe bias of an estimator is: \\[\n\\text{Bias}(\\hat{\\underline\\beta}) = \\underline\\beta - E(\\hat{\\underline\\beta})\n\\]\nFor the case where \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 +\\underline\\epsilon\\), \\[\n\\text{Bias}(\\hat{\\underline\\beta}) = \\underline\\beta - (\\underline\\beta + A\\underline\\beta_2) = A\\underline\\beta_2\n\\]\n\n\nExpected Mean Square\nSee text.\nUses the identity: For an \\(n\\times n\\) matrix \\(Q\\) and \\(n\\times 1\\) random vector \\(Y\\) with variance \\(V(Y)=\\Sigma\\), \\[\nE(Y^TQY) = (E(Y))^TQE(Y) + trace(Q\\Sigma)\n\\]\nThis may be useful for a future assignment question (will notify if you need it), but for now I’m going to explore this via simulation in the Rmd.\n\n\nSummary\n\nChoosing the wrong set of predictors can affect the model!",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting the Wrong Model</span>"
    ]
  },
  {
    "objectID": "L11-Wrong_Model.html#what-have-we-learned",
    "href": "L11-Wrong_Model.html#what-have-we-learned",
    "title": "11  Getting the Wrong Model",
    "section": "11.2 What have we learned?",
    "text": "11.2 What have we learned?\n\nMultiple Linear Regression Concepts\n\nIf you add a predictor, the other coefficients change.\nVariance is everything\n\n\\(MS_E\\) = variance of residuals, \\(MS_{Reg}\\) = variance of the line!\n\\(SS_T = SS_{Reg} + SS_E\\)\n\n\\(df_T = df_{Reg} + df_E\\)\n\n\nAlways check assumptions\n\nResidual plots (using the appropriate residuals)!\n\nTry to test as few hypotheses as possible!\nThe hat matrix is magical.\n\n\n\nOrdinary Least Squares Estimates\nFor the model \\(y = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i\\):\n\nFind the OLS estimate by minimizing \\(\\sum_{i=1}^n\\epsilon_i^2 = \\sum(y_i - \\beta_0 - \\beta_1x_{i1} - \\beta_2x_{i2})^2 = \\underline\\epsilon^T\\underline\\epsilon\\)\n\n\\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^TY\\)\nThese estimates do not require the normality assumption.\n\nMLE gets the same estiamtes (assuming normality).\n\nWith normality assumptions,\n\n\\(V(\\hat{\\underline\\beta}) = (X^TX)^{-1}\\sigma^2\\)\n\nConfidence intervals and confidence regions\n\n\n\n\nANOVA: Variance tells us about slopes\n\\(H_0: SS_{Reg} = 0\\) is equivalent to \\(H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_{p-1} = 0\\).\n\nA horizontal line has a variance of 0!\n\n\\(\\beta_0 = \\bar y\\) does not have variance in the \\(y\\)-direction\nMore variance is a good thing, since this is the variance explained.\n\n\nThis extends to extra sum-of-squares due to adding predictons.\n\n\\(H_0: SS_1 - SS_2 = 0\\; \\Leftrightarrow\\; H_0: \\beta_q = \\beta_{q+1} = \\dots = \\beta_{p-1} = 0\\)",
    "crumbs": [
      "OLS Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting the Wrong Model</span>"
    ]
  },
  {
    "objectID": "L12-NonLinear.html",
    "href": "L12-NonLinear.html",
    "title": "12  Non-Linear Relationships with Linear Models",
    "section": "",
    "text": "12.1 Non-Linear Relationships",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Non-Linear Relationships with Linear Models</span>"
    ]
  },
  {
    "objectID": "L12-NonLinear.html#non-linear-relationships",
    "href": "L12-NonLinear.html#non-linear-relationships",
    "title": "12  Non-Linear Relationships with Linear Models",
    "section": "",
    "text": "Arbitrarily Shaped Functions\n\n\nThe plot on the right is the function: \\[\ny = 2 + \\frac{1}{5}x^2 - 8\\log(x) - 0.005x^3 + 20\\sin\\left(\\frac{x}{2}\\right) + \\epsilon\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThe twist: The fitted line is just a polynomial model: \\(y = \\beta_0 + \\sum_{j=1}^{12}\\beta_jx^j\\)\n\n\nFitting a Polynomial\nTo fit a polynomial of order \\(k\\): \\[\ny = \\beta_0 + \\sum_{j=1}^{k}\\beta_jx^j + \\epsilon\n\\] we can simply fit a linear model to transformed predictors, i.e.: \\[\nx_1 = x;\\; x_2 = x^2;\\; x_3 = x^3;\\;...\n\\] and we can just fit a linear model as usual!\n… that seems too easy?\n\n\nChoosing Polynomial Order\nThere are two common options:\n\nDomain knowledge\n\nIs there a theoretical reason to use a cubic?\n\nReduce prediction error\n\nCross-validation or ANOVA, depending on problem.\n\n\n\n\nDomain Knowledge: Stopping Distance\n\n\nThe stopping distance is theoretically proportional to the square of the speed.\n\nA line might fit\n\nFits poorly at 0 (negative stopping distances for positive speed?)\n\nA quadratic fits better?\nA cubic does something funky at 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing Order with ANOVA\n\n\nShow the code\nX &lt;- data.frame(dist = cars$dist, x1 = cars$speed, x2 = cars$speed^2, \n    x3 = cars$speed^3, x4 = cars$speed^4)\nlm(dist ~ ., data = X) |&gt; anova()\n\n\nAnalysis of Variance Table\n\nResponse: dist\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nx1         1 21185.5 21185.5 92.5775 1.716e-12 ***\nx2         1   528.8   528.8  2.3108    0.1355    \nx3         1   190.4   190.4  0.8318    0.3666    \nx4         1   336.5   336.5  1.4707    0.2316    \nResiduals 45 10297.8   228.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this situation, Sequential Sum-of-Squares makes sense! (Disagrees with theory, though. Go with theory.)\n\n\nStopping Distance \\(\\propto\\) Speed\\(^2\\)\n\n\nA second order polynomial is: \\[\ny = \\beta_0 + \\beta_1x + \\beta_2x^2\n\\]\nThe implied model is: \\[\ny = \\beta_2x^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnconventional ESS\n\n\nShow the code\nX &lt;- data.frame(dist = cars$dist, x1 = cars$speed, x2 = cars$speed^2, \n    x3 = cars$speed^3, x4 = cars$speed^4)\nm1 &lt;- lm(dist ~ x1 + x2, data = X)\nm2 &lt;- lm(dist ~ -1 + x2, data = X)\nanova(m2, m1)\n\n\nAnalysis of Variance Table\n\nModel 1: dist ~ -1 + x2\nModel 2: dist ~ x1 + x2\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1     49 11936                           \n2     47 10825  2    1111.2 2.4123 0.1006\n\n\nNot a significant difference in models, so go with simpler one: \\(y = \\beta_2x^2\\)\n\nThis is highly specific to this situation - see cautions later.\n\n\n\nPolynomial Models will Overfit!\nTrue model: \\(f(x) = 2 + 25x + 5x^2 - x^3\\) (cubic), Var(\\(\\epsilon\\)) = 40\n\n\n\n\n\n\n\n\n\n\n\nMultiple Regression Polynomial Models\nA full polynomial model of order 2 with two predictors is: \\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2\n\\] In R this can be specified as:\n\n\nShow the code\nlm(y ~ (x1 + x2)^2)\n\n\n\nThis is why you can’t use y ~ x + x^2 to get a polynomial model - R tries to interpret this as a model specification rather than a transformation. \nWe’ll learn more about interactions and transformations in the next few lectures.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Non-Linear Relationships with Linear Models</span>"
    ]
  },
  {
    "objectID": "L12-NonLinear.html#cautions-about-polynomials",
    "href": "L12-NonLinear.html#cautions-about-polynomials",
    "title": "12  Non-Linear Relationships with Linear Models",
    "section": "12.2 Cautions about Polynomials",
    "text": "12.2 Cautions about Polynomials\n\nLower Order Terms\nUnless there’s a strong physical reason,\n\n\\((ax - b)^2\\) is the model, not \\(\\beta_0 + \\beta_1x + \\beta_{11}x^2\\)\n\n\n\nOrders higher than 3 are rarely jutified\nRecall the interpretation of a slope:\n\nA one unit increase in \\(x\\) is associated with a \\(\\beta_1\\) unit increase in \\(y\\).\n\nThis interpretation fails in quadratrics, and fails spectacularly in higher orders.\n\n\nSee splines for more flexibility\n\n\nExtrapolation is Fraught with Peril\nUnless you have the true order (you don’t), polynomials diverge almost immediately.\n\n\nShow the code\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\",\n    subdir = \"Apps/polyFit\")\n\n\n\n\n\\(x\\) and \\(x^2\\) are correlated\nThis strongly affects parameter estimates.\n… unless…\n\n\npoly() uses orthogonal polynomials\n\n\nShow the code\nbetas &lt;- replicate(1000, \n    coef(lm(y ~ poly(x, 2, raw = TRUE),\n         data = data.frame(x = runif(30, 0, 10), \n            y = (x - 2)^2 + rnorm(30, 0, 10)))))\nplot(t(betas))\n\n\nAfter squaring, cubing, etc., each column of X is transformed to be orthogonal to the previous.\n\nTakes care of transformations for you when using predict().\nThe coef() function is useless.\nMean-centering also helps!\n\n\n\nOrthogonal Polynomials\n\n\nShow the code\nx &lt;- sort(runif(60, 0, 10))\npar(mfrow = c(2,3))\nfor(i in 1:3) {\n    plot(x, poly(x, 3, raw = TRUE)[,i])\n}\nfor(i in 1:3) {\n    plot(x, poly(x, 3, raw = FALSE)[,i])\n}",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Non-Linear Relationships with Linear Models</span>"
    ]
  },
  {
    "objectID": "L12-NonLinear.html#should-i-use-a-polynomial",
    "href": "L12-NonLinear.html#should-i-use-a-polynomial",
    "title": "12  Non-Linear Relationships with Linear Models",
    "section": "12.3 Should I Use a Polynomial?",
    "text": "12.3 Should I Use a Polynomial?\n\nExample: mtcars\n[code]\nPlay around with the polynomial order in the following code to try and get a good fit! (For fun, also try a degree 17 polynomial.) Then, reveal the solution below.\nNote that you can run this code right here - you don’t need to use your own computer! This is magical!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSolution\n\nA polynomial model is not the best model here! The following plot shows three related linear models (which we’ll learn about later), demonstrating that it is linear but there’s a hidden grouping!\n\n\n\n\n\n\n\n\n\n\n\n\n\nClosing Notes: Linear Models are fine with non-linear trends\nThe formula \\(y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ... + \\epsilon_i\\) doesn’t care how \\(x\\) is defined.\n\n\\(x_{i2} = x_{i1}^2\\) works as if \\(x_{i1}\\) and \\(x_{i2}\\) are just two predictors.\n\\(x_{i2} = \\log(x_{i1})\\) is fine too!\n\\(x_{i2} = \\log(x_{i1}) + \\exp(x_{i3})\\tan(1)\\)? Sure, why not!\n\\(x_{i2} = \\log(\\beta_1x_{i1})\\) is a total no-go.\n\nLinear models are linear in the parameters.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Non-Linear Relationships with Linear Models</span>"
    ]
  },
  {
    "objectID": "L13-Transforming_Response.html",
    "href": "L13-Transforming_Response.html",
    "title": "13  Transformations",
    "section": "",
    "text": "13.1 Transformations",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "L13-Transforming_Response.html#transformations",
    "href": "L13-Transforming_Response.html#transformations",
    "title": "13  Transformations",
    "section": "",
    "text": "Transforming the Predictors\nSuppose we found that the following second order polynomial model was a “good” fit: \\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_{11}x_{i1}^2 +\\beta_2x_{i2} + \\beta_{22}x_{i2}^2 + \\beta_{12}x_{i1}x_{i2} + \\epsilon_i\n\\]\nNow consider the model: \\[\n\\ln(y_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i\n\\]\n\n3 parameters instead of 6!\n\nNo interaction term!\n\nIf we’re okay with the log scale for \\(y\\), it’s easier to interpret.\n\n\n\nTransforming the predictors\n\n\nOriginal scale\n\n\n\n\n\n\n\n\n\n\nLog scale\n\n\n\n\n\n\n\n\n\n\n\nThe logarithm made it simpler, even if it’s still not quite right.\nThe code below can be run on your local machine (the rgl library needs to make it’s own window, which it can’t do in any online format).\nPlay around with some transformations of \\(x_1\\) and \\(x_2\\), then play around with transformations of \\(y\\)!\n\nlibrary(rgl)\nset.seed(2112)\nn &lt;- 200\nx1 &lt;- runif(n, 3, 10)\nx2 &lt;- runif(n, 3, 10)\ny &lt;- 20 + 5*x1 + 15*x1^2 + 5*x2 + 15*x2^2  + 2*x1*x2 + rnorm(n, 0, 2)\n\nrgl::plot3d(x1, x2, y)\n\n\n\nConsequences of Logarithms\nConsider the simple model \\(E(y_i) = \\beta x^2\\). Taking the logarithm of both sides: \\[\n\\ln(E(y_i)) = \\ln(\\beta) + 2\\ln(x) := \\beta_0 + \\beta_1 \\ln(x)\n\\] and we have something that looks more like a linear model.\n\nNote that, instead of \\(x^2\\), \\(x^{2.1}\\) would also work as a model.\n\nThe power of \\(x\\) is estimated.\n\nIt’s also possible that the log scale is the correct scale for \\(y\\)\n\n\\(E(\\ln(y_i)) = \\beta_0 + \\beta_1x\\)\nIn other words, don’t get too bogged down by whether we take the ln of \\(x\\).\n\n\n\n\nLogarithms and Errors\nIf we believe that the log scale is a better scale for \\(y\\), we may postulate the model: \\[\n\\ln(y_i) = \\beta_0 + \\beta_1\\ln x_{i1} + \\beta_2\\ln x_{i2} + \\epsilon\n\\] which implies that the orginal scale for \\(y\\) has the form: \\[\ny_i = e^{\\beta_0}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2}e^{\\epsilon_i}\n\\] The errors are multiplicative!!!\n\nOption 1: Accept this\n\nAllows us to use least squares.\n\nOption 2: Use the model \\(y_i = e^{\\beta_0}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2} + \\epsilon_i\\)\n\nMight be better, but requires a bespoke estimation algorithm.\n\n\n\n\nAre we also taking the log of \\(x\\)?\n\n\nNo (the usual case)\n\\[\n\\ln(E(y_i)) = \\beta_0 + \\beta_1x_{i1} + \\beta_2 x_{i2}\n\\]\n\\[\n\\implies y_i = e^{\\beta_1}e^{\\beta_1x_{i1}}e^{\\beta_2x_{i2}}e^{\\epsilon_i}\n\\]\n\nUsually used in order to make the plot look more linear.\n\n\nYes (if there’s good reason)\n\\[\n\\ln(E(y_i)) = \\beta_0 + \\beta_1\\ln x_{i1} + \\beta_2 \\ln x_{i2}\n\\]\n\\[\n\\implies y_i = e^{\\beta_1}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2}e^{\\epsilon_i}\n\\]\n\nUsually used if we think the form \\(y = \\beta_0x^\\beta e^\\epsilon\\) is the correct form.\n\n\n\nSecret third option: transform the \\(x\\) values only (example: polynomial regression).\nYou may notice that the first row in each column transforms \\(E(y_i)\\), but the second is the transform of \\(y_i\\). This is not a mistake. We transform the expected relationship between \\(X\\) and \\(y\\), then fit the model assuming that the residuals are additive so we can use our usual least squares estimators. To transform them back, we don’t know the residuals and therefore they get caught up in the transformation.\n\n\nGeneral Practice\nWe often simply use the model \\(\\ln \\underline y = X\\beta + \\underline \\epsilon\\) and do everything on the log scale.\n\nSimpler, but still useful.\nGood predictions of \\(\\ln y_i\\) can be transformed to good predictions of \\(y_i\\).\nIf there’s a good reason to transform the \\(x\\) values, we’ll try that first.\n\nE.g. stopping distance \\(\\propto\\) speed\\(^2\\); decibels are on the log scale but we might want them on the raw sale, etc.\n\n\nIn general: Decide on a functional relationship between \\(f(y)\\) and \\(X\\), then use additive errors on the scale of \\(f(y)\\).\nThis has consequences for the residuals.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "L13-Transforming_Response.html#residuals-in-transformed-space",
    "href": "L13-Transforming_Response.html#residuals-in-transformed-space",
    "title": "13  Transformations",
    "section": "13.2 Residuals in Transformed Space",
    "text": "13.2 Residuals in Transformed Space\n\nVariance Stabilization\nThe two main purposes of transformations:\n\nFit non-linear functional forms.\nStabilize the variance!\n\nScale-Location plot in the R defaults.\n\n\nFor example, the log function brings large values down a lot, small values down a little.\n\nThe scale of large residuals is decreased more than the scale of small residuals.\n\n\n\n\\(f(\\underline y) = X\\beta + \\underline\\epsilon\\)\nThe estimated residuals are \\(\\hat\\epsilon_i = f(y_i) - \\widehat{f(y_i)}\\)\n\nNote the awkwardly long hat!\n\nWe’re estimating the value of the function, not the value of \\(y_i\\).\nIf \\(f(y_i) - \\widehat{f(y_i)} = f(y_i - \\widehat{y_i})\\), then the original function must have been linear (and a transformation was useless).\n\nWe’re assuming \\(\\epsilon_i\\stackrel{iid}{\\sim} N(0, \\sigma^2)\\), which is difficult to translate to \\(f^{-1}(X\\beta + \\underline\\epsilon)\\).\n\nIn the special case of \\(\\ln\\), \\(\\exp{\\epsilon_i} \\sim \\text{LogNormal}(0, \\sigma^2)\\).\nNo assumption of independence on the original scale!!!\n\nWe assume that the residuals have the same variance on the transformed scale.\n\nLikely not true for the original scale of \\(y\\).\n\n\n\n\nSome Good News\nIf \\((a,b)\\) is a \\((1-\\alpha)\\) CI on the scale of \\(f(y)\\), then \\((f^{-1}(a), f^{-1}(b))\\) is a valid CI on the scale of \\(y\\).\n\nIt’s not the only valid CI!\n\nNote that it’s not a symmetric CI!\n\nWorks for \\(y\\) as well as the \\(\\beta\\) parameters.\n\nTransformation might induce dependence among the parameters.\nA CI for \\(\\beta_1\\) is useless if there’s high covariance with \\(\\beta_2\\).\n\n\n\n\nExample: Dugongs\nCheck the diagnostic plots for the following models!\nThis is that in-browser R magic that we’ve seen before! Also ask yourself what it means for the log of age to increase with the length of the dugong. Is the log easy to interpret? Do the x and y variables make sense like this?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "L13-Transforming_Response.html#choosing-transformations",
    "href": "L13-Transforming_Response.html#choosing-transformations",
    "title": "13  Transformations",
    "section": "13.3 Choosing Transformations",
    "text": "13.3 Choosing Transformations\n\nMethods for Choosing Transformations\n\nTheory.\n\nIf theory says that the log transform makes sense, use that.\n\nDon’t even consider the next steps. Just go with theory.\n\nExample: Forest fire burn sizes are right skewed, the log-transform makes sense.\n\nExperimentation after looking at the Scale-Location plot.\n\nIf log or sqrt don’t work, move on to step three.\n\nThe Box-Cox Transformation\n\nFinds an appropriate transformation using maximum likelihood.\n\n\n\n\nBox-Cox\nWe use the transformation: \\[\nV = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda \\dot{Y}^{\\lambda - 1}} & \\text{if }\\lambda \\ne 0\\\\\n\\dot{Y}\\ln(Y) & \\text{if }\\lambda = 0\n\\end{cases}\n\\] where \\(\\dot Y\\) is the geometric mean of \\(y\\).\n\\(\\lambda\\) is chosen through maximum likelihood\n\nEssentially, refit with each value of \\(\\lambda\\) and see which minimizes the residual variance.\n\nPlot the likelihhods and choose the highest.\n\n\n\n\nSimpler Box-Cox\nThe textbook recommends the previous formula, however R uses: \\[\nW = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda} & \\text{if }\\lambda \\ne 0\\\\\n\\ln(Y) & \\text{if }\\lambda = 0\n\\end{cases}\n\\]\nNotice:\n\n\\(\\lambda \\approx -1 \\implies\\) inverse transformation.\n\\(\\lambda \\approx 0 \\implies\\) log transformation.\n\\(\\lambda \\approx 0.5 \\implies\\) sqrt transformation.\n\\(\\lambda \\approx 1 \\implies\\) no transformation.\n\\(\\lambda \\approx k, k\\in\\mathbb{Z} \\implies\\) use \\(y^k\\).\nThere’s no \\(\\lambda\\) for \\(exp(y)\\)!\n\n\n\nVariance of \\(\\lambda\\)\nIf we had a different data set, we’d get a different value of \\(\\lambda\\)!\nR reports the the log-likelihood values, along with the top 5%.\n\nAnything in the top 5% is reasonable.\n\nIt’s not an exact science.\n\nUsually, we check the best \\(\\lambda\\) values and round to something nice.\n\nlog, sqrt, squared, inverse, etc.\n\n\n\n\nExample\n\nGenerate y ~ log(x), try fitting y ~ log(x) and also a transformation for y.\nGenerate log(y) ~ x, try fitting y ~ exp(x) and also log(y) ~ x.\nGenerate log(y) ~ log(x), try various transformations of y and x.\nGenerate y ~ x^2, try various transformations of y and x.\nGenerate y^2 ~ x, try various transformations of y and x.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAfter some exploring, you should find that the log and sqrt transformations cover most situations adequately. Fitting the correct model is obviously best, but we can never know what the correct model is!!!\nAs practice, try writing out all models in the form \\(y = g(f(x) + \\epsilon)\\) to see how the error terms get affected. For example, generating data like \\(log(y) = log(x) + epsilon\\) implies the model \\(y = exp(log(x) + \\epsilon)\\), which does not simplify due to the additive error term in the exponential.\n\n\nSummary\n\nChoosing a transformation:\n\nTheory\nExploration\nRounded value from Box-Cox.\n\nWorking with a transformation:\n\nChoose functional form, assume additive errors (usually, not always!)\nStay on the transformed scale\n\nAll assumptions about residuals apply to the transformed scale!\n\n\n\nTo be useful, all transformations should consider the context of the problem!",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "L14-Modelling_Transformations.html",
    "href": "L14-Modelling_Transformations.html",
    "title": "14  Modelling with Transformations",
    "section": "",
    "text": "14.1 Introduction",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelling with Transformations</span>"
    ]
  },
  {
    "objectID": "L14-Modelling_Transformations.html#introduction",
    "href": "L14-Modelling_Transformations.html#introduction",
    "title": "14  Modelling with Transformations",
    "section": "",
    "text": "The Trees Data\n\n\n\ndata(trees)\ntrees$Diam &lt;- trees$Girth / 12\ntrees$Girth &lt;- NULL\nhead(trees)\n\n  Height Volume      Diam\n1     70   10.3 0.6916667\n2     65   10.3 0.7166667\n3     63   10.2 0.7333333\n4     72   16.4 0.8750000\n5     81   18.8 0.8916667\n6     83   19.7 0.9000000\n\n\n\nFrom the help file:\n\n“Girth” is actually the diameter, in inches\n\nRenamed Diam, transformed to feet\n\nHeight is in feet\nVolume of the timber (usable wood) (cubic feet)\n\n\n\nGoal: Model the volume of the tree as a function of Height and Diameter.\n\n\nDIY\nThe course notes include some starter code (that can be run in browser). Can you fit a better model than the basic multiple linear regression?\nGet in groups, use pen and paper to record the important parts of the transformations you try.\nHint: what’s the Volume of a cylinder?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nMy Attempts\nIn the following, I’ll try a couple different models.\nNote that I’ve made the output smaller for the slides - you want to look at more than just these values!\n\n\nMultiple Linear Regression\n\ndata(trees)\ntrees$Diam &lt;- trees$Girth / 12\ntrees$Girth &lt;- NULL\nbase_lm &lt;- lm(Volume ~ Diam + Height, data = trees)\nsummary(base_lm)$coef |&gt; round(4)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -57.9877     8.6382 -6.7129   0.0000\nDiam         56.4979     3.1712 17.8161   0.0000\nHeight        0.3393     0.1302  2.6066   0.0145\n\nsummary(base_lm)$adj.r.squared\n\n[1] 0.9442322\n\n\n\n\nMultiple Linear Regression - Plots\n\n\n\n\n\n\n\n\n\n\n\nAttempt 1: Logs\nNotice that the model \\(E(y_i) = \\beta_1x_{i1}^2x_{i2}\\) is equivalent to: \\[\n\\ln(E(y_i)) = \\ln(\\beta_1) + 2\\ln(x_{i1}) + \\ln(x_{i2})\n\\]\n\nfull_log_lm &lt;- lm(log(Volume) ~ log(Height) + log(Diam), data = trees)\nsummary(full_log_lm)$coef |&gt; round(4)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -1.7049     0.8819 -1.9332   0.0634\nlog(Height)   1.1171     0.2044  5.4644   0.0000\nlog(Diam)     1.9826     0.0750 26.4316   0.0000\n\nsummary(full_log_lm)$adj.r.squared\n\n[1] 0.976084\n\n\n\n\nAttempt 1 Residual Plots\n\n\n\n\n\n\n\n\n\n\n\nAlternative 2: log(y)\nLet’s try not logging the x-values\n\ny_log_lm &lt;- lm(log(Volume) ~ Height + Diam, data = trees)\nsummary(y_log_lm)$coef |&gt; round(4)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.1026     0.2153  0.4764   0.6375\nHeight        0.0164     0.0032  5.0508   0.0000\nDiam          1.7435     0.0790 22.0569   0.0000\n\nsummary(y_log_lm)$adj.r.squared\n\n[1] 0.9661964\n\n\n\n\nAttempt 2 Residual Plots\n\n\n\n\n\n\n\n\n\n\n\nBox-Cox?\n\n\n\nBox-Cox says something between 0 and 1, closer to 0.\nWhat could we round to?\n\n\n\nlibrary(MASS)\nboxcox(lm(Volume ~ Height + Diam, data = trees))\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the Box-Cox Parameter: Sqrt\n\ny_sqrt_lm &lt;- lm(sqrt(Volume) ~ Height + Diam, data = trees)\nsummary(y_sqrt_lm)$coef |&gt; round(4)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -2.7700     0.5094 -5.4375    0e+00\nHeight        0.0358     0.0077  4.6589    1e-04\nDiam          4.8591     0.1870 25.9828    0e+00\n\nsummary(y_sqrt_lm)$adj.r.squared\n\n[1] 0.9740084\n\n\n\n\nUsing the Box-Cox Parameter: Quarter Power\n\ny_quarter_lm &lt;- lm(I(Volume^(1/4)) ~ Height + Diam, data = trees)\nsummary(y_quarter_lm)$coef |&gt; round(4)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.5108     0.1057  4.8318        0\nHeight        0.0085     0.0016  5.3258        0\nDiam          1.0235     0.0388 26.3699        0\n\nsummary(y_quarter_lm)$adj.r.squared\n\n[1] 0.9753826\n\n\n\n\nWhat About Transforming \\(x\\) only?\n\npoly_lm &lt;- lm(Volume ~ poly(Height, 2) + poly(Diam, 2), data = trees)\nsummary(poly_lm)$coef |&gt; round(4)\n\n                 Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)       30.1710     0.4802 62.8290   0.0000\npoly(Height, 2)1  13.1319     3.1368  4.1864   0.0003\npoly(Height, 2)2   0.4266     2.9584  0.1442   0.8865\npoly(Diam, 2)1    80.2382     3.1323 25.6167   0.0000\npoly(Diam, 2)2    15.2171     2.9632  5.1354   0.0000\n\nsummary(poly_lm)$adj.r.squared\n\n[1] 0.9735436\n\n\n\n\nQuadratic Polynomial Plots\n\n\n\n\n\n\n\n\n\n\n\nPolynomials of Order 8? (\\(R^2\\) = 0.9864, \\(R^2_{adj}\\) = 0.9708)\n\n\n\n\n\n\n\n\n\n\n\nMy Solution\nLet CylVolume be the volume of the cylinder defined by Height and Diam, \\(V = \\pi (d/2)^2h\\).\nThe model is \\(Volume_i = \\beta_1CylVolume_i + \\epsilon_i\\), where \\(\\beta_1\\) is the proportion of an ideal cylinder that is actual usable wood.\n\ntrees$CylVolume &lt;- pi * (trees$Diam/2)^2 * trees$Height\ncyl_lm &lt;- lm(Volume ~ -1 + CylVolume, data = trees)\nsummary(cyl_lm)$coef |&gt; round(4)\n\n          Estimate Std. Error t value Pr(&gt;|t|)\nCylVolume   0.3865      0.005 77.4365        0\n\nsummary(cyl_lm)$adj.r.squared\n\n[1] 0.994856\n\n\n\n\nCylVolume Plots\n\npar(mfrow = c(2, 3))\nplot(Volume ~ CylVolume, data = trees,\n    main = \"Volume versus CylVolume\")\nplot(1, main = \"Blank Space\", bty = \"n\", xaxt = \"n\", yaxt = \"n\",\n    xlab = \"\", ylab = \"\", pch = \"\")\nplot(cyl_lm)\n\n\n\n\n\n\n\n\n\n\nSome Closing Thoughts\n\nUsing ln(y) ~ ln(x) made the \\(R^2\\) slightly better, even though plots looked similar.\n\nPlots looked slightly better for full log, though.\n\nWe were using log just to make the relationship linear.\n\nBox-Cox told us to use sqrt or quarter power instead - result was better!\n\nVery interestingly, models that chose powers of \\(x\\) chose 1 for Height, 2 for Diam…\nTrees are not perfect cylinders.\n\nHowever, a cylinder model fits best and with fewer parameters!!!\n\n\nIn conclusion, always think through the problem before blindly modelling.\nNote that the best model in this case used our knowledge of the physical problem, and this happened to correspond to the best fitting model. This isn’t always the case! It is very common that we’ll have to choose between a model that fits well and a model that matches our understanding of the world, and this is not an easy choice!",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelling with Transformations</span>"
    ]
  },
  {
    "objectID": "L15-Multicollinearity.html",
    "href": "L15-Multicollinearity.html",
    "title": "15  Multicollinearity",
    "section": "",
    "text": "15.1 The Problem",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multicollinearity</span>"
    ]
  },
  {
    "objectID": "L15-Multicollinearity.html#the-problem",
    "href": "L15-Multicollinearity.html#the-problem",
    "title": "15  Multicollinearity",
    "section": "",
    "text": "The Problem with Multicollinearity\n\n\n\nMultiple regression fits a hyperplane\nIf the points form a “tube”, an infinite number of hyperplanes work.\n\nRotate plane around axis of tube.\n\n\n\nshiny::runGitHub(\n    repo = \"DB7-CourseNotes/TeachingApps\",\n    subdir = \"Apps/multico\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsequences of the Problem\n\n\nHigh cor. in \\(X\\) \\(\\implies\\) high cor. in \\(\\hat{\\underline\\beta}\\).\n\nMany combos of \\(\\hat{\\underline\\beta}\\) are equally likely\nNo meaningful CIs\n\n\n\n\nShow the code\nset.seed(2112)\nreplicate(1000, {\n    y &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))[-1]\n}) |&gt; \n    t() |&gt; \n    plot(xlab = expression(hat(beta)[1]), \n        ylab = expression(hat(beta[2])),\n        main = \"Estimated betas for correlated\\npredictors, many samples\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother Formulation of the Problem\nConsider the model \\(y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i\\), where \\(x_{i1} = a + bx_{i2} + z_i\\) and \\(z_i\\) represents some extra uncertainty.\nFitting the model, we could:\n\nSet \\(\\hat\\beta_1\\) to 0, let \\(x_2\\) model all of the variance.\nSet \\(\\hat\\beta_2\\) to 0, let \\(x_1\\) model all of the variance.\nSet \\(\\hat\\beta_1\\) to any value, solve for \\(\\hat\\beta_2\\).\n\nBasically the same results regardless of what \\(\\hat\\beta_1\\) is chosen as.\n\n\nIn other words, the parameter estimates are not unique (or nearly not unique).\n\n\nThe Source of the Problem\n\\[\n\\hat{\\underline{\\beta}} = (X^TX)^{-1}X^TY,\\quad V(\\hat{\\underline{\\beta}}) = (X^TX)^{-1}\\sigma^2\n\\]\n\nIf two columns of \\(X\\) are linearly dependent, then \\(X^TX\\) is singular.\n\nConstant predictor value (linearly dependent with column of 1s).\nUnit change (one column for Celcius, one for Fahrenheit).\n\nIf two columns of \\(X\\) are nearly linearly dependent, then some elements of \\((X^TX)^{-1}\\) are humungous.\n\nTwo proxy measure for the same thing (e.g., daily high and low temperatures).\nNearly linear transformation (e.g., polynomial or BMI)\n\n\n\n\nDetecting the Problem\nThe variance-covariance matrix of \\(X\\) can be useful: \\[\nCov(X) = \\begin{bmatrix}\n0 & 0 & 0 & 0 & \\cdots\\\\\n0 & V(X_1) & Cov(X_1, X_2) & Cov(X_1, X_3) & \\cdots\\\\\n0 & Cov(X_1, X_2) & V(X_2) & Cov(X_2, X_3) & \\cdots\\\\\n0 & Cov(X_1, X_3) & Cov(X_2, X_3) & V(X_3) & \\cdots\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\] Why are the first column/row 0?\n\n\nPlotting \\(V(X)\\)\n\n\nShow the code\nlibrary(palmerpenguins); library(GGally)\nggcorr(penguins)\n\n\nWarning in ggcorr(penguins): data in column(s) 'species', 'island', 'sex' are\nnot numeric and were ignored\n\n\n\n\n\n\n\n\n\n\n\nDetecting the Problem: \\(V(\\hat{\\underline\\beta})\\)\nUnfortunately, the var-covar matrix is hard to get from R.\n\nWe can look at the SE column of the summary output!\n\nVery very very much not conclusive.\n\nThe Variance Inflation Factor\n\n\n\nThe Variance Inflation Factor\nWe can write the variance of each estimated coefficeint as: \\[\nV(\\hat\\beta_i) = VIF_i\\frac{\\sigma^2}{S_{ii}}\n\\] where \\(S_{ii} = \\sum_{k=1}^n(x_{ki} - \\bar{x_i})^2\\) is the “SS” for the \\(i\\)th column of \\(X\\).\n\nIf there is no “Variance Inflation”, then VIF = 1\n\n“Inflation” comes from the idea of rotating a plane around a “tube”.\nAlso interpreted as a measure of linear dependence with other columns of \\(X\\).\n\n\n\n\nInterpreting the Variance Inflation Factor\nConsider a regression of \\(X_i\\) against all other predictors.\n\nThe \\(R^2\\) measures how well the other predictors can model \\(X_i\\)\n\nLabel this \\(R_i^2\\) to indicate it’s the \\(R^2\\) for \\(X_i\\) against other columns.\n\nImportant: We’re not considering \\(\\underline y\\) at all!\n\nThe VIF is calculated as: \\[\nVIF_i = \\frac{1}{1 - R_i^2}\n\\]\n\nIf \\(R_i^2=0\\), then \\(VIF_i = 1\\)\nIf \\(R_i^2\\rightarrow 1\\), then \\(VIF_i \\rightarrow \\infty\\)\n\n\n\nPenguins VIF in R\n\n\nShow the code\nlibrary(car) # vif() function\nbogy_mass_lm &lt;- lm(\n    formula = body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = subset(penguins, species == \"Chinstrap\")\n)\nvif(bogy_mass_lm)\n\n\nflipper_length_mm    bill_length_mm     bill_depth_mm \n         1.541994          1.785702          2.092955 \n\n\n\n\nBad VIF in R\n\n\n\n\nShow the code\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- 2 + x1 + runif(n, -1, 1)\ny &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n\nmylm &lt;- lm(y ~ x1 + x2)\nvif(mylm)\n\n\n      x1       x2 \n23.39082 23.39082 \n\n\nRule of thumb: VIF &gt; 10 is a bad thing. 5 &lt; VIF &lt; 10 is worth looking into.\n\n\n\nShow the code\nscatter3D(x1, x2, y, bty = \"g\", colkey = FALSE,\n    xlab = \"x1\", ylab = \"x2\", zlab = \"y\")\n\n\n\n\n\n\n\n\n\n\n\nTry changing the code until the VIF is less than 10. What do you notice about the plot?\nChange the variables that say “Change me!” and see what happens. Note: You can hold the “Shift” button and hit “Enter” to run the code. You can hold shift and hit enter repeatedly to do a sort of psuedo-simulation (notice how VIF also has a sampling distribution!).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLimitation: This is just one of many, many, many ways in which predictors can be correlated.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multicollinearity</span>"
    ]
  },
  {
    "objectID": "L15-Multicollinearity.html#will-scaling-fix-the-problem",
    "href": "L15-Multicollinearity.html#will-scaling-fix-the-problem",
    "title": "15  Multicollinearity",
    "section": "15.2 Will Scaling Fix the Problem",
    "text": "15.2 Will Scaling Fix the Problem\n\nScaling the Predictors\nIf we subtract the mean and divide by the sd, some of the correlation goes away.\n\nThis is actually kinda bad - we’ve hidden some multicollinearity from ourselves!\n\nIf \\(Z\\) is the standardized version of \\(X\\), then \\[\nCor(X) = Z^TZ/(n-1)\n\\]\nIf \\(Z\\) is the mean-centered version of \\(X\\), then \\[\nCov(X) = Z^TZ/(n-1)\n\\]",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multicollinearity</span>"
    ]
  },
  {
    "objectID": "L15-Multicollinearity.html#fixing-the-problem",
    "href": "L15-Multicollinearity.html#fixing-the-problem",
    "title": "15  Multicollinearity",
    "section": "15.3 Fixing The Problem",
    "text": "15.3 Fixing The Problem\n\nOne way to fix the problem\nDon’t.\nWe can’t get good estimates of the \\(\\hat\\beta\\)s, but we can still get good predictions.\n\nThis only works if the new values are in the same “tube” as the others.\nIf the multicollinearity is real, what estimates do you expect?\n\nWithout a controlled experiment, there isn’t a good way to estimate the effect of \\(X_1\\) on it’s own!\n\n\n\n\nRemoving predictors\nIf two predictors are measuring the same thing, then just include one?\n\nThis might lose some information!\n\nIt also might not!\n\nThe estimated \\(\\beta\\) won’t be meaningful.\n\nInferences will be difficult.\n\nThere might be a good reason to choose one predictor (or transform them).\n\nExample: Height and weight are correlated, but BMI might be more useful to medical researchers.\n\nBMI is highly fraught.\n\nExample: If you have Celcius and Fahrenheight…",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Multicollinearity</span>"
    ]
  },
  {
    "objectID": "L16-Dummies.html",
    "href": "L16-Dummies.html",
    "title": "16  Dummy Variables",
    "section": "",
    "text": "16.1 0/1 Predictors",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dummy Variables</span>"
    ]
  },
  {
    "objectID": "L16-Dummies.html#predictors",
    "href": "L16-Dummies.html#predictors",
    "title": "16  Dummy Variables",
    "section": "",
    "text": "Dummy Coding\n“Dummy” variables are just predictors that only take the values 0 and 1.\n\n0 pairs of glasses versus 1 pair of glasses\n\nThis is a count that can only be 0 or 1\n\n0 means automatic, 1 means manual\n\nArbitrary choice of 0/1\n\n0 means off, 1 means on\n\nNatural choice of 0/1, but still arbitrary\n\n\n\n\nSlopes with a Dummy Variable\nUsual interpretation: as \\(x\\) increases by 1, \\(y\\) increases by \\(\\beta\\).\nThis doesn’t go away, but we get a new interpretation!\n\n\\(x\\) can only increase by one (from 0 to 1).\n\n\\(\\beta\\) is the difference in groups.\n\n\nNote that we assume constant variance; this means the variance is the same in both groups\n\nExact same assumptions as a t-test.\n\n(Different from a “Welch” t-test)\n\n\n\n\nCategorical Variables\nConsider the species column in penguins. We could code three dummy variables:\n\n\\(I(species == Adelie)\\)\n\\(I(species == Chinstrap)\\)\n\\(I(species == Gentoo)\\)\n\nFor brevity, \\(I(Adelie)\\) is the same as \\(I(species == Adelie)\\).\nThis would lead to the model: \\[\ny = \\beta_0 + \\beta_1I(Adelie) + \\beta_2I(Chinstrap) + \\beta_3I(Gentoo)\n\\]\nWhat’s the interpretation of the intercept here?\n\n\nCategorical Variable Dummy Coding\nBetter: set one as a reference variable and let the intercept “absorb” it:\n\n\\(I(species == Chinstrap)\\)\n\\(I(species == Gentoo)\\)\n\nThis would lead to the model: \\[\ny = \\beta_0 + \\beta_1I(Chinstrap) + \\beta_2I(Gentoo)\n\\] where\n\n\\(\\beta_0\\) is the mean of body mass for Adelie penguins.\n\\(\\beta_1\\) is the difference in mean body mass between Adelie and Chinstrap.\n\\(\\beta_2\\) is the difference in mean body mass for Adelie versus Gentoo.\n\nDifference between Chinstrap and Gentoo can be found with some cleverness.\n\n\n\n\nModel-related plot\n\n\n\nbody_mass_g varies by different levels of species\n\nModel assumes equal variance.\n\n\nThis is the plot you should check to see if a dummy variable makes sense to include.\nAnother plot better describes the results…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels with Categorical Variables\nThe model \\(y = \\beta_0 + \\beta_1I(species == Chinstrap) + \\beta_2I(species == Gentoo)\\) is equivalent to: \\[\ny_i = \\begin{cases}\\beta_0 + 0 & \\text{if }\\; species == Adelie\\\\ \\beta_0 + \\beta_1 & \\text{if }\\; species == Chinstrap\\\\\\beta_0  + \\beta_2 & \\text{if }\\; species == Gentoo\\end{cases}\n\\]\nThis is equivalent to fitting an intercept-only model \\(y = \\beta_0\\) for subsets of the data.\nBy putting them in the same model, we can easily test for significance.\nThe following code demonstrates that this is true - make sure you understand where each value is coming from!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nTesting Significance\n\n\nShow the code\nsummary(lm(body_mass_g ~ species, data = penguins))$coef\n\n\n                   Estimate Std. Error    t value      Pr(&gt;|t|)\n(Intercept)      3706.16438   38.13563 97.1837676 6.880362e-245\nspeciesChinstrap   26.92385   67.65243  0.3979732  6.909073e-01\nspeciesGentoo    1386.27259   56.90893 24.3594924  1.009947e-75\n\n\nSo… is species significant? \n\n\nShow the code\nanova(lm(body_mass_g ~ species, data = penguins))\n\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n           Df    Sum Sq  Mean Sq F value    Pr(&gt;F)    \nspecies     2 145190219 72595110  341.89 &lt; 2.2e-16 ***\nResiduals 330  70069447   212332                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt’s an ESS test!!!\n\n\nPlotting the Results (Correct, but Don’t Use This)\n\n\nWe fit a model with two predictors: \\[\nI(species == Chinstrap)\\text{ and }I(species == Gentoo)\n\\] As far as R knows, these are two completely separate predictors!\nFor plots on the right, Red line is the mean of body_mass_g when species is Adelie, Chinstrap, or Gentoo, respectively.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dummy Variables</span>"
    ]
  },
  {
    "objectID": "L16-Dummies.html#interactions",
    "href": "L16-Dummies.html#interactions",
    "title": "16  Dummy Variables",
    "section": "16.2 Interactions",
    "text": "16.2 Interactions\n\nSame Slope but Different Intercepts\nIf we have species and flipper_length_mm in the model, we get the following:\n\\[\ny = \\beta_0 + \\beta_1I(species == Chinstrap) + \\beta_2I(species == Gentoo) + \\beta_3 flipper_length_mm\n\\] which is equivalent to: \\[\ny_i = \\begin{cases}(\\beta_0  + 0) + \\beta_3 flipper_length_mm& \\text{if }\\; species == Adelie\\\\ (\\beta_0 + \\beta_1)  + \\beta_3 flipper_length_mm& \\text{if }\\; species == Chinstrap\\\\(\\beta_0  + \\beta_2)  + \\beta_3 flipper_length_mm& \\text{if }\\; species == Gentoo\\end{cases}\n\\]\n\nLike three different models, but with a different intercept depending on species.\n\nUnless \\(\\overline{flipper_length_mm} = 0\\), the intercept and slope are correlated.\n\n\nIn this case, the model is not equivalent. The slope must be the same for all three models, which cannot be done if we’re fitting three separate models. As you know, the slope and intercept are correlated, so different slopes lead to different intercepts.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nVisualizing Different Intercepts (Same Slopes)\n\n\n\nspecies == \"Adelie\" seems to have a different intercept than the others.\nspecies == \"Chinstrap\" and species == \"Gentoo\" could probably have the same intercept.\n\nWe would not generally fit a model where only one dummy gets a different value unless there was good reason.\n\n\n\nShow the code\nmylm &lt;- lm(body_mass_g ~ flipper_length_mm + species, data = penguins)\npenguins$preds &lt;- predict(mylm)\n\nggplot(penguins) +\n    theme_bw() +\n    aes(x = flipper_length_mm, colour = species) +\n    geom_point(aes(y = body_mass_g)) +\n    geom_line(aes(y = preds))\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Significance\nLet’s introduce some fun R magic!\n\n\nShow the code\nmylm &lt;- lm(body_mass_g ~ species + flipper_length_mm, data = penguins)\nanova(mylm, update(mylm, ~ . - species))\n\n\nAnalysis of Variance Table\n\nModel 1: body_mass_g ~ species + flipper_length_mm\nModel 2: body_mass_g ~ flipper_length_mm\n  Res.Df      RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    329 45843144                                  \n2    331 51211963 -2  -5368818 19.265 1.225e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe update() function takes mylm, uses the full formula (~ .), then removes species.\nThis is an ESS ANOVA. There is a sig. diff., but it won’t say which group is different!\nNote that we can get the same results if we put the predictors in a specific order and use R’s built-in Type 2 ESS algorithm.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIgnore the line for flipper_length_mm, since that’s testing whether adding flipper_length_mmlacement to an empty model improves the results.\nUsing the code above, verify and explain the following:\n\nThe p-value in the table would be different if we wrote species + flipper_length_mm.\n\nNote that it would be incorrect.\n\nThe p-value for flipper_length_mm is different from the one we get in the summary(mylm) table.\n\n\n\nInteraction Terms: Different Intercepts, Different Slopes\nWe can expand the model above with an interaction term. \\[\ny = \\beta_0 + \\beta_1I(Chinstrap) + \\beta_2I(Gentoo) + \\beta_3 flipper_length_mm + \\beta_4I(Chinstrap)flipper_length_mm + \\beta_5I(Gentoo)flipper_length_mm\n\\] where \\(I(Chinstrap)\\) is just shorthand for \\(I(species == Chinstrap)\\).\nThis is the same as: \\[\ny_i = \\begin{cases}(\\beta_0 + 0)  + (\\beta_3 + 0) flipper_length_mm& \\text{if }\\; species == Adelie\\\\ (\\beta_0 + \\beta_1)  + (\\beta_3 + \\beta_4) flipper_length_mm& \\text{if }\\; species == Chinstrap\\\\(\\beta_0  + \\beta_2)  + (\\beta_3 + \\beta_5) flipper_length_mm& \\text{if }\\; species == Gentoo\\end{cases}\n\\] In this case, we might as well fit 3 completely different models!\n(Except we can test for significance!)\nAnd we’re back to equivalence! Double check the intercepts and slopes for species == \"Chinstrap\" and species == \"Gentoo\", using the equations above.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nIt’s essentially three different models.\n\n\nShow the code\nggplot(penguins) +\n    theme_bw() +\n    aes(x = flipper_length_mm, y = body_mass_g, colour = species) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x)\n\n\n\n\n\n\n\n\n\n\n\nWhich “Significance” are we testing?\n\n\nShow the code\ninteract_lm &lt;- lm(body_mass_g ~ species * flipper_length_mm, data = penguins)\n\n\n\nSignificance of species?\nSignificance of the interaction?\n\nIt depends on the context!\n\n\nSignificance of the Interaction\nThis is just an ESS for a model with versus without the interaction term:\n\n\nShow the code\ninteract_lm &lt;- lm(body_mass_g ~ species * flipper_length_mm, data = penguins)\nbase_lm &lt;- lm(body_mass_g ~ species + flipper_length_mm, data = penguins)\nanova(interact_lm, base_lm)\n\n\nAnalysis of Variance Table\n\nModel 1: body_mass_g ~ species * flipper_length_mm\nModel 2: body_mass_g ~ species + flipper_length_mm\n  Res.Df      RSS Df Sum of Sq     F   Pr(&gt;F)   \n1    327 44391669                               \n2    329 45843144 -2  -1451475 5.346 0.005193 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe difference in \\(SS_{Reg}\\) is significant - what does that mean?\n\n\nANCOVA\nIf g is a categorical variable, then:\n\nlm(y ~ g) is a t-test (with equal variance) if g is binary\nlm(y ~ g) is an ANOVA if g has more than 2 categories\nlm(y ~ x * g) is an ANCOVA model\n\nANalysis of COVAriance.\n\n\nMain idea: Is the covariance (or correlation) between \\(x\\) and \\(y\\) different for different categories of \\(g\\)?\n\nOnly a small extension to ANOVA\n\nt-test: exactly 2 means\nANOVA: 2+ means\nANCOVA: 2+ covariances\n\n\n\n\nBeyond \\(x\\) and \\(g\\)\n\nIn the simple cases, we’re doing t-test, ANOVA, or ANCOVA.\nBeyond this, we’re just doing regression, no special names.\n\n“Controlling for” is a term we might use later.\n\nChoosing interaction terms is hard.\n\nggplot2 makes parts of it a lot easier.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dummy Variables</span>"
    ]
  },
  {
    "objectID": "L16-Dummies.html#significance-of-a-group",
    "href": "L16-Dummies.html#significance-of-a-group",
    "title": "16  Dummy Variables",
    "section": "16.3 Significance of a Group",
    "text": "16.3 Significance of a Group\n\nOutput of summary.lm()\nThe output compares each slope to 0.\n\nFor a categorical predictor, this tests if a category is different from the reference.\n\nThere is not an easy built-in way to check significance of a specific group\n\nFor example, we may want to test for equality of intercepts and slopes for Chinstrap and Gentoo penguins, allowing Adelie to have separate values.\n\nTest for equality of slopes is something covered in the textbook\nAlternative: Change Reference group and use ESS\n\n\n\n\nChanging the reference group\nSuppose we want to compare Gentoo and Chinstrap penguins. We can set up our model as: \\[\ny = \\beta_0 + \\beta_1I(Gentoo) + \\beta_2I(Adelie) + \\beta_3flipper_length_mm + \\beta_4flipper_length_mmI(Gentoo) + \\beta_5flipper_length_mmI(Adelie)\n\\] where now Chinstrap is the reference group.\nWe can test \\(\\beta_1=\\beta_4 = 0\\) using ESS to test whether body_mass_g versus flipper_length_mm is the same in these two categories.\nIn R, we need to set up our own dummies to do this.:\n\npenguins$species&lt;- relevel(factor(penguins$species), ref = \"Gentoo\")\nlevels(penguins$species)\n\n[1] \"Gentoo\"    \"Adelie\"    \"Chinstrap\"",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dummy Variables</span>"
    ]
  },
  {
    "objectID": "L16-Dummies.html#exercises",
    "href": "L16-Dummies.html#exercises",
    "title": "16  Dummy Variables",
    "section": "16.4 Exercises",
    "text": "16.4 Exercises\n\nComplete the runnable R code above.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dummy Variables</span>"
    ]
  },
  {
    "objectID": "L17-Example_Analysis_mtcars.html",
    "href": "L17-Example_Analysis_mtcars.html",
    "title": "17  Analysis of MTCars",
    "section": "",
    "text": "17.1 Exploratory Data Analysis",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Analysis of MTCars</span>"
    ]
  },
  {
    "objectID": "L17-Example_Analysis_mtcars.html#exploratory-data-analysis",
    "href": "L17-Example_Analysis_mtcars.html#exploratory-data-analysis",
    "title": "17  Analysis of MTCars",
    "section": "",
    "text": "Understanding Data\nWhat do the column names mean?\nThe help file gives a (very) brief description. I spent a few minutes just looking at the descriptions and trying to guess what relationships I might find.\nOverall, most of the predictors are trying to answer the question “Is this a powerful car?”\nIn the code below, change head() to str().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlotting the Data\nWrite down any conclusions about the relationships you see in the pairs plot below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nOnly 1 car has carb = 6, 1 has carb = 8\nwt and drat are (-ively) correlated\n\ndisp and hp\ndisp and drat (-ive)\ndisp and wt\nhp and wt\nhp and qsec\n\n\nwt and disp are clearly multicollinear,; they’re measuring the same thing so I might want to include just one of them.\n\n\nPatterns in the Predictors\nIn the following code, try setting the term on the right of the ~ as am, cyl, gear, and carb. Try the terms on the left side of the as wt, disp, drat, and qsec. Essentially, tried every combination of these and wrote down the most interesting patterns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nwt is different across categories of am, cyl, carb, gear (all positive)\n\ndisp has same relationships\nhp has same relationships, except 4 gear cars have lower hp than 3 and 5 gear cars\ndrat has opposite relationships\n\n\nDo something similar with the following code, checking every combination of all relevant predictors and writing down anything that sticks out.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nClear separation between disp and wt when coloured by am or cyl.\n\nIn other words, there are distinct groups. This probably means that one of the continuous predictors has all of the information necessary, and it won’t be necessary to include an interaction between continuous predictors (it rarely is).\n\nOtherwise, there are not many relationships that might be present.\n\nThe following plot can also be used with all combinations of categorical predictors.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSome kind of “correlation” between am and cyl.\n\nMeasuring something similar, but from different perspectives.\n\nVery little relation between am and vs - they’re measuring different things.\n\nMight be worth checking models where am is switched with vs.\n\n\n\n\nConclusions\nMost things are measuring “how powerful is this car” or “how heavy is this car”, so we should just choose the ones that make sense to us and check a few categorical predictors.\nwt and disp make the most sense as measures for mpg, and am and cyl also make some sense. I’ll try switching out some of the other predictors, but I expect that the final model will either be wt*am or disp*cyl.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Analysis of MTCars</span>"
    ]
  },
  {
    "objectID": "L17-Example_Analysis_mtcars.html#more-eda-relationships-with-the-response-interactions",
    "href": "L17-Example_Analysis_mtcars.html#more-eda-relationships-with-the-response-interactions",
    "title": "17  Analysis of MTCars",
    "section": "17.2 More EDA: Relationships with the Response / Interactions",
    "text": "17.2 More EDA: Relationships with the Response / Interactions\nNow we’re finally looking at mpg!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFrom looking at many many plots, propose 3 (and only 3) candidate models.\n\nmpg versus disp * cyl\nmpg versus wt * am (cyl?)\nmpg versus wt * vs (maybe not an interaction)\nmpg versus wt * gear?\n\nI had also considered including qsec, but a plot of mpg versus qsec with colours from cyl revealed that cyl explains the relationship; if we include cyl, then the slope for mpg versus qsec is 0. The same thing happens with drat, so cyl is probably enough to include in the model rather than either qsec or drat.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Analysis of MTCars</span>"
    ]
  },
  {
    "objectID": "L17-Example_Analysis_mtcars.html#modelling",
    "href": "L17-Example_Analysis_mtcars.html#modelling",
    "title": "17  Analysis of MTCars",
    "section": "17.3 Modelling",
    "text": "17.3 Modelling\nLet’s test out some models! The following code is already set up with a potentially reasonable model (but it isn’t the final model I would have chosen). Change it to test out your top 3 models, writing down any conclusions about the residuals.\nWhen you’ve chosen your top candidate write it down!\nMy final model is as follows:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, investigate a couple changes to the model, justifying each change based on your plots above. The following code tests removing the interaction term, but you should completely change it according to what you’ve done. You are encouraged to go back and try new plots before you test them in a model.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOnce you have your final model, write some interpretations!\n\nResiduals versus fitted looks good\nQQ norm looks great! For this small of a data set, we don’t expect much from the qq-plot, so this is actually very nice.\nScale-Location has a slight U shape, which isn’t ideal. There may still be a predictor that’s worth including.\nThere’s a high influence point. This is likely due to the interaction between cyl and disp.\n\nWhen we have this kind of interaction, there are essentially three lines, each with fewer observations. It is much easier for a point to be influential with interaction present.\n\n\nMy second model is as follows:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFirst plot looks good!\nQQplot has some heavy tails - not bad, but not ideal. dispmodel was better.\nScale-location is great!\nNo high leverage points.\n\nBoth models are good in different ways. Let’s check their summaries.\n(Because of the way webr works, each code chunk is a separate R process. This means I have to re-define the model again in every code chunk.)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe \\(R^2\\) for dispmodel is a fair bit higher (although there’s no standard for how much an \\(R^2\\) should change, so this might not be a meaningful difference). As we saw in class, the \\(R^2\\) is based on the same quantities as the F-test for different models.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe models fit significantly differently. Which one fits better?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ndispmodel has a higher \\(R^2\\) and a lower MSE, so it seems to be the winner.\nFrom the pairs plot, I saw that disp has a slight relationship with other continuous predictors, and the scale-location plot wasn’t perfect. Perhaps another predictor will help?\nI can do this with the magical update() function. The ~ . + hp notation means the response versus (~) everything ., then add hp. The ~ means “versus” (with the response on the left, which isn’t allowed to change in this case, and the predictors on the right), and the . means “everything”, which in this case refers to everything that was already in the model. The form lm(mpg ~ ., data = mtcars) will fit mpg against everything else it sees in the mtcars dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nI checked qsec, drat, and hp, and none seemed worth including in the model. I’ll just leave it as is.\nTo interpret the model we must be careful about the interaction term!\n\\[\nmpg = \\begin{cases}\n\\beta_0 + \\beta_1 disp & \\text{if }cyl == 4\\\\\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_4) disp & \\text{if }cyl == 6\\\\\n(\\beta_0 + \\beta_3) + (\\beta_1 + \\beta_5) disp & \\text{if }cyl == 8\\\\\n\\end{cases}\n\\]\n\nFor 4 cylinder cars, the baseline mpg is 40 and decreases by 0.135 for each one unit increase in disp.\nFor 6 cylinder cars, the baseline mpg is about 21.5 and isn’t really related to the displacement.\nFor 8 cylinder cars, the baseline mpg is about 24.5 and decreases by about 0.02 for each one-unit increase in displacement.\n\nNote that displacement has really large units, so 0.02 over hundreds of one-unit increases is still a lot!",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Analysis of MTCars</span>"
    ]
  },
  {
    "objectID": "L17-Example_Analysis_mtcars.html#conclusions-1",
    "href": "L17-Example_Analysis_mtcars.html#conclusions-1",
    "title": "17  Analysis of MTCars",
    "section": "17.4 Conclusions",
    "text": "17.4 Conclusions\nWe chose the final model purely based on balancing a good fit without overfitting. This lead us to modelling the fuel efficiency of the car as a linear function of the engine displacement, with a different relationship depending on the number of cylinders. Generally, cars with more cylinders tend to have a lower fuel efficiency, and a larger engine is also associated with lower efficiency.\nThe engine displacement is a measure of both the weight of the car as well as the power of the car. The number of cylinders can also be seen as a measure of these two things, so our final model seems reasonable.\nIn this study, we are limited by both the size of the data and the purpose of data collection. These are observational data, with no effort made to control any of the predictors. For instance, we did not simply take a Mazda RX4 and change the number of cylinders to see what might happen. Instead, we observed a correlation in the data. A car company that wants to build a more fuel efficient car might want to use these associations as a starting point, but would need to do some controlled experiments to see what causal relationships might exist.\nThe data do not include all of the predictors that I would have liked. For instance, there is no measure of the aerodynamics of the cars. Any study focused on fuel efficiency would benefit from such a study. Furthermore, the fuel efficiency is the one reported by the company; a controlled study would measure the actual fuel efficiency in identical conditions for all cars.\nIn addition, the data are old. Like, very old. Cars are very different now, and these data are very outdated. There are absolutely no conclusions from this model that would actually be useful for modern cars.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Analysis of MTCars</span>"
    ]
  },
  {
    "objectID": "L17-Example_Analysis_mtcars.html#comment-the-purpose-of-a-study",
    "href": "L17-Example_Analysis_mtcars.html#comment-the-purpose-of-a-study",
    "title": "17  Analysis of MTCars",
    "section": "17.5 Comment: The purpose of a study",
    "text": "17.5 Comment: The purpose of a study\nIn this study, we focused on the model fit. However, if a company was interested specifically in the speed of a car rather than it’s weight, we might have instead focused on the qsec predictor.\nIn many studies, researchers may have a set of predictors that they are particularly interested in. For example, in a study on whether a new treatment affects the patients’ heart rate, while “controlling” for other factors (biosex, race, weight, age, etc.). Such a regression task could be written as: \\[\nheartrate_i = \\beta_0 + \\beta_1I(treatment_i) + \\beta_2age_i + \\beta_3I(biosexMale_i) + \\beta_4weight_i \\beta_5I(raceBlack_i) + \\beta_6I(raceHispanic) + ... + \\epsilon_i\n\\] and we would never investigate a model that does not have the predictor that defines the treatment. Instead, we might find the best model according to all other predictors (possibly with transformations), then look at whether the treatment variable is significant.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Analysis of MTCars</span>"
    ]
  },
  {
    "objectID": "L18-Modelling_Poorly.html",
    "href": "L18-Modelling_Poorly.html",
    "title": "18  Modelling Poorly",
    "section": "",
    "text": "18.1 Motivation",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Modelling Poorly</span>"
    ]
  },
  {
    "objectID": "L18-Modelling_Poorly.html#motivation",
    "href": "L18-Modelling_Poorly.html#motivation",
    "title": "18  Modelling Poorly",
    "section": "",
    "text": "Selecting Predictors\nAs you’ve seen, choosing predictors is hard!\nWouldn’t it be nice if the computer would choose the best model for you?\n\n\nThe “Best” Model\n\nBest represents the Data Generating Process (DGP)\n\nBest for inference\n\nMisses the DGP, but provides useful insights into the relationships\n\nAlso best for inference\n\nFits the current data the best\n\nOverfitting?\n\nBest able to predict new values\n\nRandom Forests and Neural Nets\n\n\nThe “best” model depends on the goal of the study!",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Modelling Poorly</span>"
    ]
  },
  {
    "objectID": "L18-Modelling_Poorly.html#picking-the-best-fitting-model",
    "href": "L18-Modelling_Poorly.html#picking-the-best-fitting-model",
    "title": "18  Modelling Poorly",
    "section": "18.2 Picking the “Best Fitting” Model",
    "text": "18.2 Picking the “Best Fitting” Model\n\nModel Comparison Criteria\n\n\\(R^2\\), or adjusted \\(R^2\\)\n\nHigher is better, but \\(R^2\\) increases as we add predictors\n\n\\(s^2\\), the residual variance.\n\nAdding predictors always decreases \\(s^2\\)\n\nMallow’s \\(C_p\\) statistic\n\n\\(C_p = RSS_p/s^2 - (n - 2p)\\)\n\n\\(RSS_p\\) is the RSS of the smaller model, with \\(p\\) parameters\n\\(s^2\\) is the MSE from the largest model under consideration\n\nAdding predictors does not increase this statistic.\n\nAIC, the Akiake Information Criterion\n\n\\(AIC = 2p - 2\\ln(\\hat L)\\), where \\(\\hat L\\) is the likelihood evaluated at the estimated parameters.\n\nE.g., when \\(\\epsilon_i\\sim N(0,\\sigma^2)\\), the likelihood is the product of normal distributions with a mean of \\(X\\hat{\\underline\\beta}\\) and variance \\(s^2\\).\n\nDoes not increase with added predictors.\n\n\n\n\nMore on AIC\n\\[\nAIC = 2p - 2\\ln(\\hat L)\n\\]\nRecall from Maximum Likelihood Estimation, the likelihood is the likelihood of observing the particular data, given the parameters: \\[\nL(y|\\underline\\beta, X, \\sigma) = \\prod_{i=1}^nf_Y(X|\\underline\\beta, \\sigma^2),\n\\] where \\(f_Y(X|\\underline\\beta, \\sigma^2)\\) is the normal distribution.\n\nA high AIC means we either:\n\nHave too many parameters, or\nOur model doesn’t fit the data well.\n\nA low AIC means we’ve got a good model that isn’t overly complicated\n\n“Low” is relative to other models",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Modelling Poorly</span>"
    ]
  },
  {
    "objectID": "L18-Modelling_Poorly.html#automated-model-selection-algorithms",
    "href": "L18-Modelling_Poorly.html#automated-model-selection-algorithms",
    "title": "18  Modelling Poorly",
    "section": "18.3 Automated Model Selection Algorithms",
    "text": "18.3 Automated Model Selection Algorithms\n\nBest Subset\n\nFind the collection of predictors that optimizes the statistic of interest.\n\nThat’s it. You just try them all.\n\n\nBackward and Forward Selection\n\nBackward Selection\n\nInclude all predictors, try removing one\n\nCheck the \\(R^2\\), p-values, Mallow’s Cp, or AIC\n\nPut that one back in the model, try removing another\nRepeat unti you’ve found the “best” predictor to exclude. Then find the next best one!\n\nForward Selection\n\nFind the best predictor to include first\nFind the best predictor to include second\n…\n\n\nBoth have some sort of stopping criteria.\n\n\nBackward Selection\n\nFit a model with all \\(p\\) predictors\nTry all models with \\(p-1\\) predictors.\n\nIdentify the best one, say remove \\(x_j\\)\nCheck stopping criteria.\n\nIf stopping critera not met, try all models with \\(p-2\\) predictors, not including \\(x_j\\).\n\n\n\nBackward Selection Example\n\nStart with mpg ~ disp + wt + am + cyl + qsec.\nCheck all of the AICs, remove cyl.\nCheck all of the AICs, remove disp.\nCheck all AICs, stop.\n\nFinal model: mpg ~ wt + am + qsec\n\n\nForward Selection\n\nStart with mpg ~ 1.\nTest each predictor individually, check AIC, keep wt.\nTest each remaining predictor, check AIC, keep cyl.\nTest each remaining predictor, stop.\n\nFinal model: mpg ~ wt + cyl\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nThink-Pair-Share\nWhat might these methods be missing?\nWhen would these methods be useful?\n\n\nEvaluating Algorithmic Predictor Selection\nSuppose we have measured 30 predictors that we know are not related to the response.\nHow many predictors should Backwards Selection select?\nTry it out yourself!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nA Special Case: Race\n\nIn general, we almost always want to include race if it was measured.\n\nIf our model is using race to make a decision, we want to know about it!\n\nPossible approach: not dummy variables.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Modelling Poorly</span>"
    ]
  },
  {
    "objectID": "L18-Modelling_Poorly.html#the-best-model-1",
    "href": "L18-Modelling_Poorly.html#the-best-model-1",
    "title": "18  Modelling Poorly",
    "section": "18.4 The Best Model",
    "text": "18.4 The Best Model\n\nNeural Networks and Random Forests\n\nNeural Networks\n\nEssentially a series of linear regressions with a minor non-linear transformation.\nA “deep” neural network is non-linear transformations and all interactions.\nVery finicky, but very powerful when necessary.\n\nRandom Forests\n\nAlso a series of non-linear effects with interactions.\nMuch much much less finicky.\n\n\n\n\nCausal Inference\nExperiments are our way of controlling variables so that we can isolate their effect.\nMost data we tend to use is observational.\n\nCausal inference is statistical magic to determine causality from observation.\n\n… with varying degrees of success.\n\n\n\n\nWhy are you telling us this, Devan?\nWhich model is “best”?\n\nBest predictions?\n\nNN and RF, with cross-validation.\n\nBest inference?\n\nBuild a model based on the context of the problem.\nChoose transformations and interactions appropriately.\nOnly check p-values at the very end.\n\nBest subset of predictors?\n\nRecall: multicollinearity. Without an experiment, correlated predictors mean that there’s no way to tell which predictors are best.\n\n\nOpinion: Algorithmic selection methods are bad approximations to better techniques that are outside out the scope of this course.",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Modelling Poorly</span>"
    ]
  },
  {
    "objectID": "L19-Degrees_of_Freedom.html",
    "href": "L19-Degrees_of_Freedom.html",
    "title": "19  Degrees of Freedom",
    "section": "",
    "text": "19.1 Generating Data (for the Project)",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Degrees of Freedom</span>"
    ]
  },
  {
    "objectID": "L19-Degrees_of_Freedom.html#generating-data-for-the-project",
    "href": "L19-Degrees_of_Freedom.html#generating-data-for-the-project",
    "title": "19  Degrees of Freedom",
    "section": "",
    "text": "model.matrix() and Matrix Multiplication\n\nYou may find it convenient to use the model.matrix() function to set up dummy variables and polynomial terms for you. It will make sure that there is a reference category, which it will choose alphabetically.\nAlso note that you’ll want to set raw = TRUE to ensure that people can find the coefficient values that you set. With raw = FALSE they will get the exact same predictions (and thus the same error), but not the same coefficient values.\nIn the code below, I also demonstrate a log-transform for \\(y\\). To get a log on the left side of the equation, we use an exponential on the right.\n\n\n\nShow the code\nmycat &lt;- c(\"category\", \"category\", \"dogegory\", \"category\", \"birdegory\")\nmycont &lt;- c(12, 14, 4, 10,  20 )\n\nX &lt;- model.matrix(~ mycat + poly(mycont, 2, raw = TRUE))\nX\n\n\n  (Intercept) mycatcategory mycatdogegory poly(mycont, 2, raw = TRUE)1\n1           1             1             0                           12\n2           1             1             0                           14\n3           1             0             1                            4\n4           1             1             0                           10\n5           1             0             0                           20\n  poly(mycont, 2, raw = TRUE)2\n1                          144\n2                          196\n3                           16\n4                          100\n5                          400\nattr(,\"assign\")\n[1] 0 1 1 2 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$mycat\n[1] \"contr.treatment\"\n\n\nShow the code\n# Names are just for my own purposes, they don't need to be included.\n# The names help me keep track of which column in X they correspond to.\nbetas &lt;- c(intercpt = 10, cat = 20, dog = 0, c1 = 0, c2 = 0.05)\nX %*% betas\n\n\n  [,1]\n1 37.2\n2 39.8\n3 10.8\n4 35.0\n5 30.0\n\n\nShow the code\nmydf &lt;- data.frame(\n    y = exp(X %*% betas + rnorm(5, 0, 0.01)), \n    mycat = mycat, \n    mycont = mycont\n)\ncoef(lm(log(y) ~ mycat + poly(mycont, 2, raw = TRUE), data = mydf))\n\n\n                 (Intercept)                mycatcategory \n                9.712145e+00                 1.978742e+01 \n               mycatdogegory poly(mycont, 2, raw = TRUE)1 \n                5.406038e-04                 8.415754e-02 \npoly(mycont, 2, raw = TRUE)2 \n                4.652628e-02 \n\n\nFor the project, you ould only need to submit the mydf object; the others should be able to recover your coefficient values from that alone!",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Degrees of Freedom</span>"
    ]
  },
  {
    "objectID": "L19-Degrees_of_Freedom.html#df",
    "href": "L19-Degrees_of_Freedom.html#df",
    "title": "19  Degrees of Freedom",
    "section": "19.2 df",
    "text": "19.2 df\n\nDegrees of Freedom\nA measure of how much information you chose to put in the model.\n\nContinuous predictors add 1\nCategorical predictors add \\(k-1\\)\nEach term in a polynomial adds 1\nInteractions add 1\netc.\n\n\n\nChoose it and Use it\n\nDecide on the df, try to use all of them\n\nChoice is based on how much information you think you can extract.\nMany rows = much information; you can probably use more predictors.\n\n\n\n\nBad use of df\n\nDiscretising a categorical variable\n\nYou better have a reeeaaaalllllyyyy good justification\nFor example, discretising age to match up with insurance categories.\nYou should almost never choose to discretise based on your own logic; only to fit in with other analyses or use cases.\n\nPolynomial terms when interactions would work.\nRedundant categories\n\nCategories that are redundant\nIf you have redundant categories, then you have categories that are redundant\nFor example, if the “JobTitle” column has entries like “Data Scientist” as well as “Data Scientist and Machine Learning Expert”, you could just code those both as “DS”.\n\n\n\n\nSaving df\n\nCombine categories\n\nJust “Data Scientist” or “Software Engineer”\n\nCombine predictors\n\nVolume of beak = \\(\\pi r^2h/3\\)?\n\nTransform response rather than add polynomial terms\n\nNot always recommended.\n\n\n\nHere’s an example of using transformations to (1) match the context of the problem and (2) get better results with fewer degrees of freedom.\nWe’ll use the trees dataset that’s built into R. The “Girth” column is actually the diameter, and it’s the only column measured in inches rather than feet. I’m going to make a new predictor based on Girth that’s more useful for later models.\n\n\nShow the code\n# Saving df\nhead(trees) # \"Girth\" is actually diameter, according to help file\n\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\nShow the code\ntrees$Radius &lt;- trees$Girth/24\n\n\nA naive model might be a basic multiple linear regression.\n\n\nShow the code\nmultiple_lm &lt;- lm(Volume ~ Radius + Height, data = trees)\nsummary(multiple_lm)\n\n\n\nCall:\nlm(formula = Volume ~ Radius + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nRadius      112.9959     6.3424  17.816  &lt; 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nWe could make this fit better by blindly adding polynomial terms and doing a transformation:\n\n\nShow the code\ntransformed_and_poly &lt;- lm(log(Volume) ~ poly(Girth, 2) + poly(Height, 2), data = trees)\nsummary(transformed_and_poly)\n\n\n\nCall:\nlm(formula = log(Volume) ~ poly(Girth, 2) + poly(Height, 2), \n    data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.16094 -0.04023 -0.00295  0.05474  0.13434 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.27273    0.01492 219.360  &lt; 2e-16 ***\npoly(Girth, 2)1   2.51150    0.09732  25.808  &lt; 2e-16 ***\npoly(Girth, 2)2  -0.26046    0.09206  -2.829  0.00887 ** \npoly(Height, 2)1  0.54845    0.09746   5.628 6.47e-06 ***\npoly(Height, 2)2 -0.05518    0.09191  -0.600  0.55349    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08307 on 26 degrees of freedom\nMultiple R-squared:  0.9784,    Adjusted R-squared:  0.9751 \nF-statistic: 294.5 on 4 and 26 DF,  p-value: &lt; 2.2e-16\n\n\n\nIt is interesting that the squared term for height is not significant. Why is this so interesting to me? Look at the next equation in this lesson…\n\nA slightly better model might be one of the form \\[\nV = \\pi r^2h\n\\] which assumes that trees are perfect cylinders. This can be accomplished by modelling: \\[\\begin{align*}\n\\log(V) &= \\beta_0 + \\beta_1\\log(r) + \\beta_2\\log(h) + \\epsilon\\\\\n\\implies V &= \\exp(\\beta_0)r^\\beta_1h^\\beta_2\\exp(\\epsilon)\n\\end{align*}\\] and expecting that \\(\\exp(\\beta_0)\\) is close to \\(\\pi\\), \\(\\beta_1 = 2\\), and \\(\\beta_2 = 1\\).1\n\n\nShow the code\nvolume_logs &lt;- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)\nsummary(volume_logs)\n\n\n\nCall:\nlm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.168561 -0.048488  0.002431  0.063637  0.129223 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.63162    0.79979  -8.292 5.06e-09 ***\nlog(Girth)   1.98265    0.07501  26.432  &lt; 2e-16 ***\nlog(Height)  1.11712    0.20444   5.464 7.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08139 on 28 degrees of freedom\nMultiple R-squared:  0.9777,    Adjusted R-squared:  0.9761 \nF-statistic: 613.2 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\n\nWe get something close to our hopes!\n\nExcept for \\(\\beta_0\\), which we’ll talk about later.\n\nWith this model, we could do a hypothesis test for \\(\\beta_1 = 2\\) and \\(\\beta_2 = 1\\).\n\nIf these are reasonable values, loggers could confidently calculate the volume of a tree assuming that it’s a cylinder!\n\n\nWe could also assume these values from the start, and include a “naive” volume.\n\n\nShow the code\ntrees$naive_volume &lt;- pi * trees$Radius^2 * trees$Height\n\n\nWe could then model this according to \\[\\begin{align*}\nV & = \\beta_0N + \\epsilon\n\\end{align*}\\] where \\(N\\) is our “naive” volume. We might have the expectation that \\(\\beta_0 = 1\\) if the naive volume is correct.\n\n\nShow the code\n# Assuming trees are cylinders\ndiff_from_cylinder &lt;- lm(Volume ~ -1 + naive_volume, data = trees)\nsummary(diff_from_cylinder)\n\n\n\nCall:\nlm(formula = Volume ~ -1 + naive_volume, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6696 -1.0832 -0.3341  1.6045  4.2944 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nnaive_volume 0.386513   0.004991   77.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.455 on 30 degrees of freedom\nMultiple R-squared:  0.995, Adjusted R-squared:  0.9949 \nF-statistic:  5996 on 1 and 30 DF,  p-value: &lt; 2.2e-16\n\n\nThis tells us that the estimated usable lumber from a given tree is about 40% of what we would expect if the tree were a perfect cylinder.\nImportantly for this lecture, we have an \\(R^2\\) of 0.9949 on a single degree of freedom! \\(R^2\\) is not the greatest measure, but it’s informative in this case:\n\n\n\n\n\nmodel\nR2\ndf\n\n\n\n\nmultiple_lm\n0.9442322\n2\n\n\ntransformed_and_poly\n0.9750853\n4\n\n\nvolume_logs\n0.9760840\n2\n\n\ndiff_from_cylinder\n0.9948560\n1\n\n\n\n\n\nBy choosing our transformations carefully, we have a model that is both better and simpler! The coefficient estimate also relates to a physical quantity that is useful to us - the percent of usable wood we can get from a tree! Statistics is amazing.2\n\n\n\nResearcher Degrees of Freedom\nYou add information that isn’t measured by df!\n\nChoosing one predictor rather than another.\n\nBill length or bill depth?\n\nTransforming a predictor/response\nRemoving outliers\nUsing/not using autoregressive error structures\netc.\n\nThis is why we use RMarkdown/Quarto/Jupyter - all of this is (should be) recorded!",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Degrees of Freedom</span>"
    ]
  },
  {
    "objectID": "L19-Degrees_of_Freedom.html#footnotes",
    "href": "L19-Degrees_of_Freedom.html#footnotes",
    "title": "19  Degrees of Freedom",
    "section": "",
    "text": "It makes me very happy that we’re taking the “log” when talking about lumber.↩︎\nT-shirt idea: “If you don’t think stats is lit af then you ain’t woke, fam!”↩︎",
    "crumbs": [
      "Choosing Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Degrees of Freedom</span>"
    ]
  },
  {
    "objectID": "L20-Regularization.html",
    "href": "L20-Regularization.html",
    "title": "20  Regularization Methods",
    "section": "",
    "text": "20.1 Loss Functions",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regularization Methods</span>"
    ]
  },
  {
    "objectID": "L20-Regularization.html#loss-functions",
    "href": "L20-Regularization.html#loss-functions",
    "title": "20  Regularization Methods",
    "section": "",
    "text": "Why are we minimizing the sum of squares?\nThe MSE is defined as: \\[\nMSE(\\underline\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\n\\] This is the Maximum Likelihood Estimate, which seeks to model the mean of \\(Y\\) at each value of \\(X\\), \\(E(Y) = X\\underline\\beta\\), with Gaussian errors. \nMSE, seen as a function of \\(\\underline\\beta\\), is a loss function, i.e. the function we minimize to find our estimates.\nBut it’s FAR from the only loss function.\n\n\nOther loss functions\nBy minimizing \\[\nMAE(\\underline\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right|\n\\] we end up estimating the \\(median\\) of \\(Y\\). \nOthers:\n\nMean Absolute Log Error\n\nLower penalty for larger errors.\nMore robust to outliers?\n\nMean Relative Error\n\nPenalize errors relative to size of \\(y\\) (larger errors at large \\(y\\) values aren’t as big of a deal).\nAssumes that variance depends on mean (kinda like Poisson).\n\netc.\n\n\n\nExamples\n\n0.5 hectares verus 4 hectares can make a huge difference\n\n100 versus 150, congrats on the great prediction!\n\nPredicting an income of 15,000 versus 25,000 is big\n\nModelling the average income is not usually reasonable.\n\n\n\n\nLoss Function Summary\n\nMinimize the loss function with respect to the parameters of interest.\nFor the same parameters, there can be many loss functions.\nOther names:\n\nLikelihood function (special case of loss function)\nCost function (synonym)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regularization Methods</span>"
    ]
  },
  {
    "objectID": "L20-Regularization.html#regularization",
    "href": "L20-Regularization.html#regularization",
    "title": "20  Regularization Methods",
    "section": "20.2 Regularization",
    "text": "20.2 Regularization\n\nRegular Methods\nOrdinary least squares is a minimization problem: \\[\nRSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\n\\]\nWhat if I don’t like how big the \\(\\underline\\beta\\) values are?\n\n\nRegularization Constraints\nLet’s arbitrarily say that \\(\\sum_{j=1}^p \\beta_j = 10\\).\nWith this constraint, one large \\(\\beta_j\\) can be countered by a large negative \\(\\beta_k\\).\n\n\nRegularizing with Norms\nThe \\(L_p\\)-norm of a vector is: \\[\n||\\beta||_p = \\left(\\sum_{j=1}^p|\\beta|^p\\right)^{1/p}\n\\]\n\nWhen \\(p = 2\\), this is the Euclidean distance.\n\nPythagoras strikes again!\n\nWhen \\(p = 1\\), this is the sum of absolute values.\nWhen \\(p=\\infty\\), this ends up being max(\\(|\\underline\\beta|\\)).\n\nNot useful for our purposes, but interesting!\n\n\n\n\nWhy choose \\(||\\underline\\beta||_p = 10\\)?\nOr, in general, why choose a particular value of \\(s\\) in \\(||\\underline\\beta||_p = s\\)?\n\nThere’s no good reason to choose a particular value of \\(s\\), but regularizing stops us from having steep slopes for predictors that aren’t actually related.\nIn other words, we ignore spurious patterns!\nToo little regularization and we just have the OLS estimate. Too much regularization and we restrict the parameters too much.\n\n\n\nChoosing \\(s\\) in \\(||\\underline\\beta||_p = s\\)\n\nRecall: more flexible models are able to estimate more subtle patterns, but may find patterns that aren’t there.\n\nToo flexible = bad out-of-sample prediction.\n\nFor linear models, the least flexible model is one where all \\(\\beta_j\\) values are given a fixed value.\n\nFor example, all are 0. \n\n\nFor a linear model, restricting the values with \\(||\\underline\\beta||_p = s\\) reduces flexibility, which can improve out-of-sample prediction performance.\n#| standalone: true\nlibrary(ggplot2)\n#library(shiny)\nlibrary(MASS)\n\nmyseed &lt;- 57362\n\nui &lt;- fluidPage(\n    sidebarPanel(\n        sliderInput(\"corr\", \"Correlation between predictors\", -1, 1, 0.5, 0.1),\n        sliderInput(\"s\", \"Restriction on sum of abs(beta)\", 0, 5, 0.5, 0.1),\n        actionButton(inputId = \"doit\", label = \"Click me for new data\")\n    ),\n    mainPanel(plotOutput(\"plot\"), )\n)\n\nserver &lt;- function(input, output) {\n    new_seed &lt;- reactive({\n        input$doit\n        myseed &lt;&lt;- myseed + 1\n    })\n\n    make_data &lt;- reactive({\n        set.seed(myseed)\n        input$doit\n        X &lt;- mvrnorm(100, mu = c(0, 0), \n            Sigma = matrix(c(2, input$cor, input$corr, 2), ncol = 2))\n        x1 &lt;- X[, 1]\n        x2 &lt;- X[, 2]\n        y &lt;- -0.2 * x1 + 2*x2 + rnorm(100, 0, 3)\n\n        beta_grid &lt;- expand.grid(\n            beta_1 = seq(-2, 3, by = 0.05),\n            beta_2 = seq(-2, 3, by = 0.05)\n        )\n        beta_grid$sum &lt;- rowSums(abs(beta_grid))\n        beta_grid$sse &lt;- sapply(1:nrow(beta_grid), function(x) {\n            sum((y - beta_grid[x, 1] * x1 - beta_grid[x, 2] * x2)^2)\n        })\n\n        unrestricted_min &lt;- beta_grid[which.min(beta_grid$sse),]\n\n        beta_grid &lt;- beta_grid[beta_grid$sum &lt;= input$s, ]\n        restricted_min &lt;- beta_grid[which.min(beta_grid$sse),]\n\n        add_points &lt;- rbind(unrestricted_min, restricted_min)\n        add_points$Restriction &lt;- c(\"Unrestricted\",\n            paste0(\"sum(abs(beta)) &lt;= \", input$s))\n        list(beta_grid = beta_grid, add_points = add_points)\n    })\n\n    output$plot &lt;- renderPlot({\n        new_seed()\n        new_data &lt;- make_data()\n        beta_grid &lt;- new_data$beta_grid\n        add_points &lt;- new_data$add_points\n\n        ggplot() +\n            theme_bw() +\n            geom_tile(\n                data = beta_grid,\n                mapping = aes(x = beta_1, y = beta_2, fill = sse)\n            ) +\n            scale_fill_viridis_c() +\n            coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 3)) +\n            geom_point(\n                data = add_points,\n                mapping = aes(x = beta_1, y = beta_2, colour = Restriction),\n                size = 10\n            )\n    })\n}\n\nshinyApp(ui = ui, server = server)\n\n\nMLE estimates of \\(\\underline\\beta\\) are unbiased\n… therefore constrained estimates are biased.\n\n\nBut what about the scales of the features?\nWhat a great question! Thank you so much for asking! You must be smart.\nFor \\(||\\underline\\beta||_p\\) to make sense, the predictors must all have the same scale.\nThis is accomplished by standardizing the features: Replace each \\(x_{ij}\\) with \\[\n\\frac{x_{ij} - \\bar{\\mathbf{x}_{j}}}{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - \\bar{\\mathbf{x}_{j}})^2}\n\\]\n\n\nChoosing \\(\\lambda\\) via cross-validation\n\nFor each value of \\(\\lambda\\):\n\nSplit data into 5 “folds”.\nFor each “fold”:\n\nSet aside the data points in the current fold.\nFit the model to data in all other “folds” using your value of \\(\\lambda\\).\nPredict the missing points, record the average error.\n\n\n\nChoose the lambda with the lowest out-of-sample prediction error.\n\n\nCross-Validation\n\n\n\nSpecial Cases of Regularization: \\(L_1\\) or \\(L_2\\)?\nSo far, we’ve been talking about general \\(L_p\\) norms, i.e. \\(||\\underline\\beta||_p\\).\n\n\\(L_1\\): LASSO\n\\(L_2\\): Ridge\n\n\n\nGeometric Interpretation (Contours of the RSS)\n\n\n\n\n\nLASSO will set coefficients to 0.\n\n“Least Absolute Shrinkage and Selection Operator”\n\nRidge has less variance (why?)\n\n\n\n\n\nLangrangian Multipliers and Estimation\nWikipedia screenshot:\n\n\n\nLagrangian Multipliers and Estimation\nMinimize \\(MSE(\\underline\\beta)\\) subject to \\(||\\underline\\beta||_p\\).\nis equivalent to\nMinimize \\(MSE(\\underline\\beta) + \\lambda||\\underline\\beta||_p\\)\nFor the rest of your life, this is the way you’ll see Ridge and LASSO.\n\nRidge: Analytical solution, can calculate an arbitrary number of \\(\\lambda\\) values at once.\nLASSO: Non-iterative numerical technique\n\n\n\nRidge Regularization\n\nOne of the coefficients increases with a tighter constraint!\n\n\n\nLASSO Feature Selection as we Vary \\(\\lambda\\)\n\n\n\nAs \\(\\lambda\\) increases, more coefficients are allowed to be non-zero.\nIf \\(\\lambda\\) doesn’t constrain, we get the least squares estimate.\n\nDenoted as \\(\\hat\\beta\\) in the plot.\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPersonal Opinion Time\nWith the existence of LASSO, there’s no reason to do automated feature selection.\nBest subset selection can be written as: \\[\n\\text{Minimize } MSE(\\underline\\beta)\\text{ subject to }\\sum_{j=1}^pI(\\beta\\ne 0) \\le s\n\\] This can minimize out-of-sample error, but results in something that could be mistaken for inference.\nWith LASSO, you know the estimates are biased and you know why. Best subset tricks you into thinking your \\(\\underline\\beta\\) estimates are accurate - they are not.\n\n\nImplementation in R: glmnet\n\nThe glm in glmnet is because it fits all GLMs.\n\nIncluding Logistic Regression.\nThe family = binomial argument works as in glm()\n\nHowever, family = \"binomial\" is an optimized version.\n\n\nThe net in glmnet refers to elasticnet.\n\nNext slide or two.\n\n\n\n\nElastic Net: Like a lasso, but more “flexible”\n\\[\n\\text{Minimize } MSE(\\underline\\beta) + \\lambda\\left[\\alpha||\\underline\\beta||_1 + (1-\\alpha)||\\underline\\beta||_2\\right]\n\\]\nElastic Net is “doubly regularized”.\nElastic net needs more time to fit and needs more data.\n\n\nElasticnet and LASSO/Ridge\n\\[\n\\text{Minimize } MSE(\\underline\\beta) + \\lambda\\left[\\alpha||\\underline\\beta||_1 + (1-\\alpha)||\\underline\\beta||_2\\right]\n\\]\n\n\\(\\alpha = 0 \\implies\\) Ridge\n\\(\\alpha = 1 \\implies\\) LASSO\n\n\nHere’s an example of LASSO in R. We’ll load in the Wage data from ISLR2 package1.\nThis data set has a column for wage and a column for logwage. We’re going to use wage as our response, and removing wage makes it easier to tell R to use all columns other than logwage. I also remove region since there are some regions with too few observations and I am not going to set up cross-validation appropriately for this scenario.\n\n\nShow the code\nlibrary(glmnet) # cv.glmnet() and glmnet()\nlibrary(ISLR2) # Wage data set\n\nWage &lt;- ISLR2::Wage\n# From names(Wage), I want to remove \"region\" and \"wage\"\nWage &lt;- Wage[, -c(6, 11)]\n\n\nglmnet doesn’t use the formula notation (y ~ x); we have to manually set up the design matrix (including dummy variables) and the response vector.\n\n\nShow the code\nX &lt;- model.matrix(logwage ~ ., data = Wage)[,-1]\ny &lt;- as.numeric(Wage$logwage)\n\n\nThe first step to fitting a LASSO model is choosing \\(\\lambda\\) via cv. The cv.glmnet() function does this for us. The results are not a final model; the resultant object gives us an idea of which value of \\(\\lambda\\) is appropriate.\n\n\nShow the code\ncv_check &lt;- cv.glmnet(x = X, y = y, alpha = 1)\nplot(cv_check)\n\n\n\n\n\n\n\n\n\nThe first dotted line indicates the value of \\(\\lambda\\) that minimizes the “loss function.” However, across different samples we would get different values of \\(\\lambda\\). Because we know there’s randomness, we know that a slightly larger (more restrictive) value of \\(\\lambda\\) would also be consistent with our data. Since cross-validation emulates the idea of having many samples, we can get an estimate of the standard error of \\(\\lambda\\). We can then choose the value of \\(\\lambda\\) that is within 1 standard error of the minimum. This gives a much simpler model while still having a plausible \\(\\lambda\\).2\nNow that we have a way of telling R what value we want for lambda, we can fit the model.\n\n\nShow the code\nmy_lasso &lt;- glmnet(X, y, lambda = cv_check$lambda.1se)\nmy_lasso\n\n\n\nCall:  glmnet(x = X, y = y, lambda = cv_check$lambda.1se) \n\n  Df  %Dev Lambda\n1  9 35.59 0.0127\n\n\nThe output isn’t very informative, but the model can make predictions via the predict() function and these will be comparable or better than the predictions from an unconstrained linear model.\nLet’s compare the coefficient values to see the shrinkage in action! Of course, glmnet standardizes by default, so we need to ensure that the linear model is based on standardized predictors.\nIn the output, I include a column for the difference in the coefficients. Specifically, it’s lm minus lasso, so we may expect “shrinkage” to mean that the lasso estimates are smaller.\n\n\nShow the code\nstandardized_X &lt;- apply(X, 2, scale)\nstandardized_lm &lt;- lm(y ~ standardized_X)\ncoef_mat &lt;- cbind(coef(my_lasso),\n    coef(standardized_lm))\n\nres &lt;- cbind(\n        coef_mat, \n        apply(coef_mat, 1, function(x) abs(x[2]) - abs(x[1]))\n    ) |&gt; \n    round(3)\ncolnames(res) &lt;- c(\"lasso\", \"lm\", \"|lm|-|lasso|\")\nres\n\n\n17 x 3 sparse Matrix of class \"dgCMatrix\"\n                             lasso     lm |lm|-|lasso|\n(Intercept)                 -7.830  4.654       -3.176\nyear                         0.006  0.026        0.019\nage                          0.002  0.029        0.027\nmaritl2. Married             0.128  0.076       -0.052\nmaritl3. Widowed             .      0.004        0.004\nmaritl4. Divorced            .      0.012        0.012\nmaritl5. Separated           .      0.017        0.017\nrace2. Black                 .     -0.012        0.012\nrace3. Asian                 .     -0.005        0.005\nrace4. Other                 .     -0.007        0.007\neducation2. HS Grad          .      0.038        0.038\neducation3. Some College     0.058  0.075        0.018\neducation4. College Grad     0.166  0.119       -0.047\neducation5. Advanced Degree  0.313  0.151       -0.162\njobclass2. Information       0.017  0.013       -0.004\nhealth2. &gt;=Very Good         0.041  0.027       -0.014\nhealth_ins2. No             -0.189 -0.089       -0.100\n\n\n\nThe estimates aren’t all smaller! Lasso chose to set some to 0, which freed up some coefficient “budget” to spend elsewhere.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regularization Methods</span>"
    ]
  },
  {
    "objectID": "L20-Regularization.html#footnotes",
    "href": "L20-Regularization.html#footnotes",
    "title": "20  Regularization Methods",
    "section": "",
    "text": "ISLR stands for Introduction to Statistical Learning with R, a fantastic (and free) book if you want to learn more advanced topics in predictive modelling!↩︎\nThis is similar to the Box-Cox transformation, where we find a bunch of plausible transformations, and go with a simple one like \\log() or sqrt().↩︎",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regularization Methods</span>"
    ]
  },
  {
    "objectID": "L21-Logistic.html",
    "href": "L21-Logistic.html",
    "title": "21  Classification",
    "section": "",
    "text": "21.1 Logistic Regression",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "L21-Logistic.html#logistic-regression",
    "href": "L21-Logistic.html#logistic-regression",
    "title": "21  Classification",
    "section": "",
    "text": "Goal: Predict a 1\n\nResponse: 0 or 1\n\nPredictions: probability of a 1?\n\n\n\n\nThe Logistic Function - A Sigmoidal Function\nIf \\(t\\in\\mathbb{R}\\), then \\[\n\\sigma(t) = \\dfrac{\\exp(t)}{1 + \\exp(t)}\\in[0,1]\n\\] where \\(\\sigma(\\cdot)\\) is the logistic function.\n\n\n\n\n\n\n\n\n\n\n\nLogistic Function - Now with Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Function - Now with Parameters Estimated from DATA\n\\[\\begin{align*}\n\\eta(x_i) &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ...\\\\\np(x_i) &= \\sigma(\\eta(x_i)) = \\dfrac{\\exp(\\eta(x_i))}{1 + \\exp(\\eta(x_i))}\\\\\n\\implies \\log\\left(\\frac{p_i(x_i)}{1-p_i(x_i)}\\right) &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ...\n\\end{align*}\\]\n\n\nShow the code\nlibrary(ISLR2)\nlibrary(ggplot2)\nlibrary(dplyr)\n#| echo: false\nDefault %&gt;% \n    mutate(default = as.numeric(factor(default)) - 1) %&gt;%\n    ggplot() + theme_minimal() +\n        aes(x = balance, y = default) + \n        geom_jitter(width = 0, height = 0.05) +\n        geom_smooth(method = \"glm\", se = FALSE,\n            method.args = list(family = \"binomial\")) +\n        labs(x = \"Credit Card Balance\",\n            y = \"Default?\")\n\n\n\n\n\n\n\n\n\n\\(\\eta(x_i) = -10.65 + 0.0054\\cdot\\text{balance}_i\\)\n\n\nLogistic Regression\n\nThe response is 0 or 1 (no or yes, dont’ default or default, etc.)\nThe probability of a 1 increases according to the sigmoid function.\n\nThe linear predictor is \\(\\eta(x_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots\\)\nThe probability of class 1 is \\(P(\\text{class }1 | \\text{predictors}) = \\sigma(\\eta(x_i))\\)\n\nInstead of normality assumptions, we use a binomial distribution.\n\nIt’s just one step away from a linear model!\n\n\nInterpreting Parameters\n\nGeneral structure: “For each one unit increase in \\(x_i\\), some function of \\(y_i\\) changes by some function of \\(\\beta\\)”“.\nFor logistic regression:\n\nFor each one unit increase in \\(x_i\\), \\(\\log\\left(\\frac{p(x_i)}{1-p(x_i)}\\right)\\) increases by \\(\\beta\\).\n\nThe odds are \\(\\frac{p(x_i)}{1-p(x_i)}\\).\n\n“1 in 5 people with offs of 1/4 will default on their loan.”\n\n\\(\\beta\\) represents the change in log odds for a one unit increase.\n\n“log odds ratio”.\n\n\n\n\nEstimating Parameters: Maximum Likelihood\nFor all observations:\n\nIf \\(y_i = 0\\), we want \\(p(x_i)\\) to be as low as possible.\n\nMaximize \\(1 - P(Y_{i'} = 1|\\beta_0,\\beta_1,X)\\)\n\nIf \\(y_i = 1\\), we want \\(p(x_i)\\) to be as high as possible.\n\nMaximize \\(P(Y_{i'} = 1|\\beta_0,\\beta_1,X)\\)\n\n\nThese can be combined as: \\[\n\\ell(\\beta_0,\\beta_1) = \\prod_{i':y_{i'} = 0}(1 - P(Y_{i'} = 1|\\beta_0,\\beta_1,X))\\prod_{i:y_i=1}P(Y_i = 1|\\beta_0,\\beta_1,X)\n\\] Which is NOT just the sum of squared errors!\nUnlike linear regression, there’s no closed form for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) \\(\\Rightarrow\\) need numerical methods.\n\n\nExamples: Two different predictors in the Default data\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\eta(x_i) = -3.5 + 0.5\\cdot\\text{student}\n\\]\nThe odds of a student defaulting are \\(\\exp(0.5)\\approx1.65\\) times as high as a non-student.\n\n\n\n\n\n\n\n\n\n\n\\[\n\\eta(x_i) = -10.65 + 0.005\\cdot\\text{balance}\n\\]\nEach extra dollar of credit card balance increases the odds of defaulting by a factor of 1.005.\n\n\nThe scale of the predictors matters.\n\n\nOdds versus Probabilities\n“The odds of a student defaulting are \\(\\exp(0.5)\\approx1.65\\) times as high as a non-student.”\n\\[\n\\frac{P(\\text{defaulting} | \\text{student} = 1)}{1 - P(\\text{defaulting} | \\text{student} = 1)} \\biggm/ \\frac{P(\\text{defaulting} | \\text{student} = 0)}{1 - P(\\text{defaulting} | \\text{student} = 0)} = 1.65\n\\] This cannot be solved for \\(P(\\text{defaulting} | \\text{student} = 1)\\)!\n\\[\nP(\\text{defaulting} | \\text{student} = 1) = \\dfrac{\\exp(\\eta(x_i))}{1 + \\exp(\\eta(x_i))} = \\dfrac{\\exp(-3.5 + 0.5\\cdot 1)}{1 + \\exp(-3.5 + 0.5\\cdot 1)} \\approx 0.047\n\\]\n\n\nMultiple Linear Logistic Regression\n\nPredictors can be multicollinear, confounded, and have interactions.\n\nLogistic is just Linear on a transformed scale!\n\nWe do not look for transformations of the response.\n\nIt’s already a transformation of the response \\(p_i(x_i)\\)!\n\nWe do look for transformations of the predictors!\n\nSigmoid + Polynomial is where the real fun is.\n\n\n\n\nErrors in Logistic Regression: Deviance\n\nAll “errors” are either \\(p(x_i)\\) or \\(1 - p(x_i)\\).\n\nDistance from either 0 or 1.\n\n\nInstead, we use the deviance.\n\nIf \\(p(x_i)\\) were the true probability in a binomial distribution, what’s the probability of the observed value (0 or 1)?\n\nThis is used more broadly in Generalized Linear Models (GLMs). Logistic Regression is one of many GLMs.\n\n\n\n\nLogistic Decision Boundaries\n\\[\nP(\\text{defaulting} | \\eta(x_i)) &gt; p \\implies a + bx_1 + cx_2 + dx_3 &gt; e\n\\]\nFor some (linear) hyperplane \\(a + bx_1 + cx_2 + dx_3\\) and some value \\(e\\).\n\nChoosing \\(p=0.5\\) is logical, but other thresholds can be chosen.\n\nCancer example: want to be more admissive of false positives\n\nWould rather operate and be wrong than falsely tell the patient that they’re healthy!\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ISLR2)\n\nDefault$default &lt;- as.numeric(factor(Default$default)) - 1\nDefault$student &lt;- as.numeric(factor(Default$student)) - 1\n\ndecision_grid &lt;- expand.grid(\n    student = c(0,1),\n    balance = seq(0, 2655, length.out = 250),\n    income= seq(770, 73555, length.out = 250)\n)\n\nmy_glm &lt;- glm(default ~ student + balance + income,\n    data = Default, family = binomial)\ndecision_grid$pred &lt;- predict(my_glm, newdata = decision_grid)\n\nstudent_labels &lt;- c(\"Not Student\", \"Student\")\nnames(student_labels) &lt;- c(0, 1)\n\nggplot() + theme_minimal() +\n    geom_tile(data = decision_grid,\n        mapping = aes(x = balance, y = income, fill = factor(pred &gt; 0.5))) +\n    scale_fill_manual(values = c(\"firebrick\", \"green\", \"firebrick\", \"green\")) +\n    geom_point(data = Default, \n        mapping = aes(x = balance, y = income, \n            fill = factor(default == 1)),\n        shape = 21) +\n    facet_wrap(~ student, \n        labeller = labeller(student = student_labels)) +\n    labs(x = \"Credit Card Balance\",\n        y = \"Income\",\n        fill = \"Default?\")\n\n\n\n\n\n\n\n\n\n\n\nPredictions - Just Plug it In!\n\n\n\n\nIntercept\nStudent\nBalance\nIncome\n\n\n\n\n\\(\\beta\\)\n-10.09\n-0.65\n0.0057\n0.000003\n\n\n\nWe can make a prediction for a student with $2,000 balance and $20,000 income: \\[\\begin{align*}\n\\eta(x) &= \\beta_0 + \\beta_1\\cdot 1 + \\beta_2\\cdot 2000 + \\beta_3\\cdot 20000 \\approx 0.0178\\\\\n&\\\\\nP(\\text{defaulting} | x) &= \\dfrac{\\exp(\\eta(x))}{1 + \\exp(\\eta(x))} \\approx \\dfrac{\\exp(0.0178)}{1 + \\exp(0.0178)} \\approx 0.504\\\\\n&\\\\\n&P(\\text{defaulting} | x) &gt; 0.5 \\implies \\text{Predict Default}\n\\end{align*}\\]",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "L21-Logistic.html#classification-basics",
    "href": "L21-Logistic.html#classification-basics",
    "title": "21  Classification",
    "section": "21.2 Classification Basics",
    "text": "21.2 Classification Basics\n\nGoal: Predict a Category\n\nBinary: Yes/no, success/failure, etc.\nCategorical: 2 or more categories.\n\nA.k.a. qualitative, but that’s a social science word.\n\n\nIn both: predict whether an observation is in category \\(j\\) given its predictors. \\[\nP(Y_i = j| x = x_i) \\stackrel{def}{=} p_j(x_i)\n\\]\n\n\nClassification Confusion\nConfusion Matrix: A tabular summary of classification errors.\n\n\n\n\n\n\nTrue Pay (\\(\\cdot 0\\))\nTrue Def (\\(\\cdot 1\\))\n\n\n\n\nPred Pay (\\(0 \\cdot\\))\nGood (00)\nBad (01)\n\n\nPred Def (\\(1 \\cdot\\))\nBad (10)\nGood (11)\n\n\n\n\n\nTwo ways to be wrong\nTwo ways to be right\nDifferent applications have different needs\n\n\n\nAccuracy: \\(\\dfrac{\\text{Correct Predictions}}{\\text{Number of Predictions}} =\\frac{00 + 11}{00 + 01 + 10 + 11}\\)\n\n\nIs “Accuracy” Good?\nTask: Predict whether a person has cancer\n(In this made up example, 0.02% of people have cancer).\n\n\n\n\nTrue Healthy\nTrue Cancer\n\n\n\n\nPred. Healthy\nSave a Life\nLose a Life\n\n\nPred. Cancer\nExpensive/Invasive\nAll good\n\n\n\n\n\n\nEasy: 99.8% accuracy.\n\nAlways guess “Not Cancer”\n\n\n\n\nVery Hard: 99.82% accuracy.\n\n\n\n\n\nThe Confusion Matrix for Default Data\n\n\n\n\nTrue Payment\nTrue Default\n\n\n\n\nPred Payment\n9627\n228\n\n\nPred Default\n40\n105\n\n\n\n\nThis model: 97.32% accuracy.\n\nNaive model: always predict “Pay” - 96.67% accuracy!\n\n\nOther important measures (not on exam):\n\nSensitivity: \\(\\dfrac{\\text{True Positives}}{\\text{All Positives in Data}} = \\dfrac{9627}{9627 + 40} = 99.58%\\) (Naive: 100%)\nSpecificity: \\(\\dfrac{\\text{True Negatives}}{\\text{All Negatives in Data}} = \\dfrac{105}{105 + 228} = 31.53\\) (Naive: 0%)\n\n\n\nLogistic Regression in R\nSee Course Notes\n\nModel building works very similarly, but it’s very difficult to interpret the residual plots.\n\n\nShow the code\nlibrary(palmerpenguins)\npeng &lt;- penguins[complete.cases(penguins), ]\n\nlog_cont &lt;- glm(sex ~ bill_length_mm + \n        bill_depth_mm + flipper_length_mm,\n    data = peng, family = \"binomial\")\n\nanova(log_cont, test = \"Chisq\") # Sequential\n\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: sex\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                                332     461.61              \nbill_length_mm     1   41.185       331     420.42 1.385e-10 ***\nbill_depth_mm      1   96.786       330     323.64 &lt; 2.2e-16 ***\nflipper_length_mm  1   72.666       329     250.97 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nShow the code\nlog_spec &lt;- update(log_cont, ~ . + species * flipper_length_mm)\n\nanova(log_cont, log_spec, test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm\nModel 2: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       329     250.97                          \n2       325     174.71  4   76.265 1.076e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nShow the code\nfull_spec &lt;- glm(sex ~ species*(bill_length_mm + \n        bill_depth_mm + flipper_length_mm),\n    data = peng, family = \"binomial\")\n\nanova(log_spec, full_spec, test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\nModel 2: sex ~ species * (bill_length_mm + bill_depth_mm + flipper_length_mm)\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       325     174.71                     \n2       321     170.32  4   4.3894   0.3559\n\n\nShow the code\nanova(log_spec, update(log_spec, ~ . - species:flipper_length_mm), test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\nModel 2: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       325     174.71                     \n2       327     178.78 -2  -4.0697   0.1307\n\n\nShow the code\nsummary(log_spec)\n\n\n\nCall:\nglm(formula = sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    species + flipper_length_mm:species, family = \"binomial\", \n    data = peng)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2365  -0.3125   0.0033   0.3252   2.5852  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        -69.63032   10.83543  -6.426 1.31e-10 ***\nbill_length_mm                       0.65617    0.10521   6.237 4.47e-10 ***\nbill_depth_mm                        1.96180    0.29827   6.577 4.79e-11 ***\nflipper_length_mm                    0.04329    0.04375   0.990   0.3224    \nspeciesChinstrap                   -38.57778   22.93391  -1.682   0.0925 .  \nspeciesGentoo                      -34.58899   20.48998  -1.688   0.0914 .  \nflipper_length_mm:speciesChinstrap   0.15957    0.11696   1.364   0.1725    \nflipper_length_mm:speciesGentoo      0.15988    0.09666   1.654   0.0981 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 461.61  on 332  degrees of freedom\nResidual deviance: 174.71  on 325  degrees of freedom\nAIC: 190.71\n\nNumber of Fisher Scoring iterations: 7\n\n\nShow the code\ncoef(log_spec)\n\n\n                       (Intercept)                     bill_length_mm \n                      -69.63031836                         0.65616618 \n                     bill_depth_mm                  flipper_length_mm \n                        1.96180392                         0.04329349 \n                  speciesChinstrap                      speciesGentoo \n                      -38.57777987                       -34.58899421 \nflipper_length_mm:speciesChinstrap    flipper_length_mm:speciesGentoo \n                        0.15956974                         0.15987824 \n\n\nThe residual plots are the same as before:\n\n\nShow the code\npar(mfrow = c(2, 2))\nplot(log_spec)\n\n\n\n\n\n\n\n\n\nThe predictions can either be on the logit scale (type = \"link\", the default) or on the response scale (probabilities).\n\n\nShow the code\npredict(log_spec, type = \"response\") |&gt; head()\n\n\n        1         2         3         4         5         6 \n0.6335863 0.1789059 0.6382736 0.6613775 0.9918045 0.2059974 \n\n\nRegularization is often used with logistic regression (in python’s scikit-learn package, Ridge regularization is used by default without warning the user).\n\n\nShow the code\nlibrary(glmnet)\n\nX &lt;- model.matrix(sex ~ species*(bill_length_mm + \n        bill_depth_mm + flipper_length_mm),\n    data = peng)\ny &lt;- as.factor(peng$sex)\n\nmycv &lt;- cv.glmnet(X, y, family = binomial)\n\nmylasso &lt;- glmnet(X, y,\n    data = peng, family = \"binomial\", alpha = 1,\n    lambda = mycv$lambda.1se)\ncoef(mylasso)\n\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                                             s0\n(Intercept)                        -46.90824021\n(Intercept)                          .         \nspeciesChinstrap                    -3.49235983\nspeciesGentoo                        .         \nbill_length_mm                       0.32778935\nbill_depth_mm                        1.26452716\nflipper_length_mm                    0.05765817\nspeciesChinstrap:bill_length_mm      .         \nspeciesGentoo:bill_length_mm         .         \nspeciesChinstrap:bill_depth_mm       .         \nspeciesGentoo:bill_depth_mm          .         \nspeciesChinstrap:flipper_length_mm   .         \nspeciesGentoo:flipper_length_mm      .",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "L21-Logistic.html#multinomial-regression",
    "href": "L21-Logistic.html#multinomial-regression",
    "title": "21  Classification",
    "section": "21.3 Multinomial Regression",
    "text": "21.3 Multinomial Regression\n\nMultinomial Logistic Regression: K Classes\nWe have a total probability of 1 to distribute across the classes,\n\nStick breaking\n\nFit a logistic regression of class 1 versus not class 1.\n\nRemove obs. with class 1\n\nFit a logistic regression of class 2 versus not class 1.\n\nRemove obs. with class 2\n\n…\nClass \\(K\\) gets whatever probability is left over.\n\nSoftmaxing\n\nFor all classes, fit a logistic regression of class k versus not class k.\nIn the end, divide by the total probability to make sure they sum to 1.\n\n\nVery often used in machine learning!\n\n\nThese two give the same results!",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "L22-Review.html",
    "href": "L22-Review.html",
    "title": "22  Final Exam Review",
    "section": "",
    "text": "22.1 Summaries",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Final Exam Review</span>"
    ]
  },
  {
    "objectID": "L22-Review.html#summaries",
    "href": "L22-Review.html#summaries",
    "title": "22  Final Exam Review",
    "section": "",
    "text": "Generally important things\n\nThe bias and variance of \\(\\hat{\\underline\\beta}\\).\nInterpreting coefficients and inference.\nVariance, rather than point estimates.\nInterpreting residual plots.\nChoosing a reasonable model given the context of the problem.\n\n\n\n“Extra Topics” Lecture\n\nStandardizing\n\nEffect on parameter estimates\n\nGeneral Linear Hypotheses\n\nFull versus hypothesized model, and the form of the F test\n\n(Not the math, but possibly the degrees of freedom)\n\n\nWeighted/Generalized Least Squares\n\nUnderstand what \\(V\\) and \\(P\\) represent.\nManipulate formulas by replacing \\(\\underline f\\) with \\(P^{-1}\\underline \\epsilon\\).\nSuggest \\(V\\) based on the context of the question.\n\n\n\n\nGetting the Wrong Model\n\nBias due to missing predictors.\nWhat does it even mean to have the “right” model?\n\nProxy measures and their effect on the other parameter estimates\n\n\n\n\nTransforming the Predictors\n\nPolynomial regression\n\nWhen to use them\nLower order terms\nExtrapolation\n\nOther transformations (e.g. log, combining predictors, etc.)\n\n\n\nTransforming the Response\n\nExplain “Stabilizing the variance”\nDiagnosing the need for a transformation\nChoosing transformations\nThe effect on the model\n\nE.g. multiplicative errors, changes to the parameter estimates\n\n\n\n\nDummy Variables\n\nDefinition and interpretation\n\nfactor(cyl)8 in the coefficients table\n\nIf there are three categories, we need two dummies\n\n“Reference” category is absorbed into the intercept.\n\nInteraction terms: different intercept, different slope.\nSignificance of a dummy variable (or interaction with one)\nExtra sum-of-squares to test whether categories are statistically different\n\nWhat kind of test is this? ANOVA? ANCOVA?\n\n\n\n\nMulticollinearity\n\nWhy it increases variance (many different parameter combinations are equivalent)\nDetecting via the variance inflation factor\n\nApprox 10 is bad, but this is just a rule-of-thumb.\n\nWhat to do about it, and what it means for interpreting coefficients.\n\n\n\nBest Subset Selection\n\nGoal: Inference or prediction?\n\nHow does Subset Selection fit into this?\n\nGeneral idea of the algorithms.\n\nDon’t need to know Mallow’s Cp etc.\n\nWhy the p-values can’t really be trusted.\nUseful as a preliminary step (sometimes).\n\n\n\nDegrees of Freedom\n\nGeneral modelling strategies.\nChoosing transformations based on domain knowledge.\nBeing explicit about the decisions made while modelling.\n\n\n\nRegularization\n\nIt’s just linear regression with “smaller” slope estimates!\n\nIntercept isn’t constrained.\nSome slopes can be bigger than the slopes for linear regression, but sum of abs/squares is smaller.\n\nRegularization prevents overfitting.\n\nChoose the penalty parameter \\(\\lambda\\) by minimizing out-of-sample prediction error, measured via cross-validation.\nWithin 1 SE of minimum MSE leads to a simpler (more regularized) model.\n\nAdds bias, which is a good thing?\nRequires standardization of the predictors.\nLASSO sets parameters to 0, Ridge does not.\n\n\n\nClassification\n\nResponse values are 0 or 1\n\nExpected value of \\(y\\) is the proportion of 1s given a value of \\(x\\).\nPredictions can be converted to 0s and 1s, with two types of errors (confusion matrix).\n\nLogistic function looks like an “S”\n\nExact shape determined by linear predictor \\(\\eta(X) = X\\underline\\beta\\)\n\nOther than transformations of response, all lm topics apply.\n\nIncluding regularization!\n\n“Residuals” are weird\n\nNot “observed minus expected” anymore!",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Final Exam Review</span>"
    ]
  },
  {
    "objectID": "L22-Review.html#midterm-solutions",
    "href": "L22-Review.html#midterm-solutions",
    "title": "22  Final Exam Review",
    "section": "22.2 Midterm Solutions",
    "text": "22.2 Midterm Solutions\n\nANOVA\nExplain how a hypotheses test based on the ratio \\(MS_{Reg}/MS_E\\) in the ANOVA table is a test for whether any of the slope parameters are 0.\n\nA line with all slopes equal to 0 is a horizontal line, which will have 0 variance around \\(\\bar y\\), i.e. \\(MS_{Reg} = 0\\).\nDue to random chance, we will never actually get \\(MS_{Reg} = 0\\). Dividing by MSE gives us a way to evaluate the size of \\(MS_{Reg}\\).\n\n\n\nBias/Variance\nFor this question, assume that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\nGiven the model \\(y_i = \\beta_0 + \\epsilon_i\\), show that \\(\\hat\\beta_0 = \\bar y\\) minimizes the sum of squared error.\n\n\\[\\begin{align*}\n\\frac{1}{n}\\sum(y_i - \\hat y)^2 &= \\frac{1}{n}\\sum(y_i - \\beta_0)^2\\\\\n\\frac{d}{d\\beta}\\frac{1}{n} &= \\frac{1}{n}\\sum(y_i - \\beta_0)^2\\\\\n&= \\frac{2}{n}(\\sum y_i - n\\beta_0) \\stackrel{set}{=}0\\\\\n\\implies \\frac{2}{n}\\sum y_i &= \\frac{2}{n}n\\beta_0 \\implies \\bar y = \\beta_0\n\\end{align*}\\]\nThis is a minimum since this is a polynomial in \\(\\beta_0\\) with a positive coefficient for \\(\\beta_0\\).\n\n\nBias/Variance\nFor this question, assume that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\nGiven the model \\(y_i = \\beta_0 + \\epsilon_i\\), show that \\(E(\\hat\\beta_0) = \\beta_0\\) and \\(V(\\hat\\beta_0) = \\sigma^2/n\\).\n\n\\[\\begin{align*}\nE(\\hat\\beta_0) &= E\\left(\\frac{1}{n}\\sum y_i\\right)= \\frac{1}{n}\\sum E(y_i)\\\\\n&= \\frac{1}{n}\\sum E(\\beta_0 + \\epsilon_i)= \\frac{1}{n}n\\beta_0 = \\beta_0\n\\end{align*}\\]\n\\[\\begin{align*}\nV(\\hat\\beta_0) &= V\\left(\\frac{1}{n}\\sum y_i\\right)= \\frac{1}{n^2}\\sum V(y_i)\\\\\n&= \\frac{1}{n^2}\\sum V(\\beta_0 + \\epsilon_i)= \\frac{1}{n^2}n\\sigma^2 = \\frac{\\sigma^2}{n}\n\\end{align*}\\]\n\n\nBias/Variance\nFor this question, assume that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\nNow consider the multiple linear regression model \\(Y = X\\beta + \\underline\\epsilon\\), where we know that \\(\\hat{\\underline{\\beta}} = (X^TX)^{-1}X^TY\\). Show that \\(E(\\hat{\\underline{\\beta}}) = \\underline{\\beta}\\).\n\n\\[\nE(\\hat{\\underline{\\beta}}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TE(Y) = (X^TX)^{-1}X^TX\\underline\\beta = \\underline\\beta\n\\]",
    "crumbs": [
      "Review",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Final Exam Review</span>"
    ]
  },
  {
    "objectID": "Lb12-Corr_in_Betas.html",
    "href": "Lb12-Corr_in_Betas.html",
    "title": "Appendix F — Extra Topics",
    "section": "",
    "text": "Show the code\nn_sim &lt;- 1000\nn &lt;- 30\nbetas &lt;- matrix(ncol = 3, nrow = n_sim)\nbetacs &lt;- betas\n\nfor (i in 1:n_sim) {\n    x1 &lt;- runif(n, 0, 10)\n    x2 &lt;- runif(n, 0, 10) + 2*x1\n    y &lt;- 3 - 8*x1 + 4*x2 + rnorm(n, 0, 4)\n    betas[i, ] &lt;- coef(lm(y ~ x1 + x2))\n\n    x1c &lt;- scale(x1)\n    x2c &lt;- scale(x2)\n    betacs[i, ] &lt;- coef(lm(y ~ x1c + x2c))\n}\n\npar(mfrow = c(2, 3))\nplot(betas[, c(1,3)])\nplot(betas[, c(1,3)])\nplot(betas[, c(2,3)])\nplot(betacs[, c(1,2)])\nplot(betacs[, c(1,3)])\nplot(betacs[, c(2,3)])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Extra Topics</span>"
    ]
  },
  {
    "objectID": "Lb19-Permutation.html",
    "href": "Lb19-Permutation.html",
    "title": "Appendix L — Permutation Tests",
    "section": "",
    "text": "L.1 Introduction\nA permutation is a re-ordering of a set of things. In this case, we’re randomly re-ordering the data. Note that we are not randomly sampling rows, we are just changing the order. In particular, we’re randomly re-ordering the measurements independently of each other!\nWhy would we do this? Because it destroys the correlation between our variables while preserving their statistical properties. This gives us a “null” distribution that makes no assumptions about the distribution of the data!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "Lb19-Permutation.html#intuition-the-facial-impact-test",
    "href": "Lb19-Permutation.html#intuition-the-facial-impact-test",
    "title": "Appendix L — Permutation Tests",
    "section": "L.2 Intuition: the facial impact test",
    "text": "L.2 Intuition: the facial impact test\nThe best statistical test is the “facial impact test”. In this test, we reject the null hypothesis if the results are so obvious that they jump off the page and slap you in the face (facial impact).\nFor this example I’m going to plot 20 different plots. One of them is going to be the original data, and the rest will be random permutations. If you can tell which is the tru data, then there’s probably something important going on! In other words, the relationship between the response and the predictor matters.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nI’ve ran this several times and my cheek hurts from how much the results have slapped me in the face.\nHere’s a less obvious example, but I’m sure you can still pick out the real data:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "Lb19-Permutation.html#the-permutation-test-for-significance-of-regression",
    "href": "Lb19-Permutation.html#the-permutation-test-for-significance-of-regression",
    "title": "Appendix L — Permutation Tests",
    "section": "L.3 The Permutation Test for Significance of Regression",
    "text": "L.3 The Permutation Test for Significance of Regression\nA test for significance usually finds the probability of getting results at least as significant as the results we got, assuming that the null is true.\nFor regression, the null is that there is no relationship. Permutations destroy the relationship, so permutations create a null distribution without making any normality assumptions! To get the p-value, we can do a bunch of permutations and look at our data compared to the distribution under the null hypothesis.\nThe code below demonstrates this for the cars data. Notice how similar this is to the simulation code we’ve used, but we’re not generating any new data. Try it again with the women data!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUnder the null, the sample slopes can range from -2 to 2. However, our actual slope was 4! Notice that the p-value calculation is the probability of a value at least as extreme; we first find the distance from the mean, then count the number of values this far or further from the mean. The expression betas &lt; below | betas &gt; above returns a bunch of true/false values, with “TRUE” meaning that the point is either below below or above above. The mean() function, when applied to a bunch of true/false values, is a proportion. Thus, we are calculating the proportion of points that were at least as far away as the data we got, which is the definition of a p-value!\nIn the end, we got a p-value of exactly 0. The p-value from normality assumptions is below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThat’s a p-value of 0.0000000000015. To get a p-value this low in a permutation test, we’d need somewhere near 10000000000 permutations, and even then we might not get one outside of our critical region. It’s not a surprise that our permutation test p-value was 0!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "Lb19-Permutation.html#permute-the-response-or-the-predictors",
    "href": "Lb19-Permutation.html#permute-the-response-or-the-predictors",
    "title": "Appendix L — Permutation Tests",
    "section": "L.4 Permute the response or the predictor(s)?",
    "text": "L.4 Permute the response or the predictor(s)?\nFor the simple linear regression examples above, it doesn’t matter. Once one is permuted, any existing relationship is destroyed.\nFor multiple linear regression, it suddenly matters a lot!\n\nIf we permute the response, it’s severing the relationship between the response and all predictors.\n\nThis is like an F-test for overall significance!!!\n\nIf we permute one of the predictors, it severs the relationship between the response and that particular predictor, while also severing any multicollinearity!\n\nThis is often a good way to do things since it tests whether that predictor contributes to the regression, not just the response.\nWatch how the other predictors’ coefficients change!\n\nIf we permute all of the predictors together then the relationship between y and x is removed, but all multicolinearity remains.\nIf we permute the predictors individually, then there should be no relationships anywhere among the data.\n\nFor the last two, we often just care about the response given our predictors; it doesn’t often make sense to permute the predictors individually.\nNote that there’s a secret fifth option: permute some of the predictors together. This is like an Extra Sum-of-Squares test for a group of predictors!\nRun the following code many times with kind = \"y\" and watch what changes. Change kind to \"x\" to see what changes, then also try \"xx\".\n(Note: you can click on the last line and hit “Shit+Enter” to run the code cell without scrolling up and clicking the “Run Code” button.)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nkind = \"y\" affects the response, without affecting multicollinearity.\nkind = \"x\" affects all relationships involving x\nkind = \"xx\" affects relationships involving x1 and x2, but there’s an important exception…",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "Lb19-Permutation.html#example-permutation-test-for-bill-depth",
    "href": "Lb19-Permutation.html#example-permutation-test-for-bill-depth",
    "title": "Appendix L — Permutation Tests",
    "section": "L.5 Example: Permutation Test for Bill Depth",
    "text": "L.5 Example: Permutation Test for Bill Depth\nIs bill_depth_mm an important predictor in the Palmer Penguins data?\nThe method for getting the answer depends on the model that we choose! In the following, we’re modelling the body mass against the flipper length, bill length, and bill depth (all with an interaction term with species), and the the different islands are given different intercepts. This is by no means a “good” model - flipper_length_mm * species is a good enough model. However, this model has an interaction term that will persist if we take bill depth out of the equation, and so it’s a good example of doing an ESS test for all parameters involving bill_depth. We also know that there’s some multicollinearity in these data, which has implications for the permutation test.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThat looks pretty definitively like bill_depth_mm affects the F statistic!\nCompare this to the ESS test:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIf you entered the formula correctly, you should get a p-value of 1.352e-11, which is also a pretty definitive difference.\nIf we wanted to do an ANOVA test for overall significance, we would have permuted body_mass_g. Try it out using the two code blocks above!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  }
]